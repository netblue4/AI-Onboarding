{
  "Phase AI system information": [
    {
      "StepName": "x(A.9.4) AI system's intended use and limitations",
      "Objectives": [
        {
          "Objective": "Document the purpose, target users, and intended use cases of the AI system."
        }
      ],
      "Fields": [
        {
          "Control": "A.9.4 – Intended use of the AI system",
          "FieldName": "(A.9.4) - AI System ID",
          "FieldLabel": "AI System ID",
          "FieldText": "AI System Unique Number",
          "FieldType": "Auto generated number"
        },
        {
          "Control": "A.9.4 – Intended use of the AI system",
          "FieldName": "(A.9.4) - Name",
          "FieldLabel": "Name",
          "FieldText": "Name of the AI application",
          "FieldType": "TextBox"
        },
        {
          "Control": "A.9.4 – Intended use of the AI system",
          "FieldName": "(A.9.4) - Business Purpose",
          "FieldLabel": "Business Purpose",
          "FieldText": "What specific business problem or task does this system address?",
          "FieldType": "TextBox"
        },
        {
          "Control": "A.9.4 – Intended use of the AI system",
          "FieldName": "(A.9.4) - Intended Use",
          "FieldLabel": "Intended Use",
          "FieldText": "Describe the use cases of how the AI system will solve the specific business problem or task",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "AI system impact assessment - Employees - Benefits",
          "FieldLabel": "Select Potential Benefits",
          "FieldText": "",
          "FieldType": "MultiSelect:Faster Service/Reduced Error Rate/Increased Efficiency/Personalized Training or Upskilling/Better Decision-Making Tools"
        },
        {
          "Control": "A.9.4 – Intended use of the AI system",
          "FieldName": "HR-WIA-2.3-AugmentationTasks",
          "FieldLabel": "Augmented/Enhanced Tasks",
          "FieldText": "List the high-value tasks that will be significantly improved, made more accurate, or accelerated by the AI system's assistance.",
          "FieldType": "TextBox"
        }
      ]
    },
    {
      "StepName": "(A.9.3) Objectives for responsible use of AI system",
      "WebFormTitle": "To establish and document the guiding principles for the ethical, fair, and responsible operation of the AI system.",
      "Objectives": [
        {
          "Objective": "1. Human Agency and Oversight: To ensure that the AI system is used to augment, not replace, human judgment. All high-stakes decisions suggested by the AI must be subject to meaningful human review and approval before taking effect."
        },
        {
          "Objective": "2. Fair and Equitable Application: To commit to using the AI system in a manner that prevents discriminatory outcomes. The system's application must be monitored to ensure it is applied equitably across all relevant demographic groups and contexts."
        },
        {
          "Objective": "3. Purpose Limitation: To guarantee that the AI system is used strictly for its defined and documented intended purpose. This principle prohibits using the system or its data for any secondary, unvalidated, or unapproved applications."
        },
        {
          "Objective": "4. Accountability and Recourse: To maintain a clear and auditable trail of the system's use, ensuring there is a designated person accountable for its outcomes and a transparent process for affected individuals to challenge decisions and seek recourse."
        }
    ],
    "Fields": []
	},
    {
      "StepName": "x(A.4.2, A.4.4) AI Systems Software and Tooling Resources",
      "Objectives": [
        {
          "Objective": "Define the software and tools used by the AI system."
        }
      ],
      "Fields": [
        {
          "FieldName": "(A.4.2, A.4.4) - Tool Name and Version",
          "FieldLabel": "Tools Name and Version",
          "FieldText": "Name of the software, libraries, or frameworks and their Version numbers.",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "(A.4.2, A.4.4) - Category",
          "FieldLabel": "Select Tool Categories",
          "FieldText": "",
          "FieldType": "MultiSelect:Programming Language/IDE/Data Processing/ML Framework/Version Control/Deployment"
        },
        {
          "FieldName": "(A.4.2, A.4.4) - Purpose/Use Case in Project",
          "FieldLabel": "Purpose/Use Case in Project",
          "FieldText": "How these tools will be used in the AI system's lifecycle.",
          "FieldType": "TextBox"
        }
      ]
    },
    {
      "StepName": "x(A.4.2, A.4.5) AI Systems Computing Resources",
      "Objectives": [
        {
          "Objective": "Define the computing resources used by the AI system"
        }
      ],
      "Fields": [
        {
          "FieldName": "(A.4.2, A.4.5) - Computing Resource Name and Version",
          "FieldLabel": "Computing Resource Name and Version",
          "FieldText": "Names of the computing resources used.",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "(A.4.2, A.4.5) - Category",
          "FieldLabel": "Category of the computing resource",
          "FieldText": "",
          "FieldType": "MultiSelect:Dev Workstation/ML Training Cluster/Inference API Server/Data Lake Storage/EC2/S3/SQL Database"
        },
        {
          "FieldName": "(A.4.2, A.4.5) - Lifecycle Phase(s) Supported",
          "FieldLabel": "Which parts of the AI lifecycle do these resource supports?",
          "FieldText": "",
          "FieldType": "MultiSelect:Development/Training/Testing/Staging/Production/Monitoring"
        }
      ]
    }
  ],
  "Phase Impact assessments": [
    {
      "StepName": "x(A.5.2, A.5.3, A.5.4) Workforce Transition and Adaptation for AI Integration",
      "Objectives": [
        {
          "Objective": "Define the workforce adaptation and training strategies to address risks from job role evolution due to AI adoption."
        }
      ],
      "Fields": [
        {
          "Control": "A.9.4 – Intended use of the AI system",
          "FieldName": "(A.9.4) - Target Users",
          "FieldLabel": "Select the job titles whose daily tasks may be altered by more than 20% due to the AI system",
          "FieldText": "",
          "FieldType": "MultiSelect:Employees/Customers/Analysts/Customer/Supplier/Partner/Regulator"
        },
        {
          "FieldName": "HR-WIA-1.2-CoreAction",
          "FieldLabel": "Identify the primary roles of the AI system relative to human workers",
          "FieldText": "",
          "FieldType": "MultiSelect:Augmentation (assisting human judgment)/Automation (replacing tasks)/Creation (enabling new tasks)"
        },
        {
          "FieldName": "HR-WIA-2.2-AutomationTasks",
          "FieldLabel": "Automated/Eliminated Tasks",
          "FieldText": "List the specific tasks that will be fully automated or eliminated for the affected roles, and the estimated percentage of work time saved across the department.",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "HR-WIA-4.1-MitigationPlan",
          "FieldLabel": "Primary Mitigation Strategy for Displacement",
          "FieldText": "If job displacement is identified, select the primary strategies for the affected workers",
          "FieldType": "MultiSelect:Internal Re-deployment/Transfer/Managed Attrition (No Backfill)/Voluntary Separation Package/External Layoff"
        },
        {
          "FieldName": "HR-WIA-4.2-TrainingProgram",
          "FieldLabel": "Structured Re-skilling Program in Place",
          "FieldText": "Describe the primary strategies to address the affected workers.",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "HR-WIA-4.2-TrainingProgram",
          "FieldLabel": "Structured Re-skilling Program Effectiveness",
          "FieldText": "Describe the Training Effectiveness measures to evaluate the success of the primary strategies to address the affected workers.",
          "FieldType": "TextBox"
        }
      ]
    },
    {
      "StepName": "x(A.5.2, A.5.3, A.5.4) Vulnerable Populations Impact Assessment",
      "Objectives": [
        {
          "Objective": "Define and evaluate the AI system's impacts, risks, and mitigation strategies specific to vulnerable populations affected by deployment."
        }
      ],
      "Fields": [
        {
          "FieldName": "VP-IMP-1.1-PopulationType",
          "FieldLabel": "Select the at-risk group(s) impacted by the AI system",
          "FieldText": "",
          "FieldType": "Option box with values:Children/Elderly/Persons with Disabilities/Economically Disadvantaged/Ethnic Minorities/Other (specify)"
        },
        {
          "FieldName": "VP-IMP-2.1-ImpactType",
          "FieldLabel": "Negitive or positive impacts",
          "FieldText": "Describe the specific ways the AI system could negatively or positively affect the vulnerable population identified.",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "VP-IMP-2.2-SeverityAssessment",
          "FieldLabel": "Rate the severity of identified impacts",
          "FieldText": "",
          "FieldType": "Dropdown box with values:/Low/Medium/High"
        },
        {
          "FieldName": "VP-IMP-2.3-SeverityAssessment",
          "FieldLabel": "Describe the severity of identified impacts",
          "FieldText": "",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "VP-IMP-3.1-MitigationStrategy",
          "FieldLabel": "Describe the main strategies chosen to prevent or reduce negative impacts for the vulnerable population",
          "FieldText": "",
          "FieldType": "MultiSelect:Enhanced Access/Targeted Education & Support/Explicit User Consent/Safeguard Controls/Stakeholder Engagement/Other (specify)"
        },
        {
          "FieldName": "VP-IMP-4.1-StakeholderInclusion",
          "FieldLabel": "Have any of the identified vulnerable populations or their representatives been consulted during the design, development, or testing of the AI system?",
          "FieldText": "",
          "FieldType": "Option box with values:Yes, extensive consultation/Yes, limited consultation/No consultation conducted/Not applicable"
        },
        {
          "FieldName": "VP-IMP-4.2-SeverityAssessment",
          "FieldLabel": "Impact Description",
          "FieldText": "Describe the severity of identified impacts.",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "VP-IMP-3.2-MitigationEffectiveness",
          "FieldLabel": "Post-Deployment Monitoring Plan",
          "FieldText": "Describe the plan for monitoring the AI system's performance and impact on vulnerable populations after deployment. Include key metrics and the frequency of review.",
          "FieldType": "TextBox"
        }
      ]
    },
    {
      "StepName": "x(A.5.2, A.5.3, A.5.5) Environmental Sustainability of AI Systems",
      "Objectives": [
        {
          "Objective": "Define proper environmental assessments, eco-efficient practices, and sustainable lifecycle management controls to minimize ecological impacts."
        }
      ],
      "Fields": [
        {
          "FieldName": "ENV-IA-1.1-EnergySource",
          "FieldLabel": "Specify the primary sources of electricity for the data centers used for training and deployment of the AI system.",
          "FieldText": "",
          "FieldType": "MultiSelect:Primarily Renewable (e.g., Solar, Wind, Hydro)/Regional Grid Mix/Primarily Fossil Fuels/Unknown or Not Specified by Provider"
        },
        {
          "FieldName": "ENV-IA-2.1-ConsumptionMetrics",
          "FieldLabel": "Energy Consumption & Carbon Footprint Estimation",
          "FieldText": "Describe the methodology and provide the estimated energy consumption (e.g., in kWh) and carbon footprint (e.g., in tons of CO2e) for the system's lifecycle (training, validation, and deployment).",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "ENV-IA-3.1-HardwareEOL",
          "FieldLabel": "Select the primary strategies for managing the hardware (e.g., servers, GPUs) at the end of its useful life.",
          "FieldText": "",
          "FieldType": "MultiSelect:Certified E-waste Recycling Program/Component Refurbishment and Reuse/Return to Manufacturer for Disposal/Unmanaged Disposal"
        },
        {
          "FieldName": "ENV-IA-4.1-EfficiencyMeasures",
          "FieldLabel": "Select main strategies for minimizing negative environmental impacts.",
          "FieldText": "",
          "FieldType": "MultiSelect:Renewable Energy Use/Model Size Optimization/Cool Data Management/E-waste Recycling/Sustainable Supply Chain/Other (specify)"
        },
        {
          "FieldName": "ENV-IA-4.2-ImpactLevel",
          "FieldLabel": "Level of Environmental Impact",
          "FieldText": "Rate the degree of environmental burden/benefit (with rationale).",
          "FieldType": "Dropdown box with values:/Low/Medium/High"
        },
        {
          "FieldName": "VP-IMP-2.3-SeverityAssessment",
          "FieldLabel": "Describe the Environmental Impact",
          "FieldText": "Describe the degree of environmental burden/benefit.",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "ENV-IA-5.1-PositiveContribution",
          "FieldLabel": "Contribution to Environmental Sustainability Goals",
          "FieldText": "Does the AI system's intended application directly contribute to positive environmental outcomes (e.g., climate change modeling, energy grid optimization, biodiversity monitoring)? If yes, please describe.",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "ENV-IA-6.1-MonitoringReview",
          "FieldLabel": "Environmental Impact Monitoring and Review",
          "FieldText": "Describe the process and frequency for monitoring and reviewing the environmental performance metrics of the AI system over its lifecycle.",
          "FieldType": "TextBox"
        }
      ]
    },
    {
      "StepName": "AI Act - Section 2: Requirements for High-Risk AI Systems",
      "Objectives": [
        {
          "Objective": "If the AI system is categorised as high-risk according to the AI Acty, define the security requirements for - High-Risk AI Systems."
        }
      ],
      "Fields": [
        {
          "FieldType": "risk",
          "FieldName": "[Article 8] - Non-Compliance with High-Risk AI System Requirements",
          "question": "Is the AI system, or the product incorporating the AI system, subject to other EU regulations, and if so, how is compliance with all applicable regulations, including the AI Act, managed and documented?",
          "controls": [
            {
              "control": "[Art-8][Par-1][1] - Establish and maintain a documented process to ensure and demonstrate that high-risk AI systems comply with the requirements of Section 2 of the AI Act, considering the system's intended purpose and the state of the art in AI.",
              "control_objective": "To ensure that high-risk AI systems meet all legal and technical standards set forth in the AI Act, and to have a clear, auditable trail of compliance."
            },
            {
              "control": "[Art-8][Par-2][2] - For products containing a high-risk AI system that are also subject to other Union harmonisation legislation, ensure the product is fully compliant with all applicable requirements from all relevant regulations.",
              "control_objective": "To achieve comprehensive legal compliance for products that incorporate AI systems and are subject to multiple regulatory frameworks."
            },
            {
              "control": "[Art-8][Par-2][3] - Where applicable, integrate the testing, reporting, and documentation processes required by the AI Act with existing compliance processes under other EU legislation to ensure consistency, avoid duplication, and minimize administrative burden.",
              "control_objective": "To streamline compliance activities, improve efficiency, and ensure a coherent approach to regulatory requirements across different legal instruments."
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "[Article 9] - Inadequate or Ineffective Risk Management",
          "question": "Does the organization have a documented and continuously updated risk management system for its high-risk AI systems that covers the entire lifecycle of the system?",
          "controls": [
            {
              "control": "[Art-9][Par-1][1] - Establish, implement, document, and maintain a comprehensive risk management system for high-risk AI systems.",
              "control_objective": "To ensure a systematic and ongoing process for identifying, evaluating, and mitigating risks associated with high-risk AI systems throughout their lifecycle."
            },
            {
              "control": "[Art-9][Par-2][2] - Implement a continuous iterative risk management process that includes regular systematic reviews and updates, covering identification, analysis, estimation, and evaluation of foreseeable risks.",
              "control_objective": "To proactively manage and adapt to evolving risks by maintaining a dynamic and up-to-date risk management framework."
            },
            {
              "control": "[Art-9][Par-2][3] - Adopt appropriate and targeted risk management measures to address identified risks, including those from post-market monitoring.",
              "control_objective": "To effectively mitigate identified risks through the implementation of specific and relevant control measures."
            },
            {
              "control": "[Art-9][Par-5][4] - Ensure that residual risks, both individual and overall, are acceptable by eliminating or reducing risks as far as technically feasible through design, implementing mitigation measures, and providing information and training.",
              "control_objective": "To reduce the potential for harm to an acceptable level by employing a multi-layered approach to risk mitigation."
            },
            {
              "control": "[Art-9][Par-6][5] - Conduct testing of high-risk AI systems to identify the most appropriate risk management measures and to ensure consistent performance and compliance with requirements.",
              "control_objective": "To validate the effectiveness of risk management measures and ensure the AI system operates as intended."
            },
            {
              "control": "[Art-9][Par-9][6] - Consider the potential adverse impact on persons under the age of 18 and other vulnerable groups when implementing the risk management system.",
              "control_objective": "To provide heightened protection for vulnerable populations who may be disproportionately affected by the AI system."
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "[Article 10] - Poor Data Quality and Governance",
          "question": "Are there appropriate data governance and management practices in place for the training, validation, and testing datasets used by the high-risk AI system to ensure they are relevant, representative, and free of errors and biases?",
          "controls": [
            {
              "control": "[Art-10][Par-2][1] - Implement and document data governance and management practices covering the entire data lifecycle, including design choices, collection, and preparation processes like annotation and cleaning.",
              "control_objective": "To ensure that data used for high-risk AI systems is handled systematically and responsibly, maintaining quality and integrity from collection to use."
            },
            {
              "control": "[Art-10][Par-2][2] - Establish a process to examine data sets for possible biases that could negatively impact fundamental rights, health, or safety, and implement measures to detect, prevent, and mitigate these biases.",
              "control_objective": "To minimize the risk of discriminatory or unfair outcomes and ensure the AI system operates in a manner that is safe and respects fundamental rights."
            },
            {
              "control": "[Art-10][Par-3][3] - Ensure that training, validation, and testing data sets are relevant, sufficiently representative, free of errors, and complete for the system's intended purpose, with appropriate statistical properties.",
              "control_objective": "To build a robust and reliable AI system by using high-quality data that accurately reflects the operational environment and minimizes performance issues."
            },
            {
              "control": "[Art-10][Par-4][4] - Verify that data sets account for the specific geographical, contextual, behavioral, or functional settings in which the high-risk AI system will be used.",
              "control_objective": "To ensure the AI system performs effectively and as intended in its specific operational context, reducing the risk of failures due to environmental mismatches."
            },
            {
              "control": "[Art-10][Par-5][5] - Where strictly necessary for bias detection and correction, process special categories of personal data only with appropriate safeguards, technical limitations, and security measures, ensuring data is deleted after use.",
              "control_objective": "To enable effective bias mitigation while upholding the highest standards of data protection and privacy for sensitive personal information."
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "[Article 12] - Inadequate Traceability and Record-Keeping",
          "question": "Does the organization need to trace the operational history of its high-risk AI system to investigate incidents, audit results, or ensure accountability?",
          "controls": [
            {
              "control": "[Art-12][Par-1][1] - Implement capabilities for the automatic recording of events (logs) while the high-risk AI system is operating.",
              "control_objective": "To ensure a level of traceability of the AI system’s functioning throughout its lifecycle that is appropriate to its intended purpose."
            },
            {
              "control": "[Art-12][Par-2][2] - For high-risk AI systems covered by specific points in Annex III (e.g., biometrics, law enforcement), ensure logging capabilities record the period of each use, the reference database, the input data, and the identity of the persons involved in verifying the results.",
              "control_objective": "To provide detailed operational transparency and accountability for AI systems used in critical public and justice-related applications."
            },
            {
              "control": "[Art-12][Par-4][3] - For AI systems intended for remote biometric identification, ensure logging capabilities record the period of use, reference database, input data, and the identification of the person generating the match.",
              "control_objective": "To enhance auditability and accountability in the use of sensitive remote biometric identification technologies."
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "[Article 13] - Lack of Transparency and Provision of Information to Users",
          "question": "Will users of this AI system need to understand its capabilities, limitations, and the meaning of its outputs to use it safely and effectively?",
          "controls": [
            {
              "control": "[Art-13][Par-1] - Ensure the design of high-risk AI systems allows users to interpret outputs and use the system appropriately.",
              "control_objective": "To enable safe and effective use of the AI system by ensuring user comprehension."
            },
            {
              "control": "[Art-13][Par-2] - Provide clear, complete, and accessible instructions for use with all high-risk AI systems.",
              "control_objective": "To ensure users have the necessary information to operate the AI system correctly and safely."
            },
            {
              "control": "[Art-13][Par-3a] - Include the identity and contact details of the provider and their authorized representative in the instructions for use.",
              "control_objective": "To establish clear lines of communication and accountability for the AI system."
            },
            {
              "control": "[Art-13][Par-3b] - Detail the AI system's characteristics, capabilities, limitations, intended purpose, accuracy, robustness, cybersecurity, and performance metrics in the instructions for use.",
              "control_objective": "To provide a comprehensive understanding of the AI system's operational parameters and performance expectations."
            },
            {
              "control": "[Art-13][Par-3c-e-f] - Specify the necessary hardware resources, expected lifetime, maintenance, and pre-determined changes for operating the AI system in the instructions for use.",
              "control_objective": "To ensure users have the required infrastructure and information to run and maintain the AI system effectively over its lifecycle."
            },
            {
              "control": "[Art-13][Par-3d] - Detail the human oversight measures from Article 14, including technical aids for interpreting system outputs, in the instructions for use.",
              "control_objective": "To facilitate effective human oversight and intervention in the AI system's operation."
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "[Article 14] - Ineffective or Insufficient Human Oversight",
          "question": "Does the AI system operate in a way that requires human monitoring, intervention, or decision-making to prevent or mitigate risks to health, safety, or fundamental rights?",
          "controls": [
            {
              "control": "[Art-14][Par-1] - Design and develop high-risk AI systems with appropriate human-machine interface tools to enable effective oversight by natural persons.",
              "control_objective": "To ensure that a human can effectively monitor and control the AI system while it is in use."
            },
            {
              "control": "[Art-14][Par-2] - Implement human oversight to prevent or minimize risks to health, safety, or fundamental rights, especially those risks that persist after other requirements have been applied.",
              "control_objective": "To provide a final layer of risk mitigation through active human involvement."
            },
            {
              "control": "[Art-14][Par-3] - Ensure human oversight measures are built into the AI system by the provider or are appropriate for implementation by the deployer.",
              "control_objective": "To integrate necessary oversight capabilities either directly into the system or into the operational procedures of the user."
            },
            {
              "control": "[Art-14][Par-4a, 4b, 4c] - Enable assigned human overseers to understand the AI system's capabilities and limitations, monitor for anomalies, and correctly interpret its output, while remaining aware of potential automation bias.",
              "control_objective": "To empower human overseers with the knowledge and awareness needed to make informed judgments about the system's performance."
            },
            {
              "control": "[Art-14][Par-4d, 4e] - Enable assigned human overseers to have the ability to decide not to use the system, override its output, or interrupt its operation via a 'stop' button or similar procedure.",
              "control_objective": "To ensure ultimate human control over the AI system's actions and decisions in any given situation."
            },
            {
              "control": "[Art-14][Par-5] - For remote biometric identification systems, ensure that any identification is verified and confirmed by at least two competent, trained, and authorized natural persons before action is taken.",
              "control_objective": "To increase the reliability and accountability of critical identification tasks performed by AI."
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "[Article 15] - Inadequate Accuracy, Robustness, and Cybersecurity",
          "question": "Will poor AI performance (e.g., incorrect or unreliable outputs) cause business disruption, compliance issues, or customer dissatisfaction?",
          "controls": [
            {
              "control": "[Art-15][Par-1] - Ensure high-risk AI systems are designed and developed to achieve and maintain an appropriate level of accuracy, robustness, and cybersecurity throughout their lifecycle.",
              "control_objective": "To maintain the system's trustworthiness and prevent harm from inaccurate or insecure operation."
            },
            {
              "control": "[Art-15][Par-2] - Encourage the development of benchmarks and measurement methodologies for accuracy and robustness in cooperation with relevant stakeholders.",
              "control_objective": "To establish standardized methods for evaluating and verifying the performance of AI systems."
            },
            {
              "control": "[Art-15][Par-3] - Clearly state the levels of accuracy and the relevant accuracy metrics in the AI system's instructions for use.",
              "control_objective": "To provide transparency to users about the system's expected performance."
            },
            {
              "control": "[Art-15][Par-4] - Design AI systems to be resilient to errors, faults, or inconsistencies, using technical redundancies and fail-safe plans where appropriate, and mitigate risks from biased feedback loops in learning systems.",
              "control_objective": "To ensure the system can handle unexpected situations and maintain stable performance without being negatively influenced by its own outputs."
            },
            {
              "control": "[Art-15][Par-5] - Implement cybersecurity measures to protect high-risk AI systems from unauthorized alteration of their use, outputs, or performance by exploiting vulnerabilities.",
              "control_objective": "To safeguard the AI system against malicious attacks such as data poisoning, model poisoning, and adversarial examples."
            }
          ]
        }
      ]
    }
  ],
  "Phase - Knowledge Base Creation (Offline or Real-Time Data Processing)": [
    {
      "StepName": "xInternal Data Sources",
      "WebFormTitle": "To uphold the principles of data integrity, relevance, currency, and compliance by creating a governed process to identify, review, and approve all proprietary data sources designated for the AI's knowledge base.",
      "Objectives": [
        {
          "Objective": "1. Data Integrity: This principle is about ensuring the information we use is the official source of truth. It means we select data from authoritative systems and explicitly exclude unofficial sources like draft documents, personal notes, or unverified information. This ensures the AI's answers are not just relevant and current, but are based on correct, approved company data."
        },
        {
          "Objective": "2. Data Relevance: This principle ensures that we only feed the AI information related to its designated function. Just as you wouldn't give a pilot a cookbook to fly a plane, we must prevent irrelevant data from polluting the AI's knowledge base. This step establishes a clear scope to keep the AI's answers focused, accurate, and helpful."
        },
        {
          "Objective": "3. Data Currency: An AI's answers are only as reliable as the information it has learned from. This step establishes a process to ensure that only up-to-date, current documents are used, and that obsolete information is explicitly excluded. This prevents the AI from providing answers based on outdated policies, which could lead to significant business risks."
        },
        {
          "Objective": "4. Data Compliance: This is a critical governance checkpoint to protect our company, employees, and customers. Before any data source is approved, it is reviewed to ensure it adheres to privacy laws (like GDPR) and our internal ethics policies. This prevents sensitive personal information or other inappropriate content from being included in the AI's knowledge base."
        }
      ],
      "Fields": [
        {
          "FieldName": "(A.4.2, A.4.3) - Dataset Name",
          "FieldLabel": "Dataset Name",
          "FieldText": "Common or descriptive name of the dataset.",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "(A.4.2, A.4.3) - Description",
          "FieldLabel": "Description",
          "FieldText": "Brief overview of the dataset's content and general purpose.",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "(A.4.2, A.4.3, A.7.5) - Source",
          "FieldLabel": "Where does the data originate from?",
          "FieldText": "",
          "FieldType": "MultiSelect:Internal database/Third-party vendor/Public repository/Synthetic generation"
        },
        {
          "FieldName": "(A.4.2, A.4.3) - Intended Use",
          "FieldLabel": "What is the specific purpose(s) of the data?",
          "FieldText": "",
          "FieldType": "Dropdown box with values:/Training data/Validation data/Test data/Production data"
        },
        {
          "FieldName": "(A.4.2, A.4.3) - Format",
          "FieldLabel": "What is the File format or storage system?",
          "FieldText": "",
          "FieldType": "MultiSelect:CSV/JSON/Parquet/SQL DB"
        },
        {
          "FieldName": "(A.4.2, A.4.3, A.7.3, A.7.2) - Acquisition of data",
          "FieldLabel": "How will the Data be acquired and selected?",
          "FieldText": "",
          "FieldType": "MultiSelect:Extracted from internal company databases (e.g., CRM, ERP)/Sourced from a publicly available dataset/Purchased or licensed from a third-party data provider/Collected directly from users with explicit consent/Scraped from public websites in compliance with terms of service/Streamed from IoT sensors or application logs/Artificially generated (synthetic data)/Selected based on defined quality and relevance criteria/Manually curated by subject matter experts/Sampled to ensure fair representation of subgroups (stratified sampling)/A combination of multiple sources/methods/Other (requires specific documentation)"
        },
        {
          "FieldName": "(A.4.2, A.4.3) - Retention Schedules",
          "FieldLabel": "What is the data Retention schedules based on legal and operational requirements?",
          "FieldText": "",
          "FieldType": "Dropdown box with values:/1 Year/5 Years/10 Years"
        },
        {
          "FieldName": "(A.4.2, A.4.3) - Secure Disposing",
          "FieldLabel": "Secure Disposing",
          "FieldText": "Descripbe the Secure methods for disposing of obsolete or redundant data.",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "(A.4.2, A.4.3) - Approximate Size",
          "FieldLabel": "Approximate Size",
          "FieldText": "What is the Estimated size of the data (e.g., number of records, GB, TB)?",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "(A.4.2, A.4.3) - Access Method",
          "FieldLabel": "How will the data be accessed?",
          "FieldText": "",
          "FieldType": "MultiSelect:S3 bucket path/API endpoint/Database query/Shared drive"
        },
        {
          "FieldName": "(A.4.2, A.4.3) - Owner/Custodian",
          "FieldLabel": "Owner/Custodian",
          "FieldText": "Person or team responsible for the data?",
          "FieldType": "TextBox"
        },
        {
          "FieldType": "risk",
          "FieldName": "Malicious Data Ingestion (Data Poisoning & Indirect Prompt Injection)",
          "RiskDescription": "The risk that malicious actors could deliberately corrupt the internal data sources (e.g., SharePoint, Confluence) that feed the AI's knowledge base. This can be done by inserting false information to mislead users (Data Poisoning) or by embedding hidden commands to hijack the AI's behavior and potentially leak data (Indirect Prompt Injection).",
          "question": "Does the AI system ingest data from internal sources (like wikis, shared drives, or databases) where a number of users have permission to create or edit content?",
          "controls": [
            {
              "control": "DATA-INT-01: Implement and enforce Role-Based Access Control (RBAC) with the principle of least privilege on all source data repositories to restrict write and modify permissions to only authorized personnel.",
              "control_objective": "To prevent unauthorized users from introducing malicious or erroneous content at the source, directly mitigating risks of intentional Data Poisoning.",
              "control_evidence": "A documented list of user roles and their assigned permissions for the data repository; screenshots or configuration exports from the system's access control panel demonstrating the principle of least privilege; and records of periodic access reviews."
            },
            {
              "control": "DATA-INT-02: Enforce mandatory version control with detailed audit logs on all source data repositories. All changes, additions, and deletions must be attributable to a specific user and timestamp.",
              "control_objective": "To ensure a complete, auditable history of all changes to the knowledge base's source data, enabling rapid detection of unauthorized modifications and rollback to a last-known-good state.",
              "control_evidence": "Screenshots of the version control system's settings demonstrating that versioning is active, and a sample commit/change history log showing a clear attribution of changes to a specific user, timestamp, and a description of the change."
            },
            {
              "control": "DATA-INT-03: Establish a mandatory content approval workflow for the addition or significant modification of documents in designated high-sensitivity data sources before they are ingested by the AI system.",
              "control_objective": "To create a formal human-in-the-loop verification gate that ensures the authenticity and appropriateness of critical information, providing a strong defense against both deliberate Data Poisoning and unintentional quality issues.",
              "control_evidence": "Documentation of the content approval process including designated approvers, and sample evidence such as a completed Pull Request with mandatory reviewer approvals, or a change management ticket (e.g., in Jira) with a logged approval signature."
            },
            {
              "control": "DATA-INT-04: Ensure every data chunk processed and stored in the vector database retains immutable metadata linking it directly to its source document, version, and author.",
              "control_objective": "To maintain full data provenance, enabling users and administrators to verify the source of any information provided by the AI and to facilitate the targeted removal of compromised data if a poisoning incident is discovered.",
              "control_evidence": "A sample query output from the vector database displaying a data chunk alongside its associated metadata fields (e.g., source_document_name, version_id, ingestion_timestamp), and a code snippet from the data ingestion pipeline explicitly demonstrating how this metadata is extracted and attached."
            }
          ]
        }
      ]
    },
    {
      "StepName": "xData Processing Pipeline (Vectorise proprietary data)",
      "WebFormTitle": "To uphold the integrity and trustworthiness of the AI's knowledge base by ensuring all source data is accurately extracted, thoroughly cleaned of irrelevant artifacts, and logically chunked for optimal processing.",
      "Objectives": [
        {
          "Objective": "1. Accurate Extraction: This is the first and most crucial step to ensure the AI learns from the correct information. It involves carefully reading the source documents, including complex formats like scanned PDFs, to create a perfect digital text copy. An error here could be like the AI misreading a word, which could alter the meaning of a key fact."
        },
        {
          "Objective": "2. Thorough Cleaning: Documents often contain digital noise that isn't part of the core knowledge, such as page numbers, headers, footers, or website navigation links. This step acts as a filter, removing this irrelevant information so the AI can focus purely on the valuable content, leading to clearer and more relevant answers."
        },
        {
          "Objective": "3. Logical Chunking: An AI cannot process an entire 100-page document at once to answer a single question. This step intelligently breaks down long documents into smaller, bite-sized, and contextually complete paragraphs or ideas. This is like creating a perfectly indexed and bookmarked version of the knowledge, making it possible for the AI to quickly find the single most relevant piece of information later on."
        },
        {
          "Objective": "4. Fairness and Bias: AI systems are designed, developed, and tested with defined fairness objectives to prevent discriminatory."
        },
        {
          "Objective": "5. Transparency and Explainability: Are AI are systems designed, developed and tested to provide understandable and sufficient information about its decisions."
        }
      ],
      "Fields": [
        {
          "FieldType": "plan",
          "FieldName": "(A.7.6) Sensitive Data Preparation Plan",
          "PlanObjective": "Ensure that raw sensitive data is anonymised before it is used by the AI system.",
          "PlanCriteria": [
            {
              "criteria": "ANON-PSEUD - 01: Direct personal identifiers must be pseudonymized using established techniques such as masking, tokenization, or hashing.",
              "control_objective": "To obscure or replace sensitive data elements in a reversible or non-reversible way, preventing direct identification of individuals while preserving data utility.",
              "criteria_evidence": "Data processing scripts or configuration files demonstrating the application of pseudonymization techniques. A comparative data sample showing data before and after the transformations, with sensitive fields appropriately masked, tokenized, or hashed."
            },
            {
              "criteria": "ANON-MIN - 01: The principle of data minimization must be applied by reducing data precision and removing unnecessary features.",
              "control_objective": "To reduce the risk of re-identification and adhere to data privacy principles by ensuring only essential data with the minimum required level of detail is processed.",
              "criteria_evidence": "The data schema for the final processed dataset, confirming the absence of pruned columns. A comparative data sample showing data before and after generalization has been applied to relevant fields."
            },
            {
              "criteria": "ANON-INT - 01: The dataset must be free of duplicate records to ensure its integrity and quality.",
              "control_objective": "To prevent data skew, improve processing efficiency, and ensure that each data point is unique, leading to a more reliable and clean dataset for the AI system.",
              "criteria_evidence": "Logs or reports from the data preparation script that quantify the number of duplicate records identified and removed. A data sample demonstrating the dataset before and after the de-duplication process."
            }
          ],
          "PlanSteps": [
            {
              "step": "ANON-PSEUD-TEST-01: Apply masking to a sample of raw data and verify that sensitive identifiers are correctly obscured with generic characters.",
              "step_objective": "To confirm that the masking technique is correctly implemented to limit the exposure of sensitive identifiers, fulfilling a component of criterion ANON-PSEUD-01."
            },
            {
              "step": "ANON-PSEUD-TEST-02: Apply tokenization to a sample of raw data and verify that sensitive elements are replaced with non-sensitive tokens.",
              "step_objective": "To validate that tokenization effectively substitutes sensitive values, reducing risk while meeting the requirements of criterion ANON-PSEUD-01."
            },
            {
              "step": "ANON-PSEUD-TEST-03: Apply a salted cryptographic hash function (e.g., SHA-256) to a sensitive field and verify that the output is a fixed-size, irreversible string.",
              "step_objective": "To ensure that data is protected using a one-way cryptographic method, satisfying the hashing requirement of criterion ANON-PSEUD-01."
            },
            {
              "step": "ANON-MIN-TEST-01: Apply generalization to a sample of data (e.g., age, date) and verify that the precision is correctly reduced to a less specific category (e.g., age range, year).",
              "step_objective": "To test that generalization is effectively reducing the risk of re-identification, providing evidence for criterion ANON-MIN-01."
            },
            {
              "step": "ANON-MIN-TEST-02: Execute the feature pruning script and verify that the specified non-essential columns are removed from the output dataset.",
              "step_objective": "To confirm adherence to the data minimization principle by eliminating superfluous data, as required by criterion ANON-MIN-01."
            },
            {
              "step": "ANON-INT-TEST-01: Process a sample dataset containing known duplicates and verify that the de-duplication script correctly identifies and removes the redundant records.",
              "step_objective": "To validate the effectiveness of the de-duplication process, ensuring the final dataset meets the integrity standard of criterion ANON-INT-01."
            }
          ]
        }
      ]
    },
    {
      "StepName": "Indexing and storing company's proprietary data",
      "WebFormTitle": "To uphold the principles of data confidentiality, integrity, and availability for all information stored in the AI's knowledge base by implementing comprehensive encryption, strict access controls, and robust disaster recovery protocols.",
      "Objectives": [
        {
          "Objective": "1. Data Confidentiality (Encryption): This ensures that the company's proprietary information, which has been converted into an AI-readable numerical format (vectorized), is unreadable to unauthorized parties. Think of this as storing the AI's knowledge in a digital safe (encryption at rest) and using a secure, armored courier when moving it between systems (encryption in transit)."
        },
        {
          "Objective": "2. Data Integrity and Access Control: This establishes strict controls over who can add, modify, or delete information in the AI's knowledge base. It prevents both accidental corruption and malicious tampering (data poisoning) by ensuring only authorized personnel or automated processes can manage the data, with all actions logged for auditing."
        },
        {
          "Objective": "3. Availability and Resilience: This guarantees that the AI's knowledge base, now a critical business asset, is protected against loss. It involves implementing robust backup and disaster recovery plans, ensuring the system can be restored quickly and reliably in the event of a technical failure, cyber-attack, or other disruption."
        }
      ],
      "Fields": []
    },
    {
      "StepName": "Connecting Internal Data <-> the Processing Pipeline",
      "WebFormTitle": "To ensure the secure and confidential transfer of all proprietary data from internal sources into the data processing pipeline by enforcing strong encryption for all data in transit.",
      "Objectives": [
        {
          "Objective": "1. Offline Path (Knowledge Base Building): Systematically transfer and process large volumes of internal documents to build the core, persistent knowledge base. This involves extracting, chunking, and embedding data to create a comprehensive, searchable vector index of the company's proprietary information."
        },
        {
          "Objective": "2. Real-Time Path (Query Augmentation): Provide an on-the-fly processing capability for user-uploaded documents. Data is processed immediately and stored in a temporary, session-specific index, allowing the system to answer questions on new information without altering the permanent knowledge base."
        },
        {
          "Objective": "3. Data in Transit Security: Protect all data during transfer between internal sources and the processing pipeline. This involves mandating strong encryption (e.g., TLS) for both offline bulk transfers and real-time user uploads to prevent eavesdropping and data interception, ensuring confidentiality is maintained throughout the ingestion process."
        }
      ],
      "Fields": []
    }
  ],
  "Phase ????": [],
  "Phase Chat With Your Document (Real-Time Data Processing)": [
    {
      "StepName": "User Interface",
      "WebFormTitle": "To uphold system integrity at the point of user interaction by validating user identity, sanitizing all submitted queries to neutralize potential threats, and ensuring non-repudiation through detailed audit logs.",
      "Objectives": [
        {
          "Objective": "1. Identity and Access: Establishes that the system must know who is making the request, which is the foundation for access control."
        },
        {
          "Objective": "2. Data Integrity: Covers the need to inspect the incoming data itself for malicious content, such as prompt injection, code, or other attacks that could compromise the system."
        },
        {
          "Objective": "3. Audibility and Non-repudiation: Creating an immutable record that proves a specific user performed a specific action, and they cannot later deny it. This is critical for security investigations and accountability."
        }
      ],
      "Fields": [
        {
          "FieldType": "risk",
          "FieldName": "LLM01 Prompt Injection",
          "question": "Will external users or other systems provide text prompts or instructions to the AI model?",
          "controls": [
            {
              "control": "[LLM01][1] - All user inputs within the UI must be validated to prevent the injection of malicious code.",
              "control_objective": "Prevent attackers from exploiting vulnerabilities in the UI to inject malicious code and compromise the AI system.",
              "control_evidence": "Unit test results demonstrating the rejection of malicious payloads (e.g., XSS, command injection strings)."
            },
            {
              "control": "[LLM01][2] - Implement input sanitization techniques to remove harmful characters from user inputs.",
              "control_objective": "Further mitigate the risk of malicious code injection attempts through the UI.",
              "control_evidence": "Code snippets showing the use of a sanitization library or function. Test cases with logs that display the 'before' and 'after' state of user inputs containing harmful characters."
            },
            {
              "control": "[LLM01][4] - Encrypt all sensitive data transmitted through APIs.",
              "control_objective": "Protect sensitive data from unauthorised interception or tampering during communication through APIs.",
              "control_evidence": "Web server or API gateway configuration files enforcing TLS 1.2 or higher. A report from an external security scanner (e.g., Qualys SSL Labs) confirming a strong HTTPS implementation."
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "LLM02 Insecure Output Handling",
          "question": "Will the AI output be used in other systems, documents, or communications without being verified first?",
          "controls": [
            {
              "control": "[LLM02][1] - Ensure that sensitive or IP data is not exposed in AI system outputs.",
              "control_objective": "To mitigate the risk of insecure output handling by treating LLM-generated outputs as potentially untrusted.",
              "control_evidence": "Implementation of an output filtering or sanitization layer that scans the LLM's response for sensitive data patterns (e.g., using regex or a DLP service) before it is sent to the user or downstream systems. Test results demonstrating that the filter successfully redacts or blocks known sensitive information or intellectual property."
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "LLM04 Model Denial of Service",
          "question": "Could this AI system be overwhelmed or misused in a way that impacts availability?",
          "controls": [
            {
              "control": "[LLM04][3] - Enforce API rate limits to restrict the number of requests an individual user or IP address can make within a specific timeframe.",
              "control_objective": "To control the rate of requests and prevent overwhelming the LLM with a high volume of concurrent requests.",
              "control_evidence": "Screenshots of the API gateway configuration, relevant code snippets defining the rate limits, or test results showing that requests are blocked after the limit is exceeded."
            },
            {
              "control": "[LLM04][4] - Limit the number of queued actions and the number of total actions in a system reacting to LLM responses.",
              "control_objective": "To prevent the accumulation of excessive workload and ensure that the system can effectively process LLM responses without becoming overwhelmed.",
              "control_evidence": "Configuration files from the task queue system (e.g., Celery, RabbitMQ), application code setting queue size or concurrency limits, or architectural diagrams illustrating these constraints."
            },
            {
              "control": "[LLM04][5] - Continuously monitor the resource utilisation of the LLM to identify abnormal spikes or patterns that may indicate a DoS attack.",
              "control_objective": "To detect and respond to anomalous resource usage patterns indicative of a denial of service attack on the LLM.",
              "control_evidence": "Screenshots from the monitoring dashboard (e.g., Grafana, Datadog) showing resource utilisation graphs, copies of the alert configurations for abnormal spikes, and the incident response procedure for such alerts."
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "LLM05: Supply Chain Vulnerabilities",
          "question": "Does the AI system depend on third-party libraries, tools, or models you haven’t vetted?",
          "controls": [
            {
              "control": "[LLM05][2] - Only use reputable plugins that have been tested for application requirements.",
              "control_objective": "Minimise plugin-related vulnerabilities.",
              "control_evidence": "A documented plugin vetting process, test results from plugin security assessments, and a list of approved plugins."
            },
            {
              "control": "[LLM05][4] - Maintain an up-to-date inventory using a Software Bill of Materials (SBOM).",
              "control_objective": "Track and manage components.",
              "control_evidence": "The current SBOM document for the application, evidence of a process for regularly updating the SBOM, and change logs."
            },
            {
              "control": "[LLM05][7] - Implement sufficient monitoring and a robust patching policy.",
              "control_objective": "Maintain system security and component currency.",
              "control_evidence": "The documented patching policy, vulnerability scan reports, and change management records demonstrating timely patch application."
            }
          ]
        }
      ]
    },
    {
      "StepName": "RAG Orchestrator",
      "WebFormTitle": "To ensure the secure and appropriate handling of user queries and retrieved data by enforcing strict access controls, mitigating complex inference-based attacks, and maintaining a detailed audit trail for accountability and non-repudiation.",
      "Objectives": [
        {
          "Objective": "1. Secure Workflow Management: The orchestrator acts as the central logic unit, managing the entire RAG process from receiving a user's query to retrieving data and generating a final, context-aware response."
        },
        {
          "Objective": "2. Access Control Enforcement: It must rigorously check a user's permissions *before* retrieving data from the knowledge base. This is the most critical function to prevent access control bypasses, where a user could otherwise access confidential information beyond their privilege level."
        },
        {
          "Objective": "3. Inference Attack Mitigation: The system must be designed to detect and counter inference attacks, where a user attempts to piece together sensitive information by asking numerous simple, seemingly harmless questions. This involves monitoring query patterns and applying appropriate safeguards."
        },
        {
          "Objective": "4. Audit and Accountability: To create an immutable record of all interactions, including user queries, data access events, and generated responses. This establishes a comprehensive audit trail critical for security monitoring, forensic analysis, and ensuring non-repudiation."
        }
      ],
      "Fields": [
        {
          "FieldType": "risk",
          "FieldName": "Insufficient Audit Trails and Logging",
          "question": "Will you need to track who accessed the AI system, what it did, and when — for accountability or regulatory purposes?",
          "controls": [
            {
              "control": "[RAG-ORCH][1] - The orchestrator must integrate with the Identity and Access Management (IAM) system to retrieve user permissions and enforce data access policies *before* querying the vector database or knowledge base.",
              "control_objective": "To prevent unauthorized data access by ensuring a user's privileges are validated at the point of orchestration, before any data retrieval operation is initiated.",
              "control_evidence": "Sequence diagram showing the IAM check in the RAG workflow. Code review demonstrating the API call to the IAM system and the conditional logic that blocks unauthorized queries. Integration test results showing that queries from users with insufficient permissions are denied access to specific documents."
            },
            {
              "control": "[RAG-ORCH][2] - Implement query velocity and complexity monitoring within the orchestrator. The system must flag or block users exhibiting anomalous query patterns (e.g., high-frequency, low-semantic-variance queries) indicative of an inference attack.",
              "control_objective": "To detect and mitigate attempts to reconstruct sensitive information by analyzing patterns of user queries over time.",
              "control_evidence": "Configuration files defining thresholds for rate limiting and query complexity. Logs from a security test showing a simulated inference attack being detected and the user's session being flagged or throttled. A runbook detailing the response procedure for a flagged account."
            },
            {
              "control": "[RAG-ORCH][3] - The orchestrator must generate structured, immutable audit logs for every user interaction. Logs must include a unique transaction ID, user identifier, timestamp, the full user query, the IDs of the documents retrieved from the knowledge base, and the final LLM-generated response.",
              "control_objective": "To create a complete and reliable trail of activity for security forensics, accountability, and non-repudiation, ensuring all critical stages of the RAG process are recorded.",
              "control_evidence": "Sample of the structured audit log (e.g., in JSON format) showing all required fields are populated. Architecture diagram illustrating the logging pipeline to a secure, write-once storage system (e.g., AWS CloudWatch Logs with object lock). Penetration test report confirming that logs cannot be tampered with or deleted by unauthorized users."
            },
            {
              "control": "[RAG-ORCH][4] - The orchestrator's state transitions and data handling between components (User Query -> Vector DB -> LLM) must be defined within a secure, isolated service. All inter-component communication must use authenticated and encrypted channels (e.g., mTLS).",
              "control_objective": "To protect the integrity and confidentiality of data as it moves through the RAG pipeline and prevent attackers from manipulating the workflow logic.",
              "control_evidence": "Network architecture diagram showing mTLS enforced between microservices. Infrastructure-as-code (e.g., Terraform, Kubernetes YAML) configuration files specifying the network policies and secret management for certificates. Vulnerability scan reports for the orchestrator service container and its dependencies."
            }
          ]
        }
      ]
    },
    {
      "StepName": "Generic LLM",
      "WebFormTitle": "To enforce strict operational security for the self-hosted LLM by isolating its network access and implementing governed MLOps deployment workflows.",
      "Objectives": [
        {
          "Objective": "1. Resource Isolation: To contain the LLM's operational environment by strictly controlling and restricting its access to all network resources, internal services, and APIs, preventing it from performing unauthorized actions."
        },
        {
          "Objective": "2. Governed Deployment: To ensure model integrity and security by implementing automated MLOps deployment pipelines that include robust governance, tracking, and formal approval workflows for all changes."
        }
      ],
      "Fields": []
    },
    {
      "StepName": "User Interface <->  RAG Orchestrator",
      "WebFormTitle": "To ensure that all queries and responses between the user interface and the RAG orchestrator are subject to strict, user-specific access control enforcement, preventing unauthorized data disclosure.",
      "Objectives": [
        {
          "Objective": "1. Enforce Access Control: The primary vulnerability is the failure to apply a user's specific permissions. This objective is to validate that the orchestrator, despite its high-level privileges, strictly filters all knowledge base queries and results according to the individual user's access rights."
        },
        {
          "Objective": "2. Prevent Data Leakage: Directly addresses the risk of a major data breach by ensuring that sensitive documents retrieved by the orchestrator are never passed back to an unauthorized user through the communication channel."
        },
        {
          "Objective": "3. Secure Communication Channel: To protect the integrity and confidentiality of the raw question and the final answer as they are transmitted between the user interface and the orchestrator, preventing interception or tampering."
        }
      ],
      "Fields": []
    },
    {
      "StepName": "RAG Orchestrator <->  Vector Database (Retrieval)",
      "WebFormTitle": "To retrieve relevant text chunks from the knowledge base in response to a user query while mitigating the risk of unintentionally over-fetching and exposing sensitive data.",
      "Objectives": [
        {
          "Objective": "1. Contextual Retrieval: To query the vector database with the user's vectorized input to find and return the most semantically relevant information needed to generate an accurate answer."
        },
        {
          "Objective": "2. Mitigate Data Over-fetching: To address the primary risk of retrieving document chunks that contain the answer but also include collateral sensitive data, such as Personally Identifiable Information (PII)."
        },
        {
          "Objective": "3. Prevent Sensitive Data Exposure: To ensure that unintentionally retrieved confidential details are not passed on to the LLM, thus preventing their potential exposure in the final generated answer."
        }
      ],
      "Fields": []
    },
    {
      "StepName": "RAG Orchestrator <-> Generic LLM (Augmented Prompt & LLM Call)",
      "WebFormTitle": "To generate a coherent, fact-based answer by sending a context-rich prompt to the LLM while mitigating risks of prompt injection and sensitive data leakage.",
      "Objectives": [
        {
          "Objective": "1. Prompt Augmentation: To combine the user's original question with the retrieved text chunks to create a single, comprehensive 'augmented prompt' for the LLM."
        },
        {
          "Objective": "2. Mitigate Prompt Injection: To defend against malicious instructions hidden within the user's query or retrieved data that could hijack the LLM and cause it to perform unintended actions."
        },
        {
          "Objective": "3. Prevent Data Leakage: To ensure the LLM does not inadvertently include sensitive PII or other confidential information from the context in its final answer, preventing the exposure of private data."
        }
      ],
      "Fields": []
    },
    {
      "StepName": "(A.4.7) AI Lifecycle Phase requirements - Generation",
      "WebFormTitle": "To generate a coherent, fact-based answer by sending an augmented prompt to the LLM, while safeguarding against prompt injection and preventing the leakage of sensitive data.",
      "Objectives": [
        {
          "Objective": "1. Augmented Prompt Construction: To combine the user's question with the retrieved text chunks into a single, comprehensive 'augmented prompt' and send it to the LLM for final answer generation."
        },
        {
          "Objective": "2. Mitigate Prompt Injection: To defend against vulnerabilities where a malicious instruction hidden in the user's query or retrieved data could hijack the LLM, causing it to perform unintended actions."
        },
        {
          "Objective": "3. Prevent Data Leakage: To ensure the LLM does not include sensitive PII or confidential notes from the provided context in its final answer, thus preventing inadvertent exposure of private information."
        }
      ],
      "Fields": []
    }
  ],
  "Phase Deployment": [
    {
      "StepName": "x(A.6.2.5) AI Lifecycle Phase requirements - Deployment",
      "WebFormTitle": "To orchestrate a secure and compliant AI system deployment by ensuring component integrity through secure packaging, formalizing a deployment plan, and verifying all prerequisite conditions are met prior to system activation.",
      "Objectives": [
        {
          "Objective": "1. Deployment Planning: To formalize the deployment strategy by documenting the complete process, including timelines, resource allocation, technical steps, and rollback procedures, ensuring a predictable and controlled rollout."
        },
        {
          "Objective": "2. Prerequisite Verification: To confirm that the AI system meets all established technical, security, ethical, and performance benchmarks through rigorous pre-deployment testing and validation, preventing the release of a non-compliant or unstable system."
        },
        {
          "Objective": "3. Secure Packaging: To guarantee the integrity and security of the deployable artifacts by ensuring that all components, particularly container images, are built from trusted sources, scanned for vulnerabilities, and hardened before deployment."
        }
      ],
      "Fields": [
        {
          "FieldType": "risk",
          "FieldName": "Insecure AI component Packaging",
          "question": "Are any components of the AI system be packaged in containers?",
          "controls": [
            {
              "control": "PROTE.02 Configure development tools, orchestrators, and container runtimes to exclusively use encrypted channels when connecting to registries.",
              "control_objective": "To safeguard the integrity and confidentiality of container images and code during transit to and from registries.",
              "control_evidence": "Configuration files for development tools, orchestrators (e.g., Kubernetes), and container runtimes demonstrating the use of TLS-encrypted connections (e.g., registry URLs starting with 'https://')."
            },
            {
              "control": "PROTE.03 Implement time-triggered pruning of registries to remove unsafe or vulnerable container images.",
              "control_objective": "To maintain the security and integrity of container images in registries by eliminating outdated and vulnerable images.",
              "control_evidence": "Configuration of the automated pruning job (e.g., a CronJob manifest) and execution logs showing that vulnerable or old images have been successfully removed."
            },
            {
              "control": "PROTE.04 Enforce read/write access control for registries containing proprietary or sensitive container images.",
              "control_objective": "To restrict unauthorised access and modifications to container images stored in registries.",
              "control_evidence": "Screenshots or configuration exports of the registry's Role-Based Access Control (RBAC) settings, showing defined user roles and their permissions for specific repositories."
            },
            {
              "control": "PROTE.05 Control access to cluster-wide administrative accounts using strong authentication methods like multifactor authentication and single sign-on to existing directory systems where applicable.",
              "control_objective": "To ensure secure and controlled access to administrative accounts within the cluster.",
              "control_evidence": "Identity Provider (IdP) configuration showing MFA is enforced for the cluster administrator group, and the orchestrator's authentication configuration file pointing to the SSO provider (e.g., OIDC or SAML settings)."
            },
            {
              "control": "PROTE.06 Implement network isolation protocols that configure orchestrators to segregate network traffic based on sensitivity levels.",
              "control_objective": "To maintain distinct network environments for different levels of data sensitivity, enhancing overall network security.",
              "control_evidence": "Copies of network policy manifests (e.g., Kubernetes 'NetworkPolicy' YAML files) or firewall rules that define and enforce network segmentation."
            },
            {
              "control": "PROTE.07 Deploy policies that configure orchestrators to isolate deployments to specific sets of hosts based on security requirements or sensitivity levels.",
              "control_objective": "To ensure that deployments are conducted on secure, appropriate hosts in alignment with their security needs.",
              "control_evidence": "Orchestrator deployment configurations (e.g., YAML files) showing the use of node selectors, taints, and tolerations to restrict pods to specific nodes."
            },
            {
              "control": "PROTE.12 Implement mechanisms to reduce Host Operating System (OS) attack surfaces, including\na) using container-specific OSs with unnecessary services disabled (e.g., print spooler)\nb) employing read-only file systems\nc) regularly updating and patching OSs and lower-level components like the kernel\nd) validating versioning of components for base OS management and functionality.",
              "control_objective": "To minimise vulnerabilities and enhance the security of the host operating systems used in containerised environments.",
              "control_evidence": "Patch management reports, host configuration files showing a minimal OS install (e.g., CIS hardened image), disabled services, and read-only file system settings. A Software Bill of Materials (SBOM) for the host OS."
            },
            {
              "control": "PROTE.13 Establish mechanisms to prevent the mixing of containerised and non-containerised workloads on the same host instance.",
              "control_objective": "To segregate containerised workloads from non-containerised ones, reducing the risk of cross-contamination and attacks.",
              "control_evidence": "Host inventory documentation or orchestrator node labels and taints that dedicate specific hosts exclusively to containerised workloads."
            },
            {
              "control": "PROTE.14 Implement mechanisms to enforce minimal file system permissions for all containers, ensuring that they cannot mount sensitive directories on the host's file system.",
              "control_objective": "To restrict container access to the host's file system, preventing unauthorised access or manipulation of sensitive data.",
              "control_evidence": "Pod security policies or admission controller configurations that enforce restrictions on hostPath volumes. Deployment manifests showing the container 'securityContext' is configured with minimal permissions."
            },
            {
              "control": "PROTE.16 Ensure that only images from trusted image stores and registries are permitted to run in the environment.",
              "control_objective": "To safeguard the environment from untrusted or potentially harmful container images.",
              "control_evidence": "Configuration of an admission controller (e.g., OPA Gatekeeper, Kyverno) that implements a policy to only allow images from an approved list of registries."
            },
            {
              "control": "PROTE.17 Utilise network policies and firewall rules to restrict container network access and isolate sensitive workloads.",
              "control_objective": "To enhance network security by controlling container access and isolating sensitive workloads.",
              "control_evidence": "Network policy manifests (e.g., Kubernetes 'NetworkPolicy') or service mesh configurations (e.g., Istio 'AuthorizationPolicy') that define granular ingress and egress rules for pods."
            },
            {
              "control": "PROTE.18 Adopt the use of immutable containers, which cannot be altered post-deployment, wherever feasible.",
              "control_objective": "To prevent runtime attacks by ensuring container configurations remain unchanged after deployment.",
              "control_evidence": "Deployment manifests showing the container's root file system is set to read-only ('readOnlyRootFilesystem: true'). CI/CD pipeline configuration demonstrating that changes are deployed by building and shipping a new image."
            },
            {
              "control": "PROTE.19 Implement security measures for APIs, including robust API authentication mechanisms (e.g., OAuth 2.0, API keys), fine-grained access controls, and rate limiting to protect against abuse.",
              "control_objective": "To ensure the secure operation of APIs",
              "control_evidence": "API gateway configuration files or screenshots demonstrating the enforcement of authentication, authorisation (e.g., access control lists), and rate-limiting policies."
            },
            {
              "control": "PROTE.20 Images should be configured to run as non-privileged users.",
              "control_objective": "To enhance security by minimising the potential impact of a security breach from a containerised environment.",
              "control_evidence": "The 'Dockerfile' showing the 'USER' instruction is used. The deployment manifest showing the 'securityContext' specifies 'runAsNonRoot: true' and a non-zero 'runAsUser' ID."
            },
            {
              "control": "PROTE.21 Secrets should be stored outside of images and provided dynamically at runtime as needed.",
              "control_objective": "To protect sensitive information like credentials and keys by managing them securely and separately from container images.",
              "control_evidence": "Review of the 'Dockerfile' to confirm no secrets are present. Orchestrator manifests showing that secrets are mounted from a secure source (e.g., Kubernetes Secrets, HashiCorp Vault) at runtime."
            },
            {
              "control": "PROTE.22 Implement security policies and access controls at both the container and host levels to restrict unauthorised access and privilege escalation.",
              "control_objective": "To enhance container and host security by limiting access and preventing unauthorised privilege escalation.",
              "control_evidence": "Host-level AppArmor or SELinux profiles. Container-level pod security standards or custom admission controller policies that restrict privileged operations."
            },
            {
              "control": "PROTE.23 Utilise built-in security features of your containerisation platform.",
              "control_objective": "To leverage platform-specific security features to enhance the security posture of containerised applications.",
              "control_evidence": "A document or report detailing the enabled platform-specific security features, such as Kubernetes Pod Security Standards, Security Contexts, and RBAC configurations."
            },
            {
              "control": "PROTE.24 Mechanisms exist to implement resource limitations to prevent containers from consuming excessive resources and potentially causing a Denial of Service (DoS) attack.",
              "control_objective": "To prevent containers from over-utilising system resources, thereby safeguarding against resource exhaustion and DoS attacks.",
              "control_evidence": "Deployment manifests (e.g., Kubernetes pod spec) showing that CPU and memory requests and limits are defined for all containers."
            }
          ]
        }
      ]
    },
    {
      "StepName": "(A.8.4) Communication of incidents",
      "Objectives": [
        {
          "Objective": "Define and document the information, reporting, and incident communication plan for all internal and external interested parties."
        }
      ],
      "Fields": [
        {
          "FieldName": "(A.8.4) - Incident Communication Plan - Data Breach",
          "FieldLabel": "Data Breach",
          "FieldText": "Describe how incidents related to \"Unintended exposure of training data\" will be comunicated.",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "(A.8.4) - Incident Communication Plan - Model Misuse",
          "FieldLabel": "Model Misuse",
          "FieldText": "Describe how incidents related to \"AI model used outside intended scope\" will be comunicated.",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "(A.8.4) - Incident Communication Plan - Model Failure",
          "FieldLabel": "Model Failure",
          "FieldText": "Describe how incidents related to \"False predictions causing harm\" will be comunicated.",
          "FieldType": "TextBox"
        }
      ]
    },
    {
      "StepName": "(A.8.2) AI System Documentation and User Information",
      "WebFormTitle": "To provide all users and stakeholders with clear, comprehensive, and accessible information required for the safe, effective, and responsible use of the AI system.",
      "Objectives": [
        {
          "Objective": "1. Clarity and Accessibility: To ensure all documentation is written in plain language, is well-structured, and is tailored to the technical level of the intended audience, from end-users to technical administrators."
        },
        {
          "Objective": "2. Comprehensive Operational Guidance: To provide clear, step-by-step instructions for all system functionalities, including standard operating procedures and guidance for handling common errors or unexpected outputs."
        },
        {
          "Objective": "3. Transparency of Capabilities and Limitations: To explicitly document the AI system's intended use, its known limitations, and potential risks. This includes describing what the system is not designed to do to prevent misuse."
        },
        {
          "Objective": "4. Communication of User Responsibilities: To clearly communicate the 'Objectives for responsible use' to all users, ensuring they understand their role and accountability in operating the system ethically and in accordance with its intended purpose."
        }
    ],
    "Fields": []
	}
  ],
  "Phase Post deployment verification": [
    {
      "StepName": "x(A.6.2.4) AI Systems verifications",
      "Objectives": [
        {
          "Objective": "Document verification and validation measures for the AI system and specify criteria for their use."
        }
      ],
      "Fields": [
        {
          "FieldType": "plan",
          "FieldName": "(A.12.1.2, A.14.2.5) - Performance & Load Test Plan",
          "PlanObjective": "To validate that the AI system meets its defined non-functional requirements for performance, stability, and scalability under realistic load conditions.",
          "PlanCriteria": [
            {
              "criteria": "PERF-TEST-01: The system's average API response time must remain below [e.g., 200ms] and the 95th percentile response time below [e.g., 500ms] during peak load simulation.",
              "control_objective": "To ensure a responsive user experience during high-traffic periods.",
              "criteria_evidence": "The final load test report containing time-series graphs of P50 and P95 latency metrics, demonstrating compliance with the defined Non-Functional Requirements (NFRs)."
            },
            {
              "criteria": "PERF-TEST-02: The system's server-side error rate must not exceed [e.g., 0.1%] of total requests during the peak load test.",
              "control_objective": "To confirm system stability and prevent service degradation under stress.",
              "criteria_evidence": "A summary table in the load test report showing the total number of requests, the number of failed requests, and a calculated error rate below the defined threshold."
            },
            {
              "criteria": "PERF-TEST-03: CPU and memory utilization for all system components must remain below a sustainable threshold (e.g., 80% average) without memory leaks throughout the duration of the load test.",
              "control_objective": "To ensure the system has adequate resource capacity and operates efficiently without risk of crashing due to resource exhaustion.",
              "criteria_evidence": "Monitoring dashboard screenshots (e.g., from Grafana, Datadog) or exported metrics showing CPU and memory usage over the test duration, confirming they stay within the acceptable range."
            }
          ],
          "PlanSteps": [
            {
              "step": "PLT-EXEC-01: Define and document specific Non-Functional Requirements (NFRs) for performance, including target response times (P50, P95), maximum acceptable error rate, and resource utilization thresholds.",
              "step_objective": "To establish clear, quantitative success criteria for the load tests, directly informing the benchmarks needed to pass criteria PERF-TEST-01, 02, and 03."
            },
            {
              "step": "PLT-EXEC-02: Develop automated test scripts using a load testing tool (e.g., JMeter, k6, Gatling) to simulate realistic user workflows and data processing volumes, creating profiles for baseline, peak, and stress load.",
              "step_objective": "To create a repeatable and accurate simulation of real-world user traffic, ensuring the test conditions are valid for assessing system performance."
            },
            {
              "step": "PLT-EXEC-03: Provision a dedicated, production-like test environment and configure monitoring tools to capture key performance indicators (KPIs) including latency, error rates, CPU, and memory usage for all services.",
              "step_objective": "To ensure that test results are accurate and representative of production performance, and that all necessary data is collected for analysis."
            },
            {
              "step": "PLT-EXEC-04: Execute the load test profiles against the test environment, starting with a baseline and gradually ramping up to peak and stress levels, while actively monitoring system health.",
              "step_objective": "To systematically apply stress to the system in a controlled manner to gather performance data across different load scenarios."
            },
            {
              "step": "PLT-EXEC-05: Aggregate and analyze the collected metrics, comparing the results against the predefined NFRs. Identify any performance bottlenecks or components that fail to meet the criteria.",
              "step_objective": "To quantitatively evaluate the system's performance against the success criteria (PERF-TEST-01, 02, 03) and pinpoint areas for optimization."
            },
            {
              "step": "PLT-EXEC-06: Compile a comprehensive load testing report summarizing the methodology, NFRs, test results (including graphs and tables), and a final conclusion on whether the system passed the performance benchmarks.",
              "step_objective": "To produce the final piece of evidence required by all criteria, providing a clear record of the test outcomes for stakeholder review and approval."
            }
          ]
        },
        {
          "FieldType": "plan",
          "FieldName": "(A.7.4) Fairness and Bias in AI Systems Test Plan",
          "PlanObjective": "Ensure AI systems are designed, developed, and tested with defined fairness objectives to prevent discriminatory or inequitable outcomes.",
          "PlanCriteria": [
            {
              "criteria": "FAIR-CTX - 01: The context for fairness analysis, including the system's purpose, favorable outcomes, and protected attributes, must be clearly defined and documented.",
              "control_objective": "To establish a clear and unambiguous framework for all subsequent fairness assessments, ensuring that tests are relevant to the system's specific use case.",
              "criteria_evidence": "A signed-off document specifying: 1. The AI system's intended purpose. 2. The definition of the 'favorable outcome' variable. 3. A comprehensive list of protected attributes relevant to the legal jurisdiction (e.g., GDPR in Luxembourg)."
            },
            {
              "criteria": "FAIR-REP - 01: The dataset's demographic composition must be representative of the target real-world population.",
              "control_objective": "To identify and mitigate representation bias, ensuring that the dataset does not systematically under- or over-represent certain demographic groups.",
              "criteria_evidence": "A data profiling report that includes: 1. Visualizations (e.g., bar charts) of the distribution of all subgroups for each protected attribute. 2. A comparative analysis of these distributions against external population benchmarks (e.g., census data, applicant pool statistics)."
            },
            {
              "criteria": "FAIR-HIST - 01: The dataset must not exhibit significant statistical disparities in historical outcomes between protected groups.",
              "control_objective": "To prevent the AI model from learning and amplifying past discriminatory patterns present in the training data.",
              "criteria_evidence": "A historical bias analysis report demonstrating: 1. Calculation of favorable outcome rates for all subgroups. 2. Application of a statistical disparity test (e.g., the Four-Fifths Rule), with results showing that no subgroup's outcome rate is less than 80% of the most favorable group's rate."
            },
            {
              "criteria": "FAIR-PRX - 01: Features that could act as proxies for protected attributes must be identified, analyzed, and documented.",
              "control_objective": "To prevent indirect discrimination by ensuring the model does not rely on non-protected features that are highly correlated with sensitive attributes.",
              "criteria_evidence": "A correlation analysis report (e.g., correlation matrix, feature importance analysis) that identifies features highly correlated with protected attributes. A documented decision for the inclusion or exclusion of each identified proxy."
            },
            {
              "criteria": "FAIR-GOV - 01: A formal governance process must be in place to report on and mitigate identified fairness risks.",
              "control_objective": "To ensure accountability and a structured, auditable response to any fairness issues discovered in the dataset before model development.",
              "criteria_evidence": "1. A formal 'Dataset Bias Report' summarizing all findings from the context, representation, historical, and proxy analyses. 2. A documented and approved mitigation plan outlining the actions to be taken (e.g., re-sampling, re-weighting, risk acceptance)."
            }
          ],
          "PlanSteps": [
            {
              "step": "FAIR-CTX-TEST-01: Document the intended purpose and operational context of the dataset (e.g., screening job applicants, predicting loan defaults).",
              "step_objective": "To establish the specific fairness context, which dictates the relevance of different biases and informs the selection of appropriate fairness metrics, as required by FAIR-CTX-01."
            },
            {
              "step": "FAIR-CTX-TEST-02: Identify and document the 'favorable outcome' variable and its values within the dataset (e.g., loan_approved = 1, hired = TRUE).",
              "step_objective": "To provide a clear, measurable target for performing historical bias analysis and calculating outcome rates across groups, fulfilling a key part of criterion FAIR-CTX-01."
            },
            {
              "step": "FAIR-CTX-TEST-03: List the protected attributes relevant to the system's context and jurisdiction (e.g., Gender, Age, Ethnicity, Disability per GDPR in Luxembourg).",
              "step_objective": "To define the specific demographic and sensitive groups against which all subsequent fairness and bias tests will be conducted, completing the requirements for FAIR-CTX-01."
            },
            {
              "step": "FAIR-REP-TEST-01: For each protected attribute, calculate and visualize the distribution of individuals across all subgroups (e.g., bar charts showing counts for each gender).",
              "step_objective": "To quantify the demographic composition of the dataset and identify any underrepresentation, directly testing for criterion FAIR-REP-01."
            },
            {
              "step": "FAIR-REP-TEST-02: Compare the dataset's demographic proportions against relevant real-world population benchmarks (e.g., national census data, applicant pool statistics).",
              "step_objective": "To formally assess for representation bias by determining if the dataset is a skewed or accurate sample of the target population, providing the core evidence for FAIR-REP-01."
            },
            {
              "step": "FAIR-HIST-TEST-01: Calculate and compare the rate of the 'favorable outcome' for each subgroup within a protected attribute (e.g., calculate the loan approval rate for males vs. females).",
              "step_objective": "To measure for historical bias by identifying disparities in past outcomes, which is the first step in validating criterion FAIR-HIST-01."
            },
            {
              "step": "FAIR-HIST-TEST-02: Apply a quantitative disparity threshold, such as the 'Four-Fifths (80%) Rule', to determine if the observed differences in outcome rates are statistically significant.",
              "step_objective": "To provide a concrete, defensible method for confirming that the dataset meets the disparity requirements of criterion FAIR-HIST-01."
            },
            {
              "step": "FAIR-PRX-TEST-01: Conduct a correlation analysis to identify non-protected features that are strong predictors of protected attributes (e.g., check correlation between zip code and ethnicity).",
              "step_objective": "To uncover hidden sources of bias by identifying proxy variables, thereby generating the required analysis report for criterion FAIR-PRX-01."
            },
            {
              "step": "FAIR-GOV-TEST-01: Compile all findings into a formal 'Dataset Bias Report' detailing representation biases, historical biases, and identified proxies.",
              "step_objective": "To create a comprehensive, auditable record of the fairness assessment, producing the primary piece of evidence required by FAIR-GOV-01."
            },
            {
              "step": "FAIR-GOV-TEST-02: Based on the report, select and document a clear mitigation strategy (e.g., collect more data, apply re-sampling/re-weighting, remove proxies, or formally accept the risk).",
              "step_objective": "To ensure that identified biases are not ignored and that a deliberate, documented action plan is put in place, fulfilling the mitigation plan requirement of FAIR-GOV-01."
            }
          ]
        },
        {
          "FieldType": "plan",
          "FieldName": "(A.7.4) Transparency and Explainability in AI Systems Test Plan",
          "PlanObjective": "Ensure AI systems are designed, developed and tested to provide understandable and sufficient information about its decisions to affected individuals.",
          "PlanCriteria": [
            {
              "criteria": "TRN-DEF - 01: Explainability requirements, including audiences, their specific needs, and legal obligations, must be formally defined and documented.",
              "control_objective": "To establish a clear, stakeholder-aligned foundation for the design and implementation of the explanation system, ensuring it is fit-for-purpose and compliant.",
              "criteria_evidence": "A signed-off 'Explainability Requirements Document' that specifies: 1. A list of all identified audiences (e.g., end-users, developers). 2. The core questions each audience needs answered. 3. A review of applicable legal obligations (e.g., GDPR)."
            },
            {
              "criteria": "TRN-TECH - 01: Appropriate explanation techniques must be selected and implemented to cover both global model behavior and local, individual predictions.",
              "control_objective": "To ensure the system has the technical capability to generate explanations that address the full spectrum of stakeholder needs, from high-level validation to individual case analysis.",
              "criteria_evidence": "System architecture diagrams and documentation that specify: 1. The rationale for model selection (interpretable vs. complex). 2. The chosen global explanation technique (e.g., Feature Importance). 3. The chosen local explanation technique (e.g., SHAP, LIME)."
            },
            {
              "criteria": "TRN-VAL - 01: The implemented explanation system must be technically robust, plausible, and stable.",
              "control_objective": "To ensure that the generated explanations are reliable, accurate, and trustworthy, preventing misleading or nonsensical outputs.",
              "criteria_evidence": "Validation test reports including: 1. Evidence of successful integration into the prediction pipeline. 2. A log of domain expert reviews confirming the plausibility of sample explanations. 3. Results from stability tests showing consistent explanations for minor input variations."
            },
            {
              "criteria": "TRN-COM - 01: Technical explanation outputs must be translated into formats that are understandable, accessible, and actionable for each target audience.",
              "control_objective": "To bridge the gap between complex model outputs and user comprehension, thereby achieving true transparency and empowering users.",
              "criteria_evidence": "A 'Communication & UI/UX Design' document containing: 1. Natural language templates for end-user explanations. 2. Designs for actionable or counterfactual explanations. 3. Mockups or screenshots of visualizations for internal dashboards."
            },
            {
              "criteria": "TRN-GOV - 01: Formal governance, including clear documentation and user-facing processes, must be established for the system's explainability features.",
              "control_objective": "To ensure accountability, operationalize the right to explanation, and maintain a transparent and auditable record of the system's capabilities.",
              "criteria_evidence": "1. A completed Model Card with a dedicated 'Explainability' section. 2. A publicly documented and accessible process for users to request and receive explanations for decisions that affect them."
            }
          ],
          "PlanSteps": [
            {
              "step": "TRN-DEF-TEST-01: Identify and document all audiences for explanations, categorizing them into groups such as Developers/Auditors, Business Owners, and Affected End-Users.",
              "step_objective": "To gather the necessary inputs for the 'Explainability Requirements Document' and satisfy a core component of criterion TRN-DEF-01."
            },
            {
              "step": "TRN-DEF-TEST-02: For each identified audience, define and document the primary question the explanation needs to answer (e.g., debugging, business logic validation, or personal recourse).",
              "step_objective": "To ensure explanations are relevant and useful, fulfilling the audience needs analysis required by criterion TRN-DEF-01."
            },
            {
              "step": "TRN-DEF-TEST-03: Conduct and document a review of legal and regulatory obligations for explainability, such as those related to GDPR in Luxembourg.",
              "step_objective": "To verify that the system's transparency features are designed for legal compliance, as mandated by criterion TRN-DEF-01."
            },
            {
              "step": "TRN-TECH-TEST-01: For a given use case, justify the choice of model type (interpretable vs. complex) based on performance and transparency trade-offs.",
              "step_objective": "To test that the model selection process aligns with the principle of using the simplest effective model, as outlined in criterion TRN-TECH-01."
            },
            {
              "step": "TRN-TECH-TEST-02: For a complex model, generate and review a global explanation report (e.g., a feature importance plot) and verify it is produced correctly.",
              "step_objective": "To confirm the implementation of a global explanation technique, providing evidence for criterion TRN-TECH-01."
            },
            {
              "step": "TRN-TECH-TEST-03: For a complex model, generate a local explanation (e.g., a SHAP force plot) for a specific prediction and confirm its output.",
              "step_objective": "To confirm the implementation of a local explanation technique, providing evidence for criterion TRN-TECH-01."
            },
            {
              "step": "TRN-VAL-TEST-01: Run an end-to-end test of the prediction pipeline and verify that an explanation artifact is generated and stored alongside every prediction.",
              "step_objective": "To validate the technical integration of the XAI library, providing evidence for the robustness requirement of TRN-VAL-01."
            },
            {
              "step": "TRN-VAL-TEST-02: Submit a sample of 10-20 explanations to a domain expert for review and collect their signed approval that the explanations are logical and plausible.",
              "step_objective": "To test the contextual meaningfulness of the explanations, fulfilling the plausibility requirement of criterion TRN-VAL-01."
            },
            {
              "step": "TRN-VAL-TEST-03: Create two near-identical input samples with only minor, irrelevant variations. Generate explanations for both and verify that the resulting explanations are highly similar.",
              "step_objective": "To empirically test the output's robustness, fulfilling the stability requirement of criterion TRN-VAL-01."
            },
            {
              "step": "TRN-COM-TEST-01: Take a raw SHAP value output and process it through the natural language template. Verify the output is grammatically correct and easy to understand for a non-technical user.",
              "step_objective": "To test the effectiveness of the translation layer, providing evidence for the user-facing communication criterion TRN-COM-01."
            },
            {
              "step": "TRN-COM-TEST-02: For a simulated negative outcome (e.g., loan rejection), verify that the generated explanation includes actionable guidance or a counterfactual.",
              "step_objective": "To confirm that explanations are designed to empower users, directly testing a key component of criterion TRN-COM-01."
            },
            {
              "step": "TRN-COM-TEST-03: Generate an explanation for a business user and verify it is displayed as a simple, clear visualization on the internal dashboard.",
              "step_objective": "To ensure explanations for internal stakeholders are effective and easy to interpret, as required by criterion TRN-COM-01."
            },
            {
              "step": "TRN-GOV-TEST-01: Review the project's Model Card and confirm the presence and completeness of the 'Explainability' section.",
              "step_objective": "To audit the formal documentation and ensure it meets the governance standards set by criterion TRN-GOV-01."
            },
            {
              "step": "TRN-GOV-TEST-02: Follow the published user process to request an explanation. Verify that the request is logged and handled according to the documented procedure and timelines.",
              "step_objective": "To perform an end-to-end test of the operational process, confirming compliance with criterion TRN-GOV-01."
            }
          ]
        },
        {
          "FieldType": "plan",
          "FieldName": "(A.7.4, A.7.6) - Data quality requirements & Test Plan",
          "PlanObjective": "Ensure that data used to develop and operate the AI system meet defined Data Quality Requirements?",
          "PlanCriteria": [
            {
              "criteria": "DATA-SEN - 01: The accuracy level for sensitive data must be > 95% of data points correct when compared to a trusted source.",
              "control_objective": "To ensure that sensitive data is reliable, precise, and fit for high-stakes decision-making.",
              "criteria_evidence": "A data quality report or test results from a validation script showing an accuracy score of > 95%. Documentation of the validation methodology, including the definition of the 'trusted source'. The validation script itself should be available for review."
            },
            {
              "criteria": "DATA-SEN - 02: Sensitive data must have All critical information present and all necessary data fields populated.",
              "control_objective": "To guarantee that all necessary information required for analysis and operations is present in the sensitive dataset.",
              "criteria_evidence": "A data schema or data dictionary defining all critical and necessary data fields. A data profiling report or log from an automated script that verifies completeness, showing zero null or empty values in the designated critical fields."
            },
            {
              "criteria": "DATA-SEN - 03: Sensitive data must have No contradictory information and maintain integrity across related datasets.",
              "criteria_objective": "To maintain the integrity and trustworthiness of sensitive data by eliminating logical contradictions across related datasets.",
              "criteria_evidence": "Documentation of integrity rules and constraints applied to the data. Test results from validation scripts or database constraints (e.g., unit tests, SQL queries) that check for contradictions, with logs showing zero violations found."
            },
            {
              "criteria": "DATA-SEN - 04: Sensitive data must be maintained in Real-time.",
              "criteria_objective": "To ensure that sensitive data is timely and current for its intended use, especially in contexts requiring immediate action or decision.",
              "criteria_evidence": "System logs or monitoring dashboard metrics (e.g., from Kafka, Grafana, or a data pipeline tool) showing timestamps of data ingestion and processing. A Service Level Agreement (SLA) document defining the maximum acceptable latency, with monitoring reports confirming compliance."
            },
            {
              "criteria": "DATA-SEN - 05: The sensitive data's provenance must be fully traced and verified.",
              "criteria_objective": "To enable full auditing and verification by maintaining a complete, unalterable record of the sensitive data's origin and history (provenance).",
              "criteria_evidence": "A data lineage graph or document that maps the data flow from its origin to its final state. Immutable logs (e.g., from a blockchain or write-once log system) that record all transformations, including timestamps and the identity of the process or user performing the change."
            },
            {
              "criteria": "DATA-SEN - 06: A robust version control system like Git or DVC must be applied to manage and track sensitive data versions.",
              "criteria_objective": "To ensure that changes to sensitive data are tracked, auditable, and reversible, protecting against unauthorized or erroneous modifications.",
              "criteria_evidence": "A link to the version control repository (e.g., Git, DVC). A review of the repository's commit history demonstrating consistent and meaningful commits for data changes. A README file or documentation outlining the branching and tagging strategy for data versions."
            }
          ],
          "PlanSteps": [
            {
              "step": "BBT-EXT-ACC-01: Establish a 'golden dataset' by manually and accurately extracting all critical data points from a representative sample of 10-20 source documents of varying types (PDF, DOCX, etc.).",
              "step_objective": "To create a verified, ground-truth benchmark against which the automated extraction process's accuracy can be quantitatively measured, directly testing for control DATA-SEN-01."
            },
            {
              "step": "BBT-EXT-ACC-02: Process the sample documents through the automated pipeline and programmatically compare the extracted text against the corresponding 'golden dataset', calculating the data point accuracy percentage.",
              "step_objective": "To validate that the extraction process meets the specified >95% accuracy threshold (DATA-SEN-01) and to identify specific failure modes (e.g., issues with tables, complex layouts)."
            },
            {
              "step": "BBT-EXT-COM-01: For a sample of documents, compare the total word/character count of the source file against the total count of the extracted and cleaned text. Flag any document with a deviation greater than a set threshold (e.g., 15%).",
              "step_objective": "To perform a high-level check for major data loss, such as missed pages or entire sections, ensuring the process meets the completeness requirement of DATA-SEN-02."
            },
            {
              "step": "BBT-EXT-COM-02: Define and test for 'non-splittable entities' (e.g., a person's full name and title, a complete address, a single table row). Verify that the chunking mechanism does not break these entities across multiple, separate chunks.",
              "step_objective": "To ensure the chunking process preserves the semantic integrity of the data, preventing the creation of incomplete or misleading fragments and upholding the completeness rule (DATA-SEN-02)."
            },
            {
              "step": "BBT-EXT-CON-01: Identify documents in the source data that contain known, intentional contradictions. Process these documents and verify that the extracted chunks accurately reflect the original contradictions without introducing new ones.",
              "step_objective": "To confirm that the extraction and chunking process does not introduce new errors or artifacts that could be misinterpreted as data contradictions, thereby testing the integrity preservation required by DATA-SEN-03."
            },
            {
              "step": "BBT-EXT-TIM-01: Introduce a new document or an updated version of an existing document into the source data collection. Measure the end-to-end processing time until its corresponding chunks are generated and ready for embedding.",
              "step_objective": "To empirically test the data pipeline's latency and validate that it meets the timeliness requirements defined in DATA-SEN-04 for maintaining a real-time system."
            },
            {
              "step": "BBT-EXT-PRO-01: From the output of the vectorization pipeline, select a random sample of 50 text chunks. For each chunk, verify the presence and correctness of its metadata, including source filename, document version, and page number.",
              "step_objective": "To ensure that every piece of processed data maintains a verifiable link to its origin, satisfying the full provenance tracing requirement of DATA-SEN-05."
            },
            {
              "step": "BBT-EXT-PRO-02: Using only the metadata from a sampled chunk, perform a reverse lookup to retrieve the original source document and confirm the chunk's content matches the content at the specified location (e.g., page 5, paragraph 2).",
              "step_objective": "To validate the functional reliability of the provenance metadata, ensuring it is not just present but accurate and useful for auditing and verification as per DATA-SEN-05."
            },
            {
              "step": "BBT-EXT-VER-01: Perform a test data rollback. Check out a previous version of a source document from the version control system (e.g., Git/DVC), run it through the pipeline, and verify that the output chunks exactly match the state of that older version.",
              "step_objective": "To confirm that the version control system is properly integrated and that the data processing pipeline can reliably reproduce outputs from any given historical version of the data, fulfilling control DATA-SEN-06."
            }
          ]
        }        
      ]
    }
  ],
  "Phase Operations": [
    {
      "StepName": "(A.4.3) AI Lifecycle Phase requirements - Secure Logging and Monitoring",
      "Objectives": [
        {
          "Objective": "1. System Performance and Reliability: This is about keeping the AI healthy and efficient. By logging every interaction, we can measure key metrics like response times and success rates. This data is essential for our technical teams to identify performance bottlenecks, troubleshoot errors, and ensure the system remains reliable and responsive for all users."
        },
        {
          "Objective": "2. Transparency and Explainability: To trust the AI, we must be able to understand its thought process. This logging system functions like an airplane's black box, securely recording the user's question, the specific information the AI retrieved to form its answer, and the final response given. This is critical for investigating any incorrect answers and for explaining the AI's decision-making process to auditors or stakeholders."
        },
        {
          "Objective": "3. Fairness and Quality Assurance: This is our primary mechanism for ensuring the AI remains accurate, fair, and safe over time. By analyzing the logged data, we can proactively monitor for potential biases, detect instances where the AI may be hallucinating (straying from the provided facts), and gather the insights needed to continuously improve the quality and safety of the system."
        }
      ],
      "Fields": []
    },
    {
      "StepName": "(A.6.2.6) AI Lifecycle Phase requirements - Operation and Monitoring",
      "Objectives": [
        {
          "Objective": "Define the security requirements for lifecycle phase - Deployment, Maintenance and Updates."
        }
      ],
      "Fields": [
        {
          "FieldType": "risk",
          "FieldName": "[C.3.6] - Mismanaged AI System Deployment and Transition",
          "question": "Is the plan to deploy the AI system a 'big bang' launch without sufficient user training, a pilot phase, or a clear transition plan from the legacy system?",
          "controls": [
            {
              "control": "[C.3.6][BP-4] - Develop and deliver comprehensive training programs for all users and operators on the AI system's capabilities, limitations, and proper operational procedures before deployment.",
              "control_objective": "To minimize misuse, build user trust, and ensure the AI system is operated correctly and effectively."
            },
            {
              "control": "[C.3.6][BP-5] - Implement a phased rollout or pilot program in a controlled environment to identify and resolve unforeseen issues before a full-scale, system-wide launch.",
              "control_objective": "To reduce the risk of widespread disruption and failure by validating system performance in a live but limited setting."
            },
            {
              "control": "[C.3.6][BP-6] - Establish a clear transition plan that includes running the new AI system in parallel with any legacy system to verify outputs and ensure a smooth handover without creating an operational gap.",
              "control_objective": "To guarantee operational continuity and validate the new AI system's performance against established benchmarks before decommissioning the previous system."
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "[C.3.6] - Inadequate Post-Deployment Maintenance and Monitoring",
          "question": "Once the AI system is live, is there a risk it will be treated with a 'set and forget' mentality, without a dedicated team or process for continuous performance monitoring and regular maintenance?",
          "controls": [
            {
              "control": "[C.3.6][BP-7] - Assign clear ownership for the deployed AI system and implement a continuous monitoring framework with predefined KPIs (e.g., for accuracy, data drift, fairness) and automated alerts to detect performance degradation.",
              "control_objective": "To ensure proactive oversight and rapid detection of issues, preventing the AI system from making flawed decisions over extended periods."
            },
            {
              "control": "[C.3.6][BP-8] - Establish a formal maintenance schedule for the AI system, including periodic model retraining with new data, recalibration, and updates to adapt to changing operational environments.",
              "control_objective": "To prevent model obsolescence and ensure the AI system remains accurate, relevant, and effective throughout its operational life."
            },
            {
              "control": "[C.3.6][BP-9] - Implement a formal feedback loop for users to easily report issues, anomalies, or incorrect outputs, and ensure this feedback is systematically reviewed and used to improve the system.",
              "control_objective": "To leverage end-user experience as a critical source for identifying subtle performance issues and driving continuous improvement."
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "Insufficient Scalability Management",
          "question": "Will this AI be used across multiple teams, products, or regions — and is it ready to scale reliably?",
          "controls": [
            {
              "control": "SC[1] -  Develop and implement a structured scalability management process for AI infrastructure to ensure efficient handling of increasing workloads and data volumes.",
              "control_objective": "To ensure that the AI infrastructure can scale effectively and efficiently, meeting the growing demands of the organisation while maintaining performance, stability, and security."
            },
            {
              "control": "SC[2] -  Deploy comprehensive technical measures to ensure the AI infrastructure can scale seamlessly to accommodate varying workloads and future growth.",
              "control_objective": "To guarantee that the AI infrastructure is equipped with the necessary technical capabilities to support scalability, ensuring uninterrupted performance and resource availability as demands increase."
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "[C.3.6] - Poor Management of AI System Evolution and Updates",
          "question": "When the AI system is updated (e.g., retrained model, new features), are there formal processes for testing, documenting, and deploying these changes, including a plan to roll back if the update causes problems?",
          "controls": [
            {
              "control": "[C.3.6][BP-10] - Enforce a strict change management process for all AI system updates, requiring thorough testing (e.g., A/B testing, shadow mode), impact analysis, and a documented rollback plan before deployment.",
              "control_objective": "To prevent system degradation caused by faulty updates and ensure that changes improve, rather than harm, system performance and reliability."
            },
            {
              "control": "[C.3.6][BP-11] - Maintain comprehensive and up-to-date documentation for the AI system, including design rationale, model parameters, data sources, and a version history of all changes made.",
              "control_objective": "To ensure knowledge continuity, facilitate troubleshooting, and support consistent and informed management of the AI system over time."
            },
            {
              "control": "[C.3.6][BP-12] - Implement version control for all components of the AI system, including data, code, and models, to ensure traceability and the ability to rapidly revert to a previously stable version in case of an incident.",
              "control_objective": "To provide a safety net against failed updates and enable quick recovery of system functionality."
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "Inadequate Disaster Recovery and Business Continuity Planning",
          "question": "What would happen to your business process if this AI system suddenly stopped working?",
          "controls": [
            {
              "control": "BU[1] -  Establish a comprehensive disaster recovery and business continuity planning process to ensure preparedness and resilience in the face of disruptions to AI infrastructure.",
              "control_objective": "To develop and implement administrative controls that ensure the organisation is prepared for and can effectively respond to disruptions, thereby maintaining the continuity of AI operations and minimising downtime."
            },
            {
              "control": "BU[2] -  Implement technical measures to support disaster recovery and business continuity, ensuring the resilience and availability of AI infrastructure.",
              "control_objective": "To deploy comprehensive technical controls that enable rapid recovery and continuity of AI operations in the event of a disruption, minimising data loss and operational downtime."
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "Insufficient Performance Monitoring and Analysis",
          "question": "Will the AI’s outputs need to be continuously monitored to ensure they stay accurate and relevant?",
          "controls": [
            {
              "control": "PER[1] -  Establish a structured performance monitoring and analysis process to ensure continuous oversight and optimization of AI infrastructure performance.",
              "control_objective": "To implement robust administrative controls that define roles, responsibilities, and procedures for performance monitoring and analysis, ensuring that performance issues are identified and addressed proactively."
            },
            {
              "control": "PER[2] -  Implement advanced technical measures to ensure continuous and effective performance monitoring and analysis of AI infrastructure.",
              "control_objective": "To deploy comprehensive technical controls that provide real-time visibility into AI system performance, enabling prompt detection and resolution of performance issues to maintain optimal operation."
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "Inadequate Security Monitoring and Threat Detection",
          "question": "Do you need visibility into whether this AI system is under attack or being misused?",
          "controls": [
            {
              "control": "STM[1] -  Establish a security monitoring and threat detection process to ensure continuous oversight and protection of AI infrastructure.",
              "control_objective": "To implement robust administrative controls that define roles, responsibilities, policies, and procedures for effective security monitoring and threat detection, ensuring proactive identification and mitigation of security threats."
            },
            {
              "control": "STM[2] -  Implement advanced technical measures to ensure continuous and effective security monitoring and threat detection within AI infrastructure.",
              "control_objective": "To deploy comprehensive technical controls that provide real-time visibility into security threats and enable prompt detection and response to ensure the protection and integrity of AI systems."
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "Ineffective Incident Management",
          "question": "Do you have a process in place if the AI system causes an issue (e.g., incorrect output, outage, or data leak)?",
          "controls": [
            {
              "control": "IM[1] -  Establish an incident management capability to ensure effective detection, response, and mitigation of AI-related incidents.",
              "control_objective": "To establish incident management processes that define roles, responsibilities, and procedures for managing AI-related incidents, ensuring timely and effective response to maintain AI system integrity and security."
            },
            {
              "control": "IM[2] -  Implement advanced technical measures to detect, prevent, and mitigate AI system hallucinations and other incidents, ensuring the reliability and accuracy of AI outputs.",
              "control_objective": "To deploy comprehensive technical controls that provide real-time visibility into AI system performance and security, enabling prompt detection and resolution of incidents to maintain optimal operation."
            }
          ]
        }
      ]
    }
  ],
  "Phase 6": [
    {
      "StepName": "AI Systems approvals",
      "Objectives": [
        {
          "Objective": "Stakeholder Approval and Governance: To obtain formal sign-off from all relevant stakeholders, confirming that the deployment plan is sound and all prerequisites have been satisfied, thereby providing a clear governance gate and accountability for the deployment decision."
        }
      ],
      "Fields": [
        {
          "FieldName": "AI System Security Approver",
          "FieldLabel": "Security Approver",
          "FieldText": "Name/Role of the Security Aprover",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "AI System Security Approval",
          "FieldLabel": "Security Approved",
          "FieldText": "",
          "FieldType": "Option box with values:Yes/No"
        },
        {
          "FieldName": "AI System DPO Approver",
          "FieldLabel": "DPO Approver",
          "FieldText": "Name/Role of the DPO Aprover",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "AI System DPO Approval",
          "FieldLabel": "DPO Approved",
          "FieldText": "",
          "FieldType": "Option box with values:Yes/No"
        },
        {
          "FieldName": "AI System Risk Approver",
          "FieldLabel": "Risk Approver",
          "FieldText": "Name/Role of the Risk Aprover",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "AI System Risk Approval",
          "FieldLabel": "Risk Approved",
          "FieldText": "",
          "FieldType": "Option box with values:Yes/No"
        },
        {
          "FieldName": "AI System Business Approver",
          "FieldLabel": "Business Approver",
          "FieldText": "Name/Role of the Business Approver",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "AI System Business Approval",
          "FieldLabel": "Business Approved",
          "FieldText": "",
          "FieldType": "Option box with values:Yes/No"
        }
      ]
    }
  ]
}