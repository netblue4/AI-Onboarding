{
  "Phase AI system information": [
    {
      "StepName": "(A.9.4) AI system's intended use and limitations",
      "Objectives": [
        {
          "Objective": "Document the purpose, target users, and intended use cases of the AI system."
        }
      ],
      "Fields": [
        {
          "Control": "A.9.4 – Intended use of the AI system",
          "FieldName": "(A.9.4) - AI System ID",
          "FieldLabel": "AI System ID",
          "FieldText": "AI System Unique Number",
          "FieldType": "Auto generated number"
        },
        {
          "Control": "A.9.4 – Intended use of the AI system",
          "FieldName": "(A.9.4) - Name",
          "FieldLabel": "Name",
          "FieldText": "Name of the AI application",
          "FieldType": "TextBox"
        },
        {
          "Control": "A.9.4 – Intended use of the AI system",
          "FieldName": "(A.9.4) - Business Purpose",
          "FieldLabel": "Business Purpose",
          "FieldText": "What specific business problem or task does this system address?",
          "FieldType": "TextBox"
        },
        {
          "Control": "A.9.4 – Intended use of the AI system",
          "FieldName": "(A.9.4) - Intended Use",
          "FieldLabel": "Intended Use",
          "FieldText": "Describe the use cases of how the AI system will solve the specific business problem or task",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "AI system impact assessment - Employees - Benefits",
          "FieldLabel": "Select Potential Benefits",
          "FieldText": "",
          "FieldType": "MultiSelect:Faster Service/Reduced Error Rate/Increased Efficiency/Personalized Training or Upskilling/Better Decision-Making Tools"
        },
        {
          "Control": "A.9.4 – Intended use of the AI system",
          "FieldName": "HR-WIA-2.3-AugmentationTasks",
          "FieldLabel": "Augmented/Enhanced Tasks",
          "FieldText": "List the high-value tasks that will be significantly improved, made more accurate, or accelerated by the AI system's assistance.",
          "FieldType": "TextBox"
        }
      ]
    },
    {
      "StepName": "(A.9.3) AI system's Objectives for Responsible Use",
      "Objectives": [
        {
          "Objective": "1. Establish Clear Operational Boundaries: To formally document the precise scope of the AI system by defining what it is built to do (Permitted Prompt Categories), what it must never be used for (Known Prohibited Uses), and who is authorized to use it (Intended User Profile)."
        },
        {
          "Objective": "2. Ensure Safety and Regulatory Compliance: To proactively mitigate risks and ensure compliance with regulations like the EU AI Act by explicitly identifying and forbidding high-risk or prohibited applications from the outset."
        },
        {
          "Objective": "3. Promote Effective and Authorized Use: To ensure the system is used effectively and safely by restricting its operation to trained, authorized users and guiding them towards the specific, validated use cases for which the system was designed."
        }
      ],
      "Fields": [
        {
          "FieldName": "AI system intended use - Permitted User Prompt Categories Whitelist Definition",
		  "FieldLabel": "Whitelist Key Phrases for Permitted User Prompt Categories",
		  "FieldText": "This is the reference list of key phrases that will be allowed by the RAG Orchestrator. The RAG Orchestrator will be trained to recognize prompts similar to these examples.",
		  "FieldType": "MultiSelect:Information Retrieval and Q&A:what is, who is, where can I find, tell me about, explain the, what are the details on/Document Summarization:summarize this, give me the key points, tl;dr, provide a summary of, what are the main takeaways, create an abstract for/Content Creation and Drafting:draft an email about, write a paragraph on, create a marketing blurb for, help me write, generate a draft of, compose a message about/Policy and Procedure Guidance:what is the policy for, how do I request, what is the procedure for, explain the process of, guide me through the steps for/Onboarding and Training Support:how do I set up, what training do I need for, explain the onboarding process, guide for new hires, help me learn about/Technical Support and Troubleshooting:how to fix, I'm getting an error, troubleshoot this issue, what does this error mean, help me with this technical problem/Data Analysis and Extraction:extract all the names from, what is the total of, find the average of, pull the data for, list all the dates in, analyze the trends in"
        },
        {
		  "FieldName": "AI system intended use - Prohibited Uses Blacklist Definition",
		  "FieldLabel": "Blacklist Key Phrases for Prohibited User Prompt Categories",
		  "FieldText": "This is a reference list of key phrases that will be blocked by the RAG Orchestrator. AI systems that support these key phrases are prohibited by the AI act.",
		  "FieldType": "MultiSelect:Manipulative or Subliminal Techniques Causing Harm:influence my decision, make me buy, change my mind, secretly persuade, subliminal message, hidden command, trick me into, deceive me/Exploitation of Vulnerabilities (e.g., age, disability):target children with ads, exploit elderly, convince a disabled person, addictive game for kids, target financially desperate, exploit vulnerability/General-Purpose Social Scoring:calculate social score, rate my behavior, trustworthiness score, classify people's value, social credit system, evaluate a person based on/Predictive Policing Based Solely on Profiling:predict who will commit a crime, crime risk assessment for a person, profile potential criminals, likelihood of reoffending, predict recidivism/Untargeted Scraping of Facial Images for Databases:scrape faces from internet, create facial recognition database, find all images of a person online, collect CCTV footage for faces, build a face dataset/Emotion Recognition in Workplace or Education:analyze employee emotions, detect student mood, tell me if my team is happy, monitor engagement with emotion, read facial expressions for performance/Biometric Categorization Using Sensitive Data (e.g., race, religion):guess race from photo, determine political opinion from text, infer religious beliefs, categorize by sexual orientation, deduce sex life/'Real-Time' Remote Biometric ID in Public Spaces:live facial recognition, track people in real-time with cameras, identify everyone in this crowd, public camera identification, real-time biometric tracking"
        },
        {
          "FieldName": "AI system intended use - Intended User Profile",
          "FieldLabel": "Select the Intended User Profiles for the AI System",
          "FieldText": "Define the specific roles, departments, or groups of individuals who are authorized to use the system. This helps to control access and ensure the system is used by those with the appropriate context and training.",
          "FieldType": "MultiSelect:All Employees/Specific Department(s)/Management and Leadership/Technical Administrators and Developers/External Partners or Clients/Executive Leadership Only"
        }
      ]
    },
    {
      "StepName": "x(A.4.2, A.4.4) AI Systems Software and Tooling Resources",
      "Objectives": [
        {
          "Objective": "Define the software and tools used by the AI system."
        }
      ],
      "Fields": [
        {
          "FieldName": "(A.4.2, A.4.4) - Tool Name and Version",
          "FieldLabel": "Tools Name and Version",
          "FieldText": "Name of the software, libraries, or frameworks and their Version numbers.",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "(A.4.2, A.4.4) - Category",
          "FieldLabel": "Select Tool Categories",
          "FieldText": "",
          "FieldType": "MultiSelect:Programming Language/IDE/Data Processing/ML Framework/Version Control/Deployment"
        },
        {
          "FieldName": "(A.4.2, A.4.4) - Purpose/Use Case in Project",
          "FieldLabel": "Purpose/Use Case in Project",
          "FieldText": "How these tools will be used in the AI system's lifecycle.",
          "FieldType": "TextBox"
        }
      ]
    },
    {
      "StepName": "x(A.4.2, A.4.5) AI Systems Computing Resources",
      "Objectives": [
        {
          "Objective": "Define the computing resources used by the AI system"
        }
      ],
      "Fields": [
        {
          "FieldName": "(A.4.2, A.4.5) - Computing Resource Name and Version",
          "FieldLabel": "Computing Resource Name and Version",
          "FieldText": "Names of the computing resources used.",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "(A.4.2, A.4.5) - Category",
          "FieldLabel": "Category of the computing resource",
          "FieldText": "",
          "FieldType": "MultiSelect:Dev Workstation/ML Training Cluster/Inference API Server/Data Lake Storage/EC2/S3/SQL Database"
        },
        {
          "FieldName": "(A.4.2, A.4.5) - Lifecycle Phase(s) Supported",
          "FieldLabel": "Which parts of the AI lifecycle do these resource supports?",
          "FieldText": "",
          "FieldType": "MultiSelect:Development/Training/Testing/Staging/Production/Monitoring"
        }
      ]
    }
  ],
  "Phase Impact assessments": [
    {
      "StepName": "x(A.5.2, A.5.3, A.5.4) Workforce Transition and Adaptation for AI Integration",
      "Objectives": [
        {
          "Objective": "Define the workforce adaptation and training strategies to address risks from job role evolution due to AI adoption."
        }
      ],
      "Fields": [
        {
          "Control": "A.9.4 – Intended use of the AI system",
          "FieldName": "(A.9.4) - Target Users",
          "FieldLabel": "Select the job titles whose daily tasks may be altered by more than 20% due to the AI system",
          "FieldText": "",
          "FieldType": "MultiSelect:Employees/Customers/Analysts/Customer/Supplier/Partner/Regulator"
        },
        {
          "FieldName": "HR-WIA-1.2-CoreAction",
          "FieldLabel": "Identify the primary roles of the AI system relative to human workers",
          "FieldText": "",
          "FieldType": "MultiSelect:Augmentation (assisting human judgment)/Automation (replacing tasks)/Creation (enabling new tasks)"
        },
        {
          "FieldName": "HR-WIA-2.2-AutomationTasks",
          "FieldLabel": "Automated/Eliminated Tasks",
          "FieldText": "List the specific tasks that will be fully automated or eliminated for the affected roles, and the estimated percentage of work time saved across the department.",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "HR-WIA-4.1-MitigationPlan",
          "FieldLabel": "Primary Mitigation Strategy for Displacement",
          "FieldText": "If job displacement is identified, select the primary strategies for the affected workers",
          "FieldType": "MultiSelect:Internal Re-deployment/Transfer/Managed Attrition (No Backfill)/Voluntary Separation Package/External Layoff"
        },
        {
          "FieldName": "HR-WIA-4.2-TrainingProgram",
          "FieldLabel": "Structured Re-skilling Program in Place",
          "FieldText": "Describe the primary strategies to address the affected workers.",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "HR-WIA-4.2-TrainingProgram",
          "FieldLabel": "Structured Re-skilling Program Effectiveness",
          "FieldText": "Describe the Training Effectiveness measures to evaluate the success of the primary strategies to address the affected workers.",
          "FieldType": "TextBox"
        }
      ]
    },
    {
      "StepName": "x(A.5.2, A.5.3, A.5.4) Vulnerable Populations Impact Assessment",
      "Objectives": [
        {
          "Objective": "Define and evaluate the AI system's impacts, risks, and mitigation strategies specific to vulnerable populations affected by deployment."
        }
      ],
      "Fields": [
        {
          "FieldName": "VP-IMP-1.1-PopulationType",
          "FieldLabel": "Select the at-risk group(s) impacted by the AI system",
          "FieldText": "",
          "FieldType": "Option box with values:Children/Elderly/Persons with Disabilities/Economically Disadvantaged/Ethnic Minorities/Other (specify)"
        },
        {
          "FieldName": "VP-IMP-2.1-ImpactType",
          "FieldLabel": "Negitive or positive impacts",
          "FieldText": "Describe the specific ways the AI system could negatively or positively affect the vulnerable population identified.",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "VP-IMP-2.2-SeverityAssessment",
          "FieldLabel": "Rate the severity of identified impacts",
          "FieldText": "",
          "FieldType": "Dropdown box with values:/Low/Medium/High"
        },
        {
          "FieldName": "VP-IMP-2.3-SeverityAssessment",
          "FieldLabel": "Describe the severity of identified impacts",
          "FieldText": "",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "VP-IMP-3.1-MitigationStrategy",
          "FieldLabel": "Describe the main strategies chosen to prevent or reduce negative impacts for the vulnerable population",
          "FieldText": "",
          "FieldType": "MultiSelect:Enhanced Access/Targeted Education & Support/Explicit User Consent/Safeguard Controls/Stakeholder Engagement/Other (specify)"
        },
        {
          "FieldName": "VP-IMP-4.1-StakeholderInclusion",
          "FieldLabel": "Have any of the identified vulnerable populations or their representatives been consulted during the design, development, or testing of the AI system?",
          "FieldText": "",
          "FieldType": "Option box with values:Yes, extensive consultation/Yes, limited consultation/No consultation conducted/Not applicable"
        },
        {
          "FieldName": "VP-IMP-4.2-SeverityAssessment",
          "FieldLabel": "Impact Description",
          "FieldText": "Describe the severity of identified impacts.",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "VP-IMP-3.2-MitigationEffectiveness",
          "FieldLabel": "Post-Deployment Monitoring Plan",
          "FieldText": "Describe the plan for monitoring the AI system's performance and impact on vulnerable populations after deployment. Include key metrics and the frequency of review.",
          "FieldType": "TextBox"
        }
      ]
    },
    {
      "StepName": "x(A.5.2, A.5.3, A.5.5) Environmental Sustainability of AI Systems",
      "Objectives": [
        {
          "Objective": "Define proper environmental assessments, eco-efficient practices, and sustainable lifecycle management controls to minimize ecological impacts."
        }
      ],
      "Fields": [
        {
          "FieldName": "ENV-IA-1.1-EnergySource",
          "FieldLabel": "Specify the primary sources of electricity for the data centers used for training and deployment of the AI system.",
          "FieldText": "",
          "FieldType": "MultiSelect:Primarily Renewable (e.g., Solar, Wind, Hydro)/Regional Grid Mix/Primarily Fossil Fuels/Unknown or Not Specified by Provider"
        },
        {
          "FieldName": "ENV-IA-2.1-ConsumptionMetrics",
          "FieldLabel": "Energy Consumption & Carbon Footprint Estimation",
          "FieldText": "Describe the methodology and provide the estimated energy consumption (e.g., in kWh) and carbon footprint (e.g., in tons of CO2e) for the system's lifecycle (training, validation, and deployment).",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "ENV-IA-3.1-HardwareEOL",
          "FieldLabel": "Select the primary strategies for managing the hardware (e.g., servers, GPUs) at the end of its useful life.",
          "FieldText": "",
          "FieldType": "MultiSelect:Certified E-waste Recycling Program/Component Refurbishment and Reuse/Return to Manufacturer for Disposal/Unmanaged Disposal"
        },
        {
          "FieldName": "ENV-IA-4.1-EfficiencyMeasures",
          "FieldLabel": "Select main strategies for minimizing negative environmental impacts.",
          "FieldText": "",
          "FieldType": "MultiSelect:Renewable Energy Use/Model Size Optimization/Cool Data Management/E-waste Recycling/Sustainable Supply Chain/Other (specify)"
        },
        {
          "FieldName": "ENV-IA-4.2-ImpactLevel",
          "FieldLabel": "Level of Environmental Impact",
          "FieldText": "Rate the degree of environmental burden/benefit (with rationale).",
          "FieldType": "Dropdown box with values:/Low/Medium/High"
        },
        {
          "FieldName": "VP-IMP-2.3-SeverityAssessment",
          "FieldLabel": "Describe the Environmental Impact",
          "FieldText": "Describe the degree of environmental burden/benefit.",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "ENV-IA-5.1-PositiveContribution",
          "FieldLabel": "Contribution to Environmental Sustainability Goals",
          "FieldText": "Does the AI system's intended application directly contribute to positive environmental outcomes (e.g., climate change modeling, energy grid optimization, biodiversity monitoring)? If yes, please describe.",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "ENV-IA-6.1-MonitoringReview",
          "FieldLabel": "Environmental Impact Monitoring and Review",
          "FieldText": "Describe the process and frequency for monitoring and reviewing the environmental performance metrics of the AI system over its lifecycle.",
          "FieldType": "TextBox"
        }
      ]
    }
  ],
  "Phase AI Act": [
	{
	  "StepName": "EU AI Act: Prohibited AI Practices Assessment",
	  "Objectives": [
		{
		  "Objective": "To determine if the AI system's intended purpose constitutes a prohibited practice under the EU AI Act."
		}
	  ],
	  "Fields": [
		{
		  "FieldName": "EU-AI-ACT-PROHIBITED-ASSESSMENT",
		  "FieldLabel": "Will the AI system be used for any of the following prohibited purposes?",
		  "FieldText": "The EU AI Act strictly prohibits certain AI practices that pose an unacceptable risk. If any of the following options are selected, the AI system is considered prohibited and cannot be deployed.",
		  "FieldType": "MultiSelect:Manipulating human behavior to cause physical or psychological harm/Exploiting vulnerabilities of specific groups (e.g., age, disability) to cause harm/General-purpose social scoring by public authorities/Real-time remote biometric identification in public spaces for law enforcement (outside of strictly defined exceptions)"
		}
	  ]
	},
	{
	  "StepName": "EU AI Act: High-Risk System Classification",
	  "Objectives": [
		{
		  "Objective": "To classify the AI system's risk level by assessing its intended purpose against the high-risk categories defined in the EU AI Act."
		}
	  ],
	  "Fields": [
		{
		  "FieldName": "EU-AI-ACT-HIGH-RISK-ASSESSMENT",
		  "FieldLabel": "Will the AI system be used for any of the following purposes?",
		  "FieldText": "Under the EU AI Act, a system is classified as high-risk if its intended use falls into specific categories. Please select all that apply. If any option is selected, the AI system will be classified as high-risk.",
		  "FieldType": "MultiSelect:As a safety component in a regulated product (e.g., medical devices, cars, toys)/Biometric identification or categorisation of people/Management of critical infrastructure (e.g., water, gas, electricity)/Determining access to education or scoring exams/Recruitment, promotion, or employee performance management/Assessing creditworthiness or eligibility for public benefits/Law enforcement purposes (e.g., risk assessment, evidence evaluation)/Migration, asylum, and border control management/Assisting judicial authorities in legal proceedings"
		}
	  ]
	},
    {
      "StepName": "AI Act - Section 2: Requirements for High-Risk AI Systems",
      "Objectives": [
        {
          "Objective": "If the AI system is categorised as high-risk according to the AI Acty, define the security requirements for - High-Risk AI Systems."
        }
      ],
      "Fields": [
        {
          "FieldType": "risk",
          "FieldName": "[Article 8] - Non-Compliance with High-Risk AI System Requirements",
          "question": "Is the AI system, or the product incorporating the AI system, subject to other EU regulations, and if so, how is compliance with all applicable regulations, including the AI Act, managed and documented?",
          "controls": [
            {
              "control": "[Art-8][Par-1][1] - Establish and maintain a documented process to ensure and demonstrate that high-risk AI systems comply with the requirements of Section 2 of the AI Act, considering the system's intended purpose and the state of the art in AI.",
              "control_objective": "To ensure that high-risk AI systems meet all legal and technical standards set forth in the AI Act, and to have a clear, auditable trail of compliance."
            },
            {
              "control": "[Art-8][Par-2][2] - For products containing a high-risk AI system that are also subject to other Union harmonisation legislation, ensure the product is fully compliant with all applicable requirements from all relevant regulations.",
              "control_objective": "To achieve comprehensive legal compliance for products that incorporate AI systems and are subject to multiple regulatory frameworks."
            },
            {
              "control": "[Art-8][Par-2][3] - Where applicable, integrate the testing, reporting, and documentation processes required by the AI Act with existing compliance processes under other EU legislation to ensure consistency, avoid duplication, and minimize administrative burden.",
              "control_objective": "To streamline compliance activities, improve efficiency, and ensure a coherent approach to regulatory requirements across different legal instruments."
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "[Article 9] - Inadequate or Ineffective Risk Management",
          "question": "Does the organization have a documented and continuously updated risk management system for its high-risk AI systems that covers the entire lifecycle of the system?",
          "controls": [
            {
              "control": "[Art-9][Par-1][1] - Establish, implement, document, and maintain a comprehensive risk management system for high-risk AI systems.",
              "control_objective": "To ensure a systematic and ongoing process for identifying, evaluating, and mitigating risks associated with high-risk AI systems throughout their lifecycle."
            },
            {
              "control": "[Art-9][Par-2][2] - Implement a continuous iterative risk management process that includes regular systematic reviews and updates, covering identification, analysis, estimation, and evaluation of foreseeable risks.",
              "control_objective": "To proactively manage and adapt to evolving risks by maintaining a dynamic and up-to-date risk management framework."
            },
            {
              "control": "[Art-9][Par-2][3] - Adopt appropriate and targeted risk management measures to address identified risks, including those from post-market monitoring.",
              "control_objective": "To effectively mitigate identified risks through the implementation of specific and relevant control measures."
            },
            {
              "control": "[Art-9][Par-5][4] - Ensure that residual risks, both individual and overall, are acceptable by eliminating or reducing risks as far as technically feasible through design, implementing mitigation measures, and providing information and training.",
              "control_objective": "To reduce the potential for harm to an acceptable level by employing a multi-layered approach to risk mitigation."
            },
            {
              "control": "[Art-9][Par-6][5] - Conduct testing of high-risk AI systems to identify the most appropriate risk management measures and to ensure consistent performance and compliance with requirements.",
              "control_objective": "To validate the effectiveness of risk management measures and ensure the AI system operates as intended."
            },
            {
              "control": "[Art-9][Par-9][6] - Consider the potential adverse impact on persons under the age of 18 and other vulnerable groups when implementing the risk management system.",
              "control_objective": "To provide heightened protection for vulnerable populations who may be disproportionately affected by the AI system."
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "[Article 10] - Poor Data Quality and Governance",
          "question": "Are there appropriate data governance and management practices in place for the training, validation, and testing datasets used by the high-risk AI system to ensure they are relevant, representative, and free of errors and biases?",
          "controls": [
            {
              "control": "[Art-10][Par-2][1] - Implement and document data governance and management practices covering the entire data lifecycle, including design choices, collection, and preparation processes like annotation and cleaning.",
              "control_objective": "To ensure that data used for high-risk AI systems is handled systematically and responsibly, maintaining quality and integrity from collection to use."
            },
            {
              "control": "[Art-10][Par-2][2] - Establish a process to examine data sets for possible biases that could negatively impact fundamental rights, health, or safety, and implement measures to detect, prevent, and mitigate these biases.",
              "control_objective": "To minimize the risk of discriminatory or unfair outcomes and ensure the AI system operates in a manner that is safe and respects fundamental rights."
            },
            {
              "control": "[Art-10][Par-3][3] - Ensure that training, validation, and testing data sets are relevant, sufficiently representative, free of errors, and complete for the system's intended purpose, with appropriate statistical properties.",
              "control_objective": "To build a robust and reliable AI system by using high-quality data that accurately reflects the operational environment and minimizes performance issues."
            },
            {
              "control": "[Art-10][Par-4][4] - Verify that data sets account for the specific geographical, contextual, behavioral, or functional settings in which the high-risk AI system will be used.",
              "control_objective": "To ensure the AI system performs effectively and as intended in its specific operational context, reducing the risk of failures due to environmental mismatches."
            },
            {
              "control": "[Art-10][Par-5][5] - Where strictly necessary for bias detection and correction, process special categories of personal data only with appropriate safeguards, technical limitations, and security measures, ensuring data is deleted after use.",
              "control_objective": "To enable effective bias mitigation while upholding the highest standards of data protection and privacy for sensitive personal information."
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "[Article 12] - Inadequate Traceability and Record-Keeping",
          "question": "Does the organization need to trace the operational history of its high-risk AI system to investigate incidents, audit results, or ensure accountability?",
          "controls": [
            {
              "control": "[Art-12][Par-1][1] - Implement capabilities for the automatic recording of events (logs) while the high-risk AI system is operating.",
              "control_objective": "To ensure a level of traceability of the AI system’s functioning throughout its lifecycle that is appropriate to its intended purpose."
            },
            {
              "control": "[Art-12][Par-2][2] - For high-risk AI systems covered by specific points in Annex III (e.g., biometrics, law enforcement), ensure logging capabilities record the period of each use, the reference database, the input data, and the identity of the persons involved in verifying the results.",
              "control_objective": "To provide detailed operational transparency and accountability for AI systems used in critical public and justice-related applications."
            },
            {
              "control": "[Art-12][Par-4][3] - For AI systems intended for remote biometric identification, ensure logging capabilities record the period of use, reference database, input data, and the identification of the person generating the match.",
              "control_objective": "To enhance auditability and accountability in the use of sensitive remote biometric identification technologies."
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "[Article 13] - Lack of Transparency and Provision of Information to Users",
          "question": "Will users of this AI system need to understand its capabilities, limitations, and the meaning of its outputs to use it safely and effectively?",
          "controls": [
            {
              "control": "[Art-13][Par-1] - Ensure the design of high-risk AI systems allows users to interpret outputs and use the system appropriately.",
              "control_objective": "To enable safe and effective use of the AI system by ensuring user comprehension."
            },
            {
              "control": "[Art-13][Par-2] - Provide clear, complete, and accessible instructions for use with all high-risk AI systems.",
              "control_objective": "To ensure users have the necessary information to operate the AI system correctly and safely."
            },
            {
              "control": "[Art-13][Par-3a] - Include the identity and contact details of the provider and their authorized representative in the instructions for use.",
              "control_objective": "To establish clear lines of communication and accountability for the AI system."
            },
            {
              "control": "[Art-13][Par-3b] - Detail the AI system's characteristics, capabilities, limitations, intended purpose, accuracy, robustness, cybersecurity, and performance metrics in the instructions for use.",
              "control_objective": "To provide a comprehensive understanding of the AI system's operational parameters and performance expectations."
            },
            {
              "control": "[Art-13][Par-3c-e-f] - Specify the necessary hardware resources, expected lifetime, maintenance, and pre-determined changes for operating the AI system in the instructions for use.",
              "control_objective": "To ensure users have the required infrastructure and information to run and maintain the AI system effectively over its lifecycle."
            },
            {
              "control": "[Art-13][Par-3d] - Detail the human oversight measures from Article 14, including technical aids for interpreting system outputs, in the instructions for use.",
              "control_objective": "To facilitate effective human oversight and intervention in the AI system's operation."
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "[Article 14] - Ineffective or Insufficient Human Oversight",
          "question": "Does the AI system operate in a way that requires human monitoring, intervention, or decision-making to prevent or mitigate risks to health, safety, or fundamental rights?",
          "controls": [
            {
              "control": "[Art-14][Par-1] - Design and develop high-risk AI systems with appropriate human-machine interface tools to enable effective oversight by natural persons.",
              "control_objective": "To ensure that a human can effectively monitor and control the AI system while it is in use."
            },
            {
              "control": "[Art-14][Par-2] - Implement human oversight to prevent or minimize risks to health, safety, or fundamental rights, especially those risks that persist after other requirements have been applied.",
              "control_objective": "To provide a final layer of risk mitigation through active human involvement."
            },
            {
              "control": "[Art-14][Par-3] - Ensure human oversight measures are built into the AI system by the provider or are appropriate for implementation by the deployer.",
              "control_objective": "To integrate necessary oversight capabilities either directly into the system or into the operational procedures of the user."
            },
            {
              "control": "[Art-14][Par-4a, 4b, 4c] - Enable assigned human overseers to understand the AI system's capabilities and limitations, monitor for anomalies, and correctly interpret its output, while remaining aware of potential automation bias.",
              "control_objective": "To empower human overseers with the knowledge and awareness needed to make informed judgments about the system's performance."
            },
            {
              "control": "[Art-14][Par-4d, 4e] - Enable assigned human overseers to have the ability to decide not to use the system, override its output, or interrupt its operation via a 'stop' button or similar procedure.",
              "control_objective": "To ensure ultimate human control over the AI system's actions and decisions in any given situation."
            },
            {
              "control": "[Art-14][Par-5] - For remote biometric identification systems, ensure that any identification is verified and confirmed by at least two competent, trained, and authorized natural persons before action is taken.",
              "control_objective": "To increase the reliability and accountability of critical identification tasks performed by AI."
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "[Article 15] - Inadequate Accuracy, Robustness, and Cybersecurity",
          "question": "Will poor AI performance (e.g., incorrect or unreliable outputs) cause business disruption, compliance issues, or customer dissatisfaction?",
          "controls": [
            {
              "control": "[Art-15][Par-1] - Ensure high-risk AI systems are designed and developed to achieve and maintain an appropriate level of accuracy, robustness, and cybersecurity throughout their lifecycle.",
              "control_objective": "To maintain the system's trustworthiness and prevent harm from inaccurate or insecure operation."
            },
            {
              "control": "[Art-15][Par-2] - Encourage the development of benchmarks and measurement methodologies for accuracy and robustness in cooperation with relevant stakeholders.",
              "control_objective": "To establish standardized methods for evaluating and verifying the performance of AI systems."
            },
            {
              "control": "[Art-15][Par-3] - Clearly state the levels of accuracy and the relevant accuracy metrics in the AI system's instructions for use.",
              "control_objective": "To provide transparency to users about the system's expected performance."
            },
            {
              "control": "[Art-15][Par-4] - Design AI systems to be resilient to errors, faults, or inconsistencies, using technical redundancies and fail-safe plans where appropriate, and mitigate risks from biased feedback loops in learning systems.",
              "control_objective": "To ensure the system can handle unexpected situations and maintain stable performance without being negatively influenced by its own outputs."
            },
            {
              "control": "[Art-15][Par-5] - Implement cybersecurity measures to protect high-risk AI systems from unauthorized alteration of their use, outputs, or performance by exploiting vulnerabilities.",
              "control_objective": "To safeguard the AI system against malicious attacks such as data poisoning, model poisoning, and adversarial examples."
            }
          ]
        }
      ]
    }
  ],
  "Phase - Knowledge Base Creation (Offline or Real-Time Data Processing)": [
    {
      "StepName": "xInternal Data Sources",
      "WebFormTitle": "To uphold the principles of data integrity, relevance, currency, and compliance by creating a governed process to identify, review, and approve all proprietary data sources designated for the AI's knowledge base.",
      "Objectives": [
        {
          "Objective": "1. Data Integrity: This principle is about ensuring the information we use is the official source of truth. It means we select data from authoritative systems and explicitly exclude unofficial sources like draft documents, personal notes, or unverified information. This ensures the AI's answers are not just relevant and current, but are based on correct, approved company data."
        },
        {
          "Objective": "2. Data Relevance: This principle ensures that we only feed the AI information related to its designated function. Just as you wouldn't give a pilot a cookbook to fly a plane, we must prevent irrelevant data from polluting the AI's knowledge base. This step establishes a clear scope to keep the AI's answers focused, accurate, and helpful."
        },
        {
          "Objective": "3. Data Currency: An AI's answers are only as reliable as the information it has learned from. This step establishes a process to ensure that only up-to-date, current documents are used, and that obsolete information is explicitly excluded. This prevents the AI from providing answers based on outdated policies, which could lead to significant business risks."
        },
        {
          "Objective": "4. Data Compliance: This is a critical governance checkpoint to protect our company, employees, and customers. Before any data source is approved, it is reviewed to ensure it adheres to privacy laws (like GDPR) and our internal ethics policies. This prevents sensitive personal information or other inappropriate content from being included in the AI's knowledge base."
        }
      ],
      "Fields": [
        {
          "FieldName": "(A.4.2, A.4.3) - Dataset Repository",
          "FieldLabel": "Dataset Repository",
          "FieldText": "All data comprising the AI systems dataset will be aggregated and stored in a central data repository.  List the path of the data repository.",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "(A.4.2, A.4.3) - Dataset Description",
          "FieldLabel": "Dataset Description",
          "FieldText": "Brief overview of the dataset's content and general purpose.",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "(A.4.2, A.4.3, A.7.5) - Source",
          "FieldLabel": "Where does the data originate from?",
          "FieldText": "",
          "FieldType": "MultiSelect:Internal database/Third-party vendor/Public repository/Synthetic generation"
        },
        {
          "FieldName": "(A.4.2, A.4.3) - Intended Use",
          "FieldLabel": "What is the specific purpose(s) of the data?",
          "FieldText": "",
          "FieldType": "Dropdown box with values:/Training data/Validation data/Test data/Production data"
        },
        {
          "FieldName": "(A.4.2, A.4.3, A.7.3, A.7.2) - Acquisition of data",
          "FieldLabel": "How will the Data be acquired and selected?",
          "FieldText": "",
          "FieldType": "MultiSelect:Extracted from internal company databases (e.g., CRM, ERP)/Sourced from a publicly available dataset/Purchased or licensed from a third-party data provider/Collected directly from users with explicit consent/Scraped from public websites in compliance with terms of service/Streamed from IoT sensors or application logs/Artificially generated (synthetic data)/Selected based on defined quality and relevance criteria/Manually curated by subject matter experts/Sampled to ensure fair representation of subgroups (stratified sampling)/A combination of multiple sources"
        },
        {
          "FieldName": "(A.4.2, A.4.3) - Retention Schedules",
          "FieldLabel": "What is the data Retention schedules based on legal and operational requirements?",
          "FieldText": "",
          "FieldType": "Dropdown box with values:/1 Year/5 Years/10 Years"
        },
        {
          "FieldName": "(A.4.2, A.4.3) - Refresh Schedules",
          "FieldLabel": "How often does the data need to be refreshed?",
          "FieldText": "",
          "FieldType": "MultiSelect:Adhoc based on regulatory changes/Monthly/Quarterly/Yearly"
        },        
        {
          "FieldName": "(A.4.2, A.4.3) - Secure Disposing",
          "FieldLabel": "Secure Disposing",
          "FieldText": "Descripbe the Secure methods for disposing of obsolete or redundant data.",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "(A.4.2, A.4.3) - Approximate Size",
          "FieldLabel": "Approximate Size",
          "FieldText": "What is the Estimated size of the data (e.g., number of records, GB, TB)?",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "(A.4.2, A.4.3) - Owner/Custodian",
          "FieldLabel": "Owner/Custodian",
          "FieldText": "Person or team responsible for the data maintenance, refresh and disposal?",
          "FieldType": "TextBox"
        },
		{
		  "FieldType": "risk",
		  "FieldName": "Malicious Data Ingestion (Data Poisoning & Indirect Prompt Injection)",
		  "RiskDescription": "The risk that malicious actors could deliberately corrupt the internal data sources (e.g., SharePoint, Confluence) that feed the AI's knowledge base. This can be done by inserting false information to mislead users (Data Poisoning) or by embedding hidden commands to hijack the AI's behavior and potentially leak data (Indirect Prompt Injection).",
		  "question": "Does the AI system ingest data from internal sources (like wikis, shared drives, or databases) where a number of users have permission to create or edit content?",
		  "controls": [
			{
			  "control": "[DATA-INT-01] - Implement and enforce Role-Based Access Control (RBAC) with the principle of least privilege on all source data repositories to restrict write and modify permissions to only authorized personnel.",
			  "control_objective": "To prevent unauthorized users from introducing malicious or erroneous content at the source, directly mitigating risks of intentional Data Poisoning.",
			  "control_evidence": "A documented list of user roles and their assigned permissions for the data repository; screenshots or configuration exports from the system's access control panel demonstrating the principle of least privilege; and records of periodic access reviews."
			},
			{
			  "control": "[DATA-INT-02] - Enforce mandatory version control with detailed audit logs on all source data repositories. All changes, additions, and deletions must be attributable to a specific user and timestamp.",
			  "control_objective": "To ensure a complete, auditable history of all changes to the knowledge base's source data, enabling rapid detection of unauthorized modifications and rollback to a last-known-good state.",
			  "control_evidence": "Screenshots of the version control system's settings demonstrating that versioning is active, and a sample commit/change history log showing a clear attribution of changes to a specific user, timestamp, and a description of the change."
			},
			{
			  "control": "[DATA-INT-03] - Establish a mandatory content approval workflow for the addition or significant modification of documents in designated high-sensitivity data sources before they are ingested by the AI system.",
			  "control_objective": "To create a formal human-in-the-loop verification gate that ensures the authenticity and appropriateness of critical information, providing a strong defense against both deliberate Data Poisoning and unintentional quality issues.",
			  "control_evidence": "Documentation of the content approval process including designated approvers, and sample evidence such as a completed Pull Request with mandatory reviewer approvals, or a change management ticket (e.g., in Jira) with a logged approval signature."
			},
			{
			  "control": "[DATA-INT-04] - Ensure every data chunk processed and stored in the vector database retains immutable metadata linking it directly to its source document, version, and author.",
			  "control_objective": "To maintain full data provenance, enabling users and administrators to verify the source of any information provided by the AI and to facilitate the targeted removal of compromised data if a poisoning incident is discovered.",
			  "control_evidence": "A sample query output from the vector database displaying a data chunk alongside its associated metadata fields (e.g., source_document_name, version_id, ingestion_timestamp), and a code snippet from the data ingestion pipeline explicitly demonstrating how this metadata is extracted and attached."
			},
			{
			  "control": "[DATA-INT-05] - During the 'Text Extraction & Cleaning' phase of the Data Processing Pipeline, implement a sanitization step that identifies and neutralizes or strips potential prompt injection payloads before the text is sent for embedding.",
			  "control_objective": "To create an automated defense layer that actively blocks malicious instructions from ever entering the vector knowledge base, protecting the LLM from being hijacked by poisoned source data.",
			  "control_evidence": "Code review of the data processing script showing the sanitization function or library in use (e.g., using regular expressions or a dedicated library like Rebuff). Unit test results demonstrating that known prompt injection strings are successfully identified and removed from a sample document."
			},
			{
			  "control": "[DATA-INT-06] - The RAG Orchestrator must be configured to include citations or direct links back to the source documents (leveraging the metadata from control DATA-INT-04) within every response generated by the LLM.",
			  "control_objective": "To empower users to verify the AI's claims and sources, thereby reducing the impact of successful data poisoning by making false information easily identifiable and auditable.",
			  "control_evidence": "Screenshots or logs of the final user-facing response clearly showing embedded citations or links. A code snippet from the RAG Orchestrator demonstrating how the source metadata from the retrieved chunks is formatted and appended to the final LLM response."
			}
		  ]
		}
      ]
    },
    {
      "StepName": "xData Processing Pipeline (Vectorise proprietary data)",
      "WebFormTitle": "To uphold the integrity and trustworthiness of the AI's knowledge base by ensuring all source data is accurately extracted, thoroughly cleaned of irrelevant artifacts, and logically chunked for optimal processing.",
      "Objectives": [
        {
          "Objective": "ToDo: 1. Accurate Extraction: This is the first and most crucial step to ensure the AI learns from the correct information. It involves carefully reading the source documents, including complex formats like scanned PDFs, to create a perfect digital text copy. An error here could be like the AI misreading a word, which could alter the meaning of a key fact."
        },
        {
          "Objective": "2. Thorough Cleaning: Documents often contain digital noise that isn't part of the core knowledge, such as page numbers, headers, footers, or website navigation links. This step acts as a filter, removing this irrelevant information so the AI can focus purely on the valuable content, leading to clearer and more relevant answers."
        },
        {
          "Objective": "3. Logical Chunking: An AI cannot process an entire 100-page document at once to answer a single question. This step intelligently breaks down long documents into smaller, bite-sized, and contextually complete paragraphs or ideas. This is like creating a perfectly indexed and bookmarked version of the knowledge, making it possible for the AI to quickly find the single most relevant piece of information later on."
        }
      ],
      "Fields": [
        {
          "FieldType": "plan",
          "FieldName": "(A.7.6) Sensitive Data Preparation Plan",
          "PlanObjective": "Ensure that raw sensitive data is anonymised before it is used by the AI system.",
          "PlanCriteria": [
            {
              "criteria": "ANON-PSEUD - 01: Direct personal identifiers must be pseudonymized using established techniques such as masking, tokenization, or hashing.",
              "control_objective": "To obscure or replace sensitive data elements in a reversible or non-reversible way, preventing direct identification of individuals while preserving data utility.",
              "criteria_evidence": "Data processing scripts or configuration files demonstrating the application of pseudonymization techniques. A comparative data sample showing data before and after the transformations, with sensitive fields appropriately masked, tokenized, or hashed."
            },
            {
              "criteria": "ANON-MIN - 01: The principle of data minimization must be applied by reducing data precision and removing unnecessary features.",
              "control_objective": "To reduce the risk of re-identification and adhere to data privacy principles by ensuring only essential data with the minimum required level of detail is processed.",
              "criteria_evidence": "The data schema for the final processed dataset, confirming the absence of pruned columns. A comparative data sample showing data before and after generalization has been applied to relevant fields."
            },
            {
              "criteria": "ANON-INT - 01: The dataset must be free of duplicate records to ensure its integrity and quality.",
              "control_objective": "To prevent data skew, improve processing efficiency, and ensure that each data point is unique, leading to a more reliable and clean dataset for the AI system.",
              "criteria_evidence": "Logs or reports from the data preparation script that quantify the number of duplicate records identified and removed. A data sample demonstrating the dataset before and after the de-duplication process."
            }
          ],
          "PlanSteps": [
            {
              "step": "ANON-PSEUD-TEST-01: Apply masking to a sample of raw data and verify that sensitive identifiers are correctly obscured with generic characters.",
              "step_objective": "To confirm that the masking technique is correctly implemented to limit the exposure of sensitive identifiers, fulfilling a component of criterion ANON-PSEUD-01."
            },
            {
              "step": "ANON-PSEUD-TEST-02: Apply tokenization to a sample of raw data and verify that sensitive elements are replaced with non-sensitive tokens.",
              "step_objective": "To validate that tokenization effectively substitutes sensitive values, reducing risk while meeting the requirements of criterion ANON-PSEUD-01."
            },
            {
              "step": "ANON-PSEUD-TEST-03: Apply a salted cryptographic hash function (e.g., SHA-256) to a sensitive field and verify that the output is a fixed-size, irreversible string.",
              "step_objective": "To ensure that data is protected using a one-way cryptographic method, satisfying the hashing requirement of criterion ANON-PSEUD-01."
            },
            {
              "step": "ANON-MIN-TEST-01: Apply generalization to a sample of data (e.g., age, date) and verify that the precision is correctly reduced to a less specific category (e.g., age range, year).",
              "step_objective": "To test that generalization is effectively reducing the risk of re-identification, providing evidence for criterion ANON-MIN-01."
            },
            {
              "step": "ANON-MIN-TEST-02: Execute the feature pruning script and verify that the specified non-essential columns are removed from the output dataset.",
              "step_objective": "To confirm adherence to the data minimization principle by eliminating superfluous data, as required by criterion ANON-MIN-01."
            },
            {
              "step": "ANON-INT-TEST-01: Process a sample dataset containing known duplicates and verify that the de-duplication script correctly identifies and removes the redundant records.",
              "step_objective": "To validate the effectiveness of the de-duplication process, ensuring the final dataset meets the integrity standard of criterion ANON-INT-01."
            }
          ]
        },
        {
          "FieldType": "plan",
          "FieldName": "(A.7.4, A.7.6) - Data quality requirements & Test Plan",
          "PlanObjective": "Ensure that data used to develop and operate the AI system meet defined Data Quality Requirements?",
          "PlanCriteria": [
            {
              "criteria": "DATA-SEN - 01: The accuracy level for sensitive data must be > 95% of data points correct when compared to a trusted source.",
              "control_objective": "To ensure that sensitive data is reliable, precise, and fit for high-stakes decision-making.",
              "criteria_evidence": "A data quality report or test results from a validation script showing an accuracy score of > 95%. Documentation of the validation methodology, including the definition of the 'trusted source'. The validation script itself should be available for review."
            },
            {
              "criteria": "DATA-SEN - 02: Sensitive data must have All critical information present and all necessary data fields populated.",
              "control_objective": "To guarantee that all necessary information required for analysis and operations is present in the sensitive dataset.",
              "criteria_evidence": "A data schema or data dictionary defining all critical and necessary data fields. A data profiling report or log from an automated script that verifies completeness, showing zero null or empty values in the designated critical fields."
            },
            {
              "criteria": "DATA-SEN - 03: Sensitive data must have No contradictory information and maintain integrity across related datasets.",
              "criteria_objective": "To maintain the integrity and trustworthiness of sensitive data by eliminating logical contradictions across related datasets.",
              "criteria_evidence": "Documentation of integrity rules and constraints applied to the data. Test results from validation scripts or database constraints (e.g., unit tests, SQL queries) that check for contradictions, with logs showing zero violations found."
            },
            {
              "criteria": "DATA-SEN - 04: Sensitive data must be maintained in Real-time.",
              "criteria_objective": "To ensure that sensitive data is timely and current for its intended use, especially in contexts requiring immediate action or decision.",
              "criteria_evidence": "System logs or monitoring dashboard metrics (e.g., from Kafka, Grafana, or a data pipeline tool) showing timestamps of data ingestion and processing. A Service Level Agreement (SLA) document defining the maximum acceptable latency, with monitoring reports confirming compliance."
            },
            {
              "criteria": "DATA-SEN - 05: The sensitive data's provenance must be fully traced and verified.",
              "criteria_objective": "To enable full auditing and verification by maintaining a complete, unalterable record of the sensitive data's origin and history (provenance).",
              "criteria_evidence": "A data lineage graph or document that maps the data flow from its origin to its final state. Immutable logs (e.g., from a blockchain or write-once log system) that record all transformations, including timestamps and the identity of the process or user performing the change."
            },
            {
              "criteria": "DATA-SEN - 06: A robust version control system like Git or DVC must be applied to manage and track sensitive data versions.",
              "criteria_objective": "To ensure that changes to sensitive data are tracked, auditable, and reversible, protecting against unauthorized or erroneous modifications.",
              "criteria_evidence": "A link to the version control repository (e.g., Git, DVC). A review of the repository's commit history demonstrating consistent and meaningful commits for data changes. A README file or documentation outlining the branching and tagging strategy for data versions."
            }
          ],
          "PlanSteps": [
            {
              "step": "BBT-EXT-ACC-01: Establish a 'golden dataset' by manually and accurately extracting all critical data points from a representative sample of 10-20 source documents of varying types (PDF, DOCX, etc.).",
              "step_objective": "To create a verified, ground-truth benchmark against which the automated extraction process's accuracy can be quantitatively measured, directly testing for control DATA-SEN-01."
            },
            {
              "step": "BBT-EXT-ACC-02: Process the sample documents through the automated pipeline and programmatically compare the extracted text against the corresponding 'golden dataset', calculating the data point accuracy percentage.",
              "step_objective": "To validate that the extraction process meets the specified >95% accuracy threshold (DATA-SEN-01) and to identify specific failure modes (e.g., issues with tables, complex layouts)."
            },
            {
              "step": "BBT-EXT-COM-01: For a sample of documents, compare the total word/character count of the source file against the total count of the extracted and cleaned text. Flag any document with a deviation greater than a set threshold (e.g., 15%).",
              "step_objective": "To perform a high-level check for major data loss, such as missed pages or entire sections, ensuring the process meets the completeness requirement of DATA-SEN-02."
            },
            {
              "step": "BBT-EXT-COM-02: Define and test for 'non-splittable entities' (e.g., a person's full name and title, a complete address, a single table row). Verify that the chunking mechanism does not break these entities across multiple, separate chunks.",
              "step_objective": "To ensure the chunking process preserves the semantic integrity of the data, preventing the creation of incomplete or misleading fragments and upholding the completeness rule (DATA-SEN-02)."
            },
            {
              "step": "BBT-EXT-CON-01: Identify documents in the source data that contain known, intentional contradictions. Process these documents and verify that the extracted chunks accurately reflect the original contradictions without introducing new ones.",
              "step_objective": "To confirm that the extraction and chunking process does not introduce new errors or artifacts that could be misinterpreted as data contradictions, thereby testing the integrity preservation required by DATA-SEN-03."
            },
            {
              "step": "BBT-EXT-TIM-01: Introduce a new document or an updated version of an existing document into the source data collection. Measure the end-to-end processing time until its corresponding chunks are generated and ready for embedding.",
              "step_objective": "To empirically test the data pipeline's latency and validate that it meets the timeliness requirements defined in DATA-SEN-04 for maintaining a real-time system."
            },
            {
              "step": "BBT-EXT-PRO-01: From the output of the vectorization pipeline, select a random sample of 50 text chunks. For each chunk, verify the presence and correctness of its metadata, including source filename, document version, and page number.",
              "step_objective": "To ensure that every piece of processed data maintains a verifiable link to its origin, satisfying the full provenance tracing requirement of DATA-SEN-05."
            },
            {
              "step": "BBT-EXT-PRO-02: Using only the metadata from a sampled chunk, perform a reverse lookup to retrieve the original source document and confirm the chunk's content matches the content at the specified location (e.g., page 5, paragraph 2).",
              "step_objective": "To validate the functional reliability of the provenance metadata, ensuring it is not just present but accurate and useful for auditing and verification as per DATA-SEN-05."
            },
            {
              "step": "BBT-EXT-VER-01: Perform a test data rollback. Check out a previous version of a source document from the version control system (e.g., Git/DVC), run it through the pipeline, and verify that the output chunks exactly match the state of that older version.",
              "step_objective": "To confirm that the version control system is properly integrated and that the data processing pipeline can reliably reproduce outputs from any given historical version of the data, fulfilling control DATA-SEN-06."
            }
          ]
        }
      ]
    },
    {
      "StepName": "Indexing and storing company's proprietary data",
      "WebFormTitle": "To uphold the principles of data confidentiality, integrity, and availability for all information stored in the AI's knowledge base by implementing comprehensive encryption, strict access controls, and robust disaster recovery protocols.",
      "Objectives": [
        {
          "Objective": "1. Data Confidentiality (Encryption): This ensures that the company's proprietary information, which has been converted into an AI-readable numerical format (vectorized), is unreadable to unauthorized parties. Think of this as storing the AI's knowledge in a digital safe (encryption at rest) and using a secure, armored courier when moving it between systems (encryption in transit)."
        },
        {
          "Objective": "2. Data Integrity and Access Control: This establishes strict controls over who can add, modify, or delete information in the AI's knowledge base. It prevents both accidental corruption and malicious tampering (data poisoning) by ensuring only authorized personnel or automated processes can manage the data, with all actions logged for auditing."
        },
        {
          "Objective": "3. Availability and Resilience: This guarantees that the AI's knowledge base, now a critical business asset, is protected against loss. It involves implementing robust backup and disaster recovery plans, ensuring the system can be restored quickly and reliably in the event of a technical failure, cyber-attack, or other disruption."
        }
      ],
      "Fields": []
    },
    {
      "StepName": "Connecting Internal Data <-> the Processing Pipeline",
      "WebFormTitle": "To ensure the secure and confidential transfer of all proprietary data from internal sources into the data processing pipeline by enforcing strong encryption for all data in transit.",
      "Objectives": [
        {
          "Objective": "1. Offline Path (Knowledge Base Building): Systematically transfer and process large volumes of internal documents to build the core, persistent knowledge base. This involves extracting, chunking, and embedding data to create a comprehensive, searchable vector index of the company's proprietary information."
        },
        {
          "Objective": "2. Real-Time Path (Query Augmentation): Provide an on-the-fly processing capability for user-uploaded documents. Data is processed immediately and stored in a temporary, session-specific index, allowing the system to answer questions on new information without altering the permanent knowledge base."
        },
        {
          "Objective": "3. Data in Transit Security: Protect all data during transfer between internal sources and the processing pipeline. This involves mandating strong encryption (e.g., TLS) for both offline bulk transfers and real-time user uploads to prevent eavesdropping and data interception, ensuring confidentiality is maintained throughout the ingestion process."
        }
      ],
      "Fields": []
    }
  ],
  "Phase Chat With Your Document (Real-Time Data Processing)": [
    {
      "StepName": "User Interface",
      "WebFormTitle": "To uphold system integrity at the point of user interaction by validating user identity, sanitizing all submitted queries to neutralize potential threats, and ensuring non-repudiation through detailed audit logs.",
      "Objectives": [
        {
          "Objective": "1. Identity and Access: Establishes that the system must know who is making the request, which is the foundation for access control."
        },
        {
          "Objective": "2. Data Integrity: Covers the need to inspect the incoming data itself for malicious content, such as prompt injection, code, or other attacks that could compromise the system."
        },
        {
          "Objective": "3. Audibility and Non-repudiation: Creating an immutable record that proves a specific user performed a specific action, and they cannot later deny it. This is critical for security investigations and accountability."
        }
      ],
      "Fields": [
        {
          "FieldType": "risk",
          "FieldName": "LLM01 Prompt Injection",
          "question": "Will external users or other systems provide text prompts or instructions to the AI model?",
          "controls": [
            {
              "control": "[LLM01][1] - All user inputs within the UI must be validated to prevent the injection of malicious code.",
              "control_objective": "Prevent attackers from exploiting vulnerabilities in the UI to inject malicious code and compromise the AI system.",
              "control_evidence": "Unit test results demonstrating the rejection of malicious payloads (e.g., XSS, command injection strings)."
            },
            {
              "control": "[LLM01][2] - Implement input sanitization techniques to remove harmful characters from user inputs.",
              "control_objective": "Further mitigate the risk of malicious code injection attempts through the UI.",
              "control_evidence": "Code snippets showing the use of a sanitization library or function. Test cases with logs that display the 'before' and 'after' state of user inputs containing harmful characters."
            },
            {
              "control": "[LLM01][4] - Encrypt all sensitive data transmitted through APIs.",
              "control_objective": "Protect sensitive data from unauthorised interception or tampering during communication through APIs.",
              "control_evidence": "Web server or API gateway configuration files enforcing TLS 1.2 or higher. A report from an external security scanner (e.g., Qualys SSL Labs) confirming a strong HTTPS implementation."
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "LLM02 Insecure Output Handling",
          "question": "Will the AI output be used in other systems, documents, or communications without being verified first?",
          "controls": [
            {
              "control": "[LLM02][1] - Ensure that sensitive or IP data is not exposed in AI system outputs.",
              "control_objective": "To mitigate the risk of insecure output handling by treating LLM-generated outputs as potentially untrusted.",
              "control_evidence": "Implementation of an output filtering or sanitization layer that scans the LLM's response for sensitive data patterns (e.g., using regex or a DLP service) before it is sent to the user or downstream systems. Test results demonstrating that the filter successfully redacts or blocks known sensitive information or intellectual property."
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "LLM04 Model Denial of Service",
          "question": "Could this AI system be overwhelmed or misused in a way that impacts availability?",
          "controls": [
            {
              "control": "[LLM04][3] - Enforce API rate limits to restrict the number of requests an individual user or IP address can make within a specific timeframe.",
              "control_objective": "To control the rate of requests and prevent overwhelming the LLM with a high volume of concurrent requests.",
              "control_evidence": "Screenshots of the API gateway configuration, relevant code snippets defining the rate limits, or test results showing that requests are blocked after the limit is exceeded."
            },
            {
              "control": "[LLM04][4] - Limit the number of queued actions and the number of total actions in a system reacting to LLM responses.",
              "control_objective": "To prevent the accumulation of excessive workload and ensure that the system can effectively process LLM responses without becoming overwhelmed.",
              "control_evidence": "Configuration files from the task queue system (e.g., Celery, RabbitMQ), application code setting queue size or concurrency limits, or architectural diagrams illustrating these constraints."
            },
            {
              "control": "[LLM04][5] - Continuously monitor the resource utilisation of the LLM to identify abnormal spikes or patterns that may indicate a DoS attack.",
              "control_objective": "To detect and respond to anomalous resource usage patterns indicative of a denial of service attack on the LLM.",
              "control_evidence": "Screenshots from the monitoring dashboard (e.g., Grafana, Datadog) showing resource utilisation graphs, copies of the alert configurations for abnormal spikes, and the incident response procedure for such alerts."
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "LLM05: Supply Chain Vulnerabilities",
          "question": "Does the AI system depend on third-party libraries, tools, or models you haven’t vetted?",
          "controls": [
            {
              "control": "[LLM05][2] - Only use reputable plugins that have been tested for application requirements.",
              "control_objective": "Minimise plugin-related vulnerabilities.",
              "control_evidence": "A documented plugin vetting process, test results from plugin security assessments, and a list of approved plugins."
            },
            {
              "control": "[LLM05][4] - Maintain an up-to-date inventory using a Software Bill of Materials (SBOM).",
              "control_objective": "Track and manage components.",
              "control_evidence": "The current SBOM document for the application, evidence of a process for regularly updating the SBOM, and change logs."
            },
            {
              "control": "[LLM05][7] - Implement sufficient monitoring and a robust patching policy.",
              "control_objective": "Maintain system security and component currency.",
              "control_evidence": "The documented patching policy, vulnerability scan reports, and change management records demonstrating timely patch application."
            }
          ]
        }
      ]
    },
    {
      "StepName": "RAG Orchestrator",
      "WebFormTitle": "To ensure the secure and appropriate handling of user queries and retrieved data by enforcing strict access controls, mitigating complex inference-based attacks, and maintaining a detailed audit trail for accountability and non-repudiation.",
      "Objectives": [
        {
          "Objective": "1. Secure Workflow Management: The orchestrator acts as the central logic unit, managing the entire RAG process from receiving a user's query to retrieving data and generating a final, context-aware response."
        },
        {
          "Objective": "2. Access Control Enforcement: It must rigorously check a user's permissions *before* retrieving data from the knowledge base. This is the most critical function to prevent access control bypasses, where a user could otherwise access confidential information beyond their privilege level."
        },
        {
          "Objective": "3. Inference Attack Mitigation: The system must be designed to detect and counter inference attacks, where a user attempts to piece together sensitive information by asking numerous simple, seemingly harmless questions. This involves monitoring query patterns and applying appropriate safeguards."
        },
        {
          "Objective": "4. Audit and Accountability: To create an immutable record of all interactions, including user queries, data access events, and generated responses. This establishes a comprehensive audit trail critical for security monitoring, forensic analysis, and ensuring non-repudiation."
        }
      ],
      "Fields": [
        {
          "FieldType": "risk",
          "FieldName": "Insufficient Audit Trails and Logging",
          "question": "Will you need to track who accessed the AI system, what it did, and when — for accountability or regulatory purposes?",
          "controls": [
            {
              "control": "[RAG-ORCH][1] - The orchestrator must integrate with the Identity and Access Management (IAM) system to retrieve user permissions and enforce data access policies *before* querying the vector database or knowledge base.",
              "control_objective": "To prevent unauthorized data access by ensuring a user's privileges are validated at the point of orchestration, before any data retrieval operation is initiated.",
              "control_evidence": "Sequence diagram showing the IAM check in the RAG workflow. Code review demonstrating the API call to the IAM system and the conditional logic that blocks unauthorized queries. Integration test results showing that queries from users with insufficient permissions are denied access to specific documents."
            },
            {
              "control": "[RAG-ORCH][2] - Implement query velocity and complexity monitoring within the orchestrator. The system must flag or block users exhibiting anomalous query patterns (e.g., high-frequency, low-semantic-variance queries) indicative of an inference attack.",
              "control_objective": "To detect and mitigate attempts to reconstruct sensitive information by analyzing patterns of user queries over time.",
              "control_evidence": "Configuration files defining thresholds for rate limiting and query complexity. Logs from a security test showing a simulated inference attack being detected and the user's session being flagged or throttled. A runbook detailing the response procedure for a flagged account."
            },
            {
              "control": "[RAG-ORCH][3] - The orchestrator must generate structured, immutable audit logs for every user interaction. Logs must include a unique transaction ID, user identifier, timestamp, the full user query, the IDs of the documents retrieved from the knowledge base, and the final LLM-generated response.",
              "control_objective": "To create a complete and reliable trail of activity for security forensics, accountability, and non-repudiation, ensuring all critical stages of the RAG process are recorded.",
              "control_evidence": "Sample of the structured audit log (e.g., in JSON format) showing all required fields are populated. Architecture diagram illustrating the logging pipeline to a secure, write-once storage system (e.g., AWS CloudWatch Logs with object lock). Penetration test report confirming that logs cannot be tampered with or deleted by unauthorized users."
            },
            {
              "control": "[RAG-ORCH][4] - The orchestrator's state transitions and data handling between components (User Query -> Vector DB -> LLM) must be defined within a secure, isolated service. All inter-component communication must use authenticated and encrypted channels (e.g., mTLS).",
              "control_objective": "To protect the integrity and confidentiality of data as it moves through the RAG pipeline and prevent attackers from manipulating the workflow logic.",
              "control_evidence": "Network architecture diagram showing mTLS enforced between microservices. Infrastructure-as-code (e.g., Terraform, Kubernetes YAML) configuration files specifying the network policies and secret management for certificates. Vulnerability scan reports for the orchestrator service container and its dependencies."
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "[RAG-ORCH-01] - Failure to Enforce Permitted User Prompt Categories",
          "RiskDescription": "The risk that the RAG Orchestrator processes user prompts that do not align with the pre-defined, permitted categories (e.g., Q&A, Summarization). This can lead the orchestrator to misinterpret user intent, retrieve irrelevant documents, and generate unreliable or non-compliant responses, fundamentally undermining the system's intended use.",
          "question": "Does the RAG Orchestrator have a specific, built-in mechanism to classify incoming user prompts against the permitted categories and reject those that do not match?",
          "controls": [
            {
              "control": "[RAG-PC-01] - Implement a Prompt Classification Module: The RAG Orchestrator must include a pre-processing module that classifies the intent of every incoming user prompt into one of the pre-defined, permitted categories. Prompts that cannot be classified or are classified as out-of-scope must be rejected before the retrieval step.",
              "control_objective": "To create a robust, intelligent gatekeeper within the RAG Orchestrator that ensures only user prompts matching the system's approved functionalities are ever processed.",
              "control_evidence": "A code snippet or documentation of the classification logic (e.g., using a smaller classification model, keyword heuristics, or an LLM call); a copy of the configuration linking this module to the 'Permitted User Prompt Categories' defined in the initial procedure step; and system logs showing both successful classifications and rejections of out-of-category prompts."
            },
            {
              "control": "[RAG-PC-02] - Configure Category-Specific System Prompts: The RAG Orchestrator's prompt templating engine must be capable of dynamically adjusting the system prompt based on the classified category, reinforcing the specific task to the LLM.",
              "control_objective": "To improve the accuracy and relevance of the LLM's response by providing it with context-specific instructions tailored to the user's classified intent (e.g., 'You are a helpful assistant. Summarize the following document...').",
              "control_evidence": "A copy of the RAG Orchestrator's configuration file showing the different prompt templates for each category; and logs from the orchestrator demonstrating that the correct system prompt was selected based on a classified user query."
            },
            {
              "control": "[RAG-PC-04] - Implement Prohibited Terms Monitoring: The RAG Orchestrator must include a monitoring component that inspects user queries for keywords from a configurable blacklist of prohibited terms (e.g., 'performance review,' 'salary,' 'disciplinary action'). Matches must be logged with user and session details for later review.",
              "control_objective": "To create a critical audit trail for attempted misuse of the system, providing the necessary data for user retraining, incident response, and continuous improvement of the prompt classification module.",
              "control_evidence": "A copy of the configuration file defining the blacklist of prohibited terms; and a sample of the monitoring logs or a screenshot from the monitoring dashboard showing a user query that was successfully flagged for containing a prohibited term."
            },
            {
              "control": "[RAG-PC-03] - Provide Category-Aware UI Directives via API: The RAG Orchestrator's API must include fields in its response that inform the User Interface of the classified prompt category, enabling the UI to provide dynamic, context-aware guidance.",
              "control_objective": "To create a feedback loop where the orchestrator's classification actively guides the user, reinforcing the system's capabilities and improving the user experience.",
              "control_evidence": "A copy of the RAG Orchestrator's API documentation showing the fields for the classified category; and a sample JSON response from the API containing a classified category (e.g., 'category': 'Document Summarization')."
            }
          ]
        }
      ]
    },
    {
      "StepName": "Generic LLM",
      "WebFormTitle": "To enforce strict operational security for the self-hosted LLM by isolating its network access and implementing governed MLOps deployment workflows.",
      "Objectives": [
        {
          "Objective": "1. Resource Isolation: To contain the LLM's operational environment by strictly controlling and restricting its access to all network resources, internal services, and APIs, preventing it from performing unauthorized actions."
        },
        {
          "Objective": "2. Governed Deployment: To ensure model integrity and security by implementing automated MLOps deployment pipelines that include robust governance, tracking, and formal approval workflows for all changes."
        }
      ],
      "Fields": []
    },
    {
      "StepName": "User Interface <->  RAG Orchestrator",
      "WebFormTitle": "To ensure that all queries and responses between the user interface and the RAG orchestrator are subject to strict, user-specific access control enforcement, preventing unauthorized data disclosure.",
      "Objectives": [
        {
          "Objective": "1. Enforce Access Control: The primary vulnerability is the failure to apply a user's specific permissions. This objective is to validate that the orchestrator, despite its high-level privileges, strictly filters all knowledge base queries and results according to the individual user's access rights."
        },
        {
          "Objective": "2. Prevent Data Leakage: Directly addresses the risk of a major data breach by ensuring that sensitive documents retrieved by the orchestrator are never passed back to an unauthorized user through the communication channel."
        },
        {
          "Objective": "3. Secure Communication Channel: To protect the integrity and confidentiality of the raw question and the final answer as they are transmitted between the user interface and the orchestrator, preventing interception or tampering."
        }
      ],
      "Fields": []
    },
    {
      "StepName": "RAG Orchestrator <->  Vector Database (Retrieval)",
      "WebFormTitle": "To retrieve relevant text chunks from the knowledge base in response to a user query while mitigating the risk of unintentionally over-fetching and exposing sensitive data.",
      "Objectives": [
        {
          "Objective": "1. Contextual Retrieval: To query the vector database with the user's vectorized input to find and return the most semantically relevant information needed to generate an accurate answer."
        },
        {
          "Objective": "2. Mitigate Data Over-fetching: To address the primary risk of retrieving document chunks that contain the answer but also include collateral sensitive data, such as Personally Identifiable Information (PII)."
        },
        {
          "Objective": "3. Prevent Sensitive Data Exposure: To ensure that unintentionally retrieved confidential details are not passed on to the LLM, thus preventing their potential exposure in the final generated answer."
        }
      ],
      "Fields": []
    },
    {
      "StepName": "RAG Orchestrator <-> Generic LLM (Augmented Prompt & LLM Call)",
      "WebFormTitle": "To generate a coherent, fact-based answer by sending a context-rich prompt to the LLM while mitigating risks of prompt injection and sensitive data leakage.",
      "Objectives": [
        {
          "Objective": "1. Prompt Augmentation: To combine the user's original question with the retrieved text chunks to create a single, comprehensive 'augmented prompt' for the LLM."
        },
        {
          "Objective": "2. Mitigate Prompt Injection: To defend against malicious instructions hidden within the user's query or retrieved data that could hijack the LLM and cause it to perform unintended actions."
        },
        {
          "Objective": "3. Prevent Data Leakage: To ensure the LLM does not inadvertently include sensitive PII or other confidential information from the context in its final answer, preventing the exposure of private data."
        }
      ],
      "Fields": []
    },
    {
      "StepName": "(A.4.7) AI Lifecycle Phase requirements - Generation",
      "WebFormTitle": "To generate a coherent, fact-based answer by sending an augmented prompt to the LLM, while safeguarding against prompt injection and preventing the leakage of sensitive data.",
      "Objectives": [
        {
          "Objective": "1. Augmented Prompt Construction: To combine the user's question with the retrieved text chunks into a single, comprehensive 'augmented prompt' and send it to the LLM for final answer generation."
        },
        {
          "Objective": "2. Mitigate Prompt Injection: To defend against vulnerabilities where a malicious instruction hidden in the user's query or retrieved data could hijack the LLM, causing it to perform unintended actions."
        },
        {
          "Objective": "3. Prevent Data Leakage: To ensure the LLM does not include sensitive PII or confidential notes from the provided context in its final answer, thus preventing inadvertent exposure of private information."
        }
      ],
      "Fields": []
    }
  ],
  "Phase Deployment": [
    {
      "StepName": "x(A.6.2.5) AI Lifecycle Phase requirements - Deployment",
      "WebFormTitle": "To orchestrate a secure and compliant AI system deployment by ensuring component integrity through secure packaging, formalizing a deployment plan, and verifying all prerequisite conditions are met prior to system activation.",
      "Objectives": [
        {
          "Objective": "1. Deployment Planning: To formalize the deployment strategy by documenting the complete process, including timelines, resource allocation, technical steps, and rollback procedures, ensuring a predictable and controlled rollout."
        },
        {
          "Objective": "2. Prerequisite Verification: To confirm that the AI system meets all established technical, security, ethical, and performance benchmarks through rigorous pre-deployment testing and validation, preventing the release of a non-compliant or unstable system."
        },
        {
          "Objective": "3. Secure Packaging: To guarantee the integrity and security of the deployable artifacts by ensuring that all components, particularly container images, are built from trusted sources, scanned for vulnerabilities, and hardened before deployment."
        }
      ],
      "Fields": [
        {
          "FieldType": "risk",
          "FieldName": "Insecure AI component Packaging",
          "question": "Are any components of the AI system be packaged in containers?",
          "controls": [
            {
              "control": "PROTE.02 Configure development tools, orchestrators, and container runtimes to exclusively use encrypted channels when connecting to registries.",
              "control_objective": "To safeguard the integrity and confidentiality of container images and code during transit to and from registries.",
              "control_evidence": "Configuration files for development tools, orchestrators (e.g., Kubernetes), and container runtimes demonstrating the use of TLS-encrypted connections (e.g., registry URLs starting with 'https://')."
            },
            {
              "control": "PROTE.03 Implement time-triggered pruning of registries to remove unsafe or vulnerable container images.",
              "control_objective": "To maintain the security and integrity of container images in registries by eliminating outdated and vulnerable images.",
              "control_evidence": "Configuration of the automated pruning job (e.g., a CronJob manifest) and execution logs showing that vulnerable or old images have been successfully removed."
            },
            {
              "control": "PROTE.04 Enforce read/write access control for registries containing proprietary or sensitive container images.",
              "control_objective": "To restrict unauthorised access and modifications to container images stored in registries.",
              "control_evidence": "Screenshots or configuration exports of the registry's Role-Based Access Control (RBAC) settings, showing defined user roles and their permissions for specific repositories."
            },
            {
              "control": "PROTE.05 Control access to cluster-wide administrative accounts using strong authentication methods like multifactor authentication and single sign-on to existing directory systems where applicable.",
              "control_objective": "To ensure secure and controlled access to administrative accounts within the cluster.",
              "control_evidence": "Identity Provider (IdP) configuration showing MFA is enforced for the cluster administrator group, and the orchestrator's authentication configuration file pointing to the SSO provider (e.g., OIDC or SAML settings)."
            },
            {
              "control": "PROTE.06 Implement network isolation protocols that configure orchestrators to segregate network traffic based on sensitivity levels.",
              "control_objective": "To maintain distinct network environments for different levels of data sensitivity, enhancing overall network security.",
              "control_evidence": "Copies of network policy manifests (e.g., Kubernetes 'NetworkPolicy' YAML files) or firewall rules that define and enforce network segmentation."
            },
            {
              "control": "PROTE.07 Deploy policies that configure orchestrators to isolate deployments to specific sets of hosts based on security requirements or sensitivity levels.",
              "control_objective": "To ensure that deployments are conducted on secure, appropriate hosts in alignment with their security needs.",
              "control_evidence": "Orchestrator deployment configurations (e.g., YAML files) showing the use of node selectors, taints, and tolerations to restrict pods to specific nodes."
            },
            {
              "control": "PROTE.12 Implement mechanisms to reduce Host Operating System (OS) attack surfaces, including\na) using container-specific OSs with unnecessary services disabled (e.g., print spooler)\nb) employing read-only file systems\nc) regularly updating and patching OSs and lower-level components like the kernel\nd) validating versioning of components for base OS management and functionality.",
              "control_objective": "To minimise vulnerabilities and enhance the security of the host operating systems used in containerised environments.",
              "control_evidence": "Patch management reports, host configuration files showing a minimal OS install (e.g., CIS hardened image), disabled services, and read-only file system settings. A Software Bill of Materials (SBOM) for the host OS."
            },
            {
              "control": "PROTE.13 Establish mechanisms to prevent the mixing of containerised and non-containerised workloads on the same host instance.",
              "control_objective": "To segregate containerised workloads from non-containerised ones, reducing the risk of cross-contamination and attacks.",
              "control_evidence": "Host inventory documentation or orchestrator node labels and taints that dedicate specific hosts exclusively to containerised workloads."
            },
            {
              "control": "PROTE.14 Implement mechanisms to enforce minimal file system permissions for all containers, ensuring that they cannot mount sensitive directories on the host's file system.",
              "control_objective": "To restrict container access to the host's file system, preventing unauthorised access or manipulation of sensitive data.",
              "control_evidence": "Pod security policies or admission controller configurations that enforce restrictions on hostPath volumes. Deployment manifests showing the container 'securityContext' is configured with minimal permissions."
            },
            {
              "control": "PROTE.16 Ensure that only images from trusted image stores and registries are permitted to run in the environment.",
              "control_objective": "To safeguard the environment from untrusted or potentially harmful container images.",
              "control_evidence": "Configuration of an admission controller (e.g., OPA Gatekeeper, Kyverno) that implements a policy to only allow images from an approved list of registries."
            },
            {
              "control": "PROTE.17 Utilise network policies and firewall rules to restrict container network access and isolate sensitive workloads.",
              "control_objective": "To enhance network security by controlling container access and isolating sensitive workloads.",
              "control_evidence": "Network policy manifests (e.g., Kubernetes 'NetworkPolicy') or service mesh configurations (e.g., Istio 'AuthorizationPolicy') that define granular ingress and egress rules for pods."
            },
            {
              "control": "PROTE.18 Adopt the use of immutable containers, which cannot be altered post-deployment, wherever feasible.",
              "control_objective": "To prevent runtime attacks by ensuring container configurations remain unchanged after deployment.",
              "control_evidence": "Deployment manifests showing the container's root file system is set to read-only ('readOnlyRootFilesystem: true'). CI/CD pipeline configuration demonstrating that changes are deployed by building and shipping a new image."
            },
            {
              "control": "PROTE.19 Implement security measures for APIs, including robust API authentication mechanisms (e.g., OAuth 2.0, API keys), fine-grained access controls, and rate limiting to protect against abuse.",
              "control_objective": "To ensure the secure operation of APIs",
              "control_evidence": "API gateway configuration files or screenshots demonstrating the enforcement of authentication, authorisation (e.g., access control lists), and rate-limiting policies."
            },
            {
              "control": "PROTE.20 Images should be configured to run as non-privileged users.",
              "control_objective": "To enhance security by minimising the potential impact of a security breach from a containerised environment.",
              "control_evidence": "The 'Dockerfile' showing the 'USER' instruction is used. The deployment manifest showing the 'securityContext' specifies 'runAsNonRoot: true' and a non-zero 'runAsUser' ID."
            },
            {
              "control": "PROTE.21 Secrets should be stored outside of images and provided dynamically at runtime as needed.",
              "control_objective": "To protect sensitive information like credentials and keys by managing them securely and separately from container images.",
              "control_evidence": "Review of the 'Dockerfile' to confirm no secrets are present. Orchestrator manifests showing that secrets are mounted from a secure source (e.g., Kubernetes Secrets, HashiCorp Vault) at runtime."
            },
            {
              "control": "PROTE.22 Implement security policies and access controls at both the container and host levels to restrict unauthorised access and privilege escalation.",
              "control_objective": "To enhance container and host security by limiting access and preventing unauthorised privilege escalation.",
              "control_evidence": "Host-level AppArmor or SELinux profiles. Container-level pod security standards or custom admission controller policies that restrict privileged operations."
            },
            {
              "control": "PROTE.23 Utilise built-in security features of your containerisation platform.",
              "control_objective": "To leverage platform-specific security features to enhance the security posture of containerised applications.",
              "control_evidence": "A document or report detailing the enabled platform-specific security features, such as Kubernetes Pod Security Standards, Security Contexts, and RBAC configurations."
            },
            {
              "control": "PROTE.24 Mechanisms exist to implement resource limitations to prevent containers from consuming excessive resources and potentially causing a Denial of Service (DoS) attack.",
              "control_objective": "To prevent containers from over-utilising system resources, thereby safeguarding against resource exhaustion and DoS attacks.",
              "control_evidence": "Deployment manifests (e.g., Kubernetes pod spec) showing that CPU and memory requests and limits are defined for all containers."
            }
          ]
        }
      ]
    },
    {
      "StepName": "(A.8.4) Communication of incidents",
      "Objectives": [
        {
          "Objective": "Define and document the information, reporting, and incident communication plan for all internal and external interested parties."
        }
      ],
      "Fields": [
        {
          "FieldName": "(A.8.4) - Incident Communication Plan - Data Breach",
          "FieldLabel": "Data Breach",
          "FieldText": "Describe how incidents related to \"Unintended exposure of training data\" will be comunicated.",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "(A.8.4) - Incident Communication Plan - Model Misuse",
          "FieldLabel": "Model Misuse",
          "FieldText": "Describe how incidents related to \"AI model used outside intended scope\" will be comunicated.",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "(A.8.4) - Incident Communication Plan - Model Failure",
          "FieldLabel": "Model Failure",
          "FieldText": "Describe how incidents related to \"False predictions causing harm\" will be comunicated.",
          "FieldType": "TextBox"
        }
      ]
    },
    {
      "StepName": "(A.8.2) AI System Documentation and User Information",
      "WebFormTitle": "To provide all users and stakeholders with clear, comprehensive, and accessible information required for the safe, effective, and responsible use of the AI system.",
      "Objectives": [
        {
          "Objective": "1. Clarity and Accessibility: To ensure all documentation is written in plain language, is well-structured, and is tailored to the technical level of the intended audience, from end-users to technical administrators."
        },
        {
          "Objective": "2. Comprehensive Operational Guidance: To provide clear, step-by-step instructions for all system functionalities, including standard operating procedures and guidance for handling common errors or unexpected outputs."
        },
        {
          "Objective": "3. Transparency of Capabilities and Limitations: To explicitly document the AI system's intended use, its known limitations, and potential risks. This includes describing what the system is not designed to do to prevent misuse."
        },
        {
          "Objective": "4. Communication of User Responsibilities: To clearly communicate the 'Objectives for responsible use' to all users, ensuring they understand their role and accountability in operating the system ethically and in accordance with its intended purpose."
        }
      ],
      "Fields": []
    }
  ],
  "Phase Post deployment verification": [
    {
      "StepName": "(A.6.2.4) AI Systems verifications",
      "Objectives": [
        {
          "Objective": "1. Overall System Validation: To execute a formal suite of verification and validation tests that ensure the AI system meets all pre-defined technical, ethical, and operational requirements before deployment."
        },
        {
          "Objective": "2. Performance and Stability Verification: To validate that the AI system is robust, stable, and scalable under realistic load conditions by executing the 'Performance & Load Test Plan'."
        },
        {
          "Objective": "3. Responsible AI Compliance: To verify that the system operates ethically by confirming adherence to fairness, bias, transparency, and explainability standards through the 'Fairness and Bias' and 'Transparency and Explainability' test plans."
        },
        {
          "Objective": "4. Operational Integrity and Safety: To ensure the system can be used safely and correctly by validating its technical guardrails against misuse through the 'Purpose Limitation and Prompt Category Enforcement Test Plan'."
        }
      ],
      "Fields": [
        {
          "FieldType": "plan",
          "FieldName": "(A.12.1.2, A.14.2.5) - Performance & Load Test Plan",
          "PlanObjective": "To validate that the AI system meets its defined non-functional requirements for performance, stability, and scalability under realistic load conditions.",
          "PlanCriteria": [
            {
              "criteria": "PERF-TEST-01: The system's average API response time must remain below [e.g., 200ms] and the 95th percentile response time below [e.g., 500ms] during peak load simulation.",
              "control_objective": "To ensure a responsive user experience during high-traffic periods.",
              "criteria_evidence": "The final load test report containing time-series graphs of P50 and P95 latency metrics, demonstrating compliance with the defined Non-Functional Requirements (NFRs)."
            },
            {
              "criteria": "PERF-TEST-02: The system's server-side error rate must not exceed [e.g., 0.1%] of total requests during the peak load test.",
              "control_objective": "To confirm system stability and prevent service degradation under stress.",
              "criteria_evidence": "A summary table in the load test report showing the total number of requests, the number of failed requests, and a calculated error rate below the defined threshold."
            },
            {
              "criteria": "PERF-TEST-03: CPU and memory utilization for all system components must remain below a sustainable threshold (e.g., 80% average) without memory leaks throughout the duration of the load test.",
              "control_objective": "To ensure the system has adequate resource capacity and operates efficiently without risk of crashing due to resource exhaustion.",
              "criteria_evidence": "Monitoring dashboard screenshots (e.g., from Grafana, Datadog) or exported metrics showing CPU and memory usage over the test duration, confirming they stay within the acceptable range."
            }
          ],
          "PlanSteps": [
            {
              "step": "PLT-EXEC-01: Define and document specific Non-Functional Requirements (NFRs) for performance, including target response times (P50, P95), maximum acceptable error rate, and resource utilization thresholds.",
              "step_objective": "To establish clear, quantitative success criteria for the load tests, directly informing the benchmarks needed to pass criteria PERF-TEST-01, 02, and 03."
            },
            {
              "step": "PLT-EXEC-02: Develop automated test scripts using a load testing tool (e.g., JMeter, k6, Gatling) to simulate realistic user workflows and data processing volumes, creating profiles for baseline, peak, and stress load.",
              "step_objective": "To create a repeatable and accurate simulation of real-world user traffic, ensuring the test conditions are valid for assessing system performance."
            },
            {
              "step": "PLT-EXEC-03: Provision a dedicated, production-like test environment and configure monitoring tools to capture key performance indicators (KPIs) including latency, error rates, CPU, and memory usage for all services.",
              "step_objective": "To ensure that test results are accurate and representative of production performance, and that all necessary data is collected for analysis."
            },
            {
              "step": "PLT-EXEC-04: Execute the load test profiles against the test environment, starting with a baseline and gradually ramping up to peak and stress levels, while actively monitoring system health.",
              "step_objective": "To systematically apply stress to the system in a controlled manner to gather performance data across different load scenarios."
            },
            {
              "step": "PLT-EXEC-05: Aggregate and analyze the collected metrics, comparing the results against the predefined NFRs. Identify any performance bottlenecks or components that fail to meet the criteria.",
              "step_objective": "To quantitatively evaluate the system's performance against the success criteria (PERF-TEST-01, 02, 03) and pinpoint areas for optimization."
            },
            {
              "step": "PLT-EXEC-06: Compile a comprehensive load testing report summarizing the methodology, NFRs, test results (including graphs and tables), and a final conclusion on whether the system passed the performance benchmarks.",
              "step_objective": "To produce the final piece of evidence required by all criteria, providing a clear record of the test outcomes for stakeholder review and approval."
            }
          ]
        },
        {
          "FieldType": "plan",
          "FieldName": "(A.7.4) Fairness and Bias in AI Systems Test Plan",
          "PlanObjective": "Ensure AI systems are designed, developed, and tested with defined fairness objectives to prevent discriminatory or inequitable outcomes.",
          "PlanCriteria": [
            {
              "criteria": "FAIR-CTX - 01: The context for fairness analysis, including the system's purpose, favorable outcomes, and protected attributes, must be clearly defined and documented.",
              "control_objective": "To establish a clear and unambiguous framework for all subsequent fairness assessments, ensuring that tests are relevant to the system's specific use case.",
              "criteria_evidence": "A signed-off document specifying: 1. The AI system's intended purpose. 2. The definition of the 'favorable outcome' variable. 3. A comprehensive list of protected attributes relevant to the legal jurisdiction (e.g., GDPR in Luxembourg)."
            },
            {
              "criteria": "FAIR-REP - 01: The dataset's demographic composition must be representative of the target real-world population.",
              "control_objective": "To identify and mitigate representation bias, ensuring that the dataset does not systematically under- or over-represent certain demographic groups.",
              "criteria_evidence": "A data profiling report that includes: 1. Visualizations (e.g., bar charts) of the distribution of all subgroups for each protected attribute. 2. A comparative analysis of these distributions against external population benchmarks (e.g., census data, applicant pool statistics)."
            },
            {
              "criteria": "FAIR-HIST - 01: The dataset must not exhibit significant statistical disparities in historical outcomes between protected groups.",
              "control_objective": "To prevent the AI model from learning and amplifying past discriminatory patterns present in the training data.",
              "criteria_evidence": "A historical bias analysis report demonstrating: 1. Calculation of favorable outcome rates for all subgroups. 2. Application of a statistical disparity test (e.g., the Four-Fifths Rule), with results showing that no subgroup's outcome rate is less than 80% of the most favorable group's rate."
            },
            {
              "criteria": "FAIR-PRX - 01: Features that could act as proxies for protected attributes must be identified, analyzed, and documented.",
              "control_objective": "To prevent indirect discrimination by ensuring the model does not rely on non-protected features that are highly correlated with sensitive attributes.",
              "criteria_evidence": "A correlation analysis report (e.g., correlation matrix, feature importance analysis) that identifies features highly correlated with protected attributes. A documented decision for the inclusion or exclusion of each identified proxy."
            },
            {
              "criteria": "FAIR-GOV - 01: A formal governance process must be in place to report on and mitigate identified fairness risks.",
              "control_objective": "To ensure accountability and a structured, auditable response to any fairness issues discovered in the dataset before model development.",
              "criteria_evidence": "1. A formal 'Dataset Bias Report' summarizing all findings from the context, representation, historical, and proxy analyses. 2. A documented and approved mitigation plan outlining the actions to be taken (e.g., re-sampling, re-weighting, risk acceptance)."
            }
          ],
          "PlanSteps": [
            {
              "step": "FAIR-CTX-TEST-01: Document the intended purpose and operational context of the dataset (e.g., screening job applicants, predicting loan defaults).",
              "step_objective": "To establish the specific fairness context, which dictates the relevance of different biases and informs the selection of appropriate fairness metrics, as required by FAIR-CTX-01."
            },
            {
              "step": "FAIR-CTX-TEST-02: Identify and document the 'favorable outcome' variable and its values within the dataset (e.g., loan_approved = 1, hired = TRUE).",
              "step_objective": "To provide a clear, measurable target for performing historical bias analysis and calculating outcome rates across groups, fulfilling a key part of criterion FAIR-CTX-01."
            },
            {
              "step": "FAIR-CTX-TEST-03: List the protected attributes relevant to the system's context and jurisdiction (e.g., Gender, Age, Ethnicity, Disability per GDPR in Luxembourg).",
              "step_objective": "To define the specific demographic and sensitive groups against which all subsequent fairness and bias tests will be conducted, completing the requirements for FAIR-CTX-01."
            },
            {
              "step": "FAIR-REP-TEST-01: For each protected attribute, calculate and visualize the distribution of individuals across all subgroups (e.g., bar charts showing counts for each gender).",
              "step_objective": "To quantify the demographic composition of the dataset and identify any underrepresentation, directly testing for criterion FAIR-REP-01."
            },
            {
              "step": "FAIR-REP-TEST-02: Compare the dataset's demographic proportions against relevant real-world population benchmarks (e.g., national census data, applicant pool statistics).",
              "step_objective": "To formally assess for representation bias by determining if the dataset is a skewed or accurate sample of the target population, providing the core evidence for FAIR-REP-01."
            },
            {
              "step": "FAIR-HIST-TEST-01: Calculate and compare the rate of the 'favorable outcome' for each subgroup within a protected attribute (e.g., calculate the loan approval rate for males vs. females).",
              "step_objective": "To measure for historical bias by identifying disparities in past outcomes, which is the first step in validating criterion FAIR-HIST-01."
            },
            {
              "step": "FAIR-HIST-TEST-02: Apply a quantitative disparity threshold, such as the 'Four-Fifths (80%) Rule', to determine if the observed differences in outcome rates are statistically significant.",
              "step_objective": "To provide a concrete, defensible method for confirming that the dataset meets the disparity requirements of criterion FAIR-HIST-01."
            },
            {
              "step": "FAIR-PRX-TEST-01: Conduct a correlation analysis to identify non-protected features that are strong predictors of protected attributes (e.g., check correlation between zip code and ethnicity).",
              "step_objective": "To uncover hidden sources of bias by identifying proxy variables, thereby generating the required analysis report for criterion FAIR-PRX-01."
            },
            {
              "step": "FAIR-GOV-TEST-01: Compile all findings into a formal 'Dataset Bias Report' detailing representation biases, historical biases, and identified proxies.",
              "step_objective": "To create a comprehensive, auditable record of the fairness assessment, producing the primary piece of evidence required by FAIR-GOV-01."
            },
            {
              "step": "FAIR-GOV-TEST-02: Based on the report, select and document a clear mitigation strategy (e.g., collect more data, apply re-sampling/re-weighting, remove proxies, or formally accept the risk).",
              "step_objective": "To ensure that identified biases are not ignored and that a deliberate, documented action plan is put in place, fulfilling the mitigation plan requirement of FAIR-GOV-01."
            }
          ]
        },
        {
          "FieldType": "plan",
          "FieldName": "(A.7.4) Transparency and Explainability in AI Systems Test Plan",
          "PlanObjective": "Ensure AI systems are designed, developed and tested to provide understandable and sufficient information about its decisions to affected individuals.",
          "PlanCriteria": [
            {
              "criteria": "TRN-DEF - 01: Explainability requirements, including audiences, their specific needs, and legal obligations, must be formally defined and documented.",
              "control_objective": "To establish a clear, stakeholder-aligned foundation for the design and implementation of the explanation system, ensuring it is fit-for-purpose and compliant.",
              "criteria_evidence": "A signed-off 'Explainability Requirements Document' that specifies: 1. A list of all identified audiences (e.g., end-users, developers). 2. The core questions each audience needs answered. 3. A review of applicable legal obligations (e.g., GDPR)."
            },
            {
              "criteria": "TRN-TECH - 01: Appropriate explanation techniques must be selected and implemented to cover both global model behavior and local, individual predictions.",
              "control_objective": "To ensure the system has the technical capability to generate explanations that address the full spectrum of stakeholder needs, from high-level validation to individual case analysis.",
              "criteria_evidence": "System architecture diagrams and documentation that specify: 1. The rationale for model selection (interpretable vs. complex). 2. The chosen global explanation technique (e.g., Feature Importance). 3. The chosen local explanation technique (e.g., SHAP, LIME)."
            },
            {
              "criteria": "TRN-VAL - 01: The implemented explanation system must be technically robust, plausible, and stable.",
              "control_objective": "To ensure that the generated explanations are reliable, accurate, and trustworthy, preventing misleading or nonsensical outputs.",
              "criteria_evidence": "Validation test reports including: 1. Evidence of successful integration into the prediction pipeline. 2. A log of domain expert reviews confirming the plausibility of sample explanations. 3. Results from stability tests showing consistent explanations for minor input variations."
            },
            {
              "criteria": "TRN-COM - 01: Technical explanation outputs must be translated into formats that are understandable, accessible, and actionable for each target audience.",
              "control_objective": "To bridge the gap between complex model outputs and user comprehension, thereby achieving true transparency and empowering users.",
              "criteria_evidence": "A 'Communication & UI/UX Design' document containing: 1. Natural language templates for end-user explanations. 2. Designs for actionable or counterfactual explanations. 3. Mockups or screenshots of visualizations for internal dashboards."
            }
          ],
          "PlanSteps": [
            {
              "step": "TRN-DEF-TEST-01: Identify and document all audiences for explanations, categorizing them into groups such as Developers/Auditors, Business Owners, and Affected End-Users.",
              "step_objective": "To gather the necessary inputs for the 'Explainability Requirements Document' and satisfy a core component of criterion TRN-DEF-01."
            },
            {
              "step": "TRN-DEF-TEST-02: For each identified audience, define and document the primary question the explanation needs to answer (e.g., debugging, business logic validation, or personal recourse).",
              "step_objective": "To ensure explanations are relevant and useful, fulfilling the audience needs analysis required by criterion TRN-DEF-01."
            },
            {
              "step": "TRN-DEF-TEST-03: Conduct and document a review of legal and regulatory obligations for explainability, such as those related to GDPR in Luxembourg.",
              "step_objective": "To verify that the system's transparency features are designed for legal compliance, as mandated by criterion TRN-DEF-01."
            },
            {
              "step": "TRN-TECH-TEST-01: For a given use case, justify the choice of model type (interpretable vs. complex) based on performance and transparency trade-offs.",
              "step_objective": "To test that the model selection process aligns with the principle of using the simplest effective model, as outlined in criterion TRN-TECH-01."
            },
            {
              "step": "TRN-TECH-TEST-02: For a complex model, generate and review a global explanation report (e.g., a feature importance plot) and verify it is produced correctly.",
              "step_objective": "To confirm the implementation of a global explanation technique, providing evidence for criterion TRN-TECH-01."
            },
            {
              "step": "TRN-TECH-TEST-03: For a complex model, generate a local explanation (e.g., a SHAP force plot) for a specific prediction and confirm its output.",
              "step_objective": "To confirm the implementation of a local explanation technique, providing evidence for criterion TRN-TECH-01."
            },
            {
              "step": "TRN-VAL-TEST-01: Run an end-to-end test of the prediction pipeline and verify that an explanation artifact is generated and stored alongside every prediction.",
              "step_objective": "To validate the technical integration of the XAI library, providing evidence for the robustness requirement of TRN-VAL-01."
            },
            {
              "step": "TRN-VAL-TEST-02: Submit a sample of 10-20 explanations to a domain expert for review and collect their signed approval that the explanations are logical and plausible.",
              "step_objective": "To test the contextual meaningfulness of the explanations, fulfilling the plausibility requirement of criterion TRN-VAL-01."
            },
            {
              "step": "TRN-VAL-TEST-03: Create two near-identical input samples with only minor, irrelevant variations. Generate explanations for both and verify that the resulting explanations are highly similar.",
              "step_objective": "To empirically test the output's robustness, fulfilling the stability requirement of criterion TRN-VAL-01."
            },
            {
              "step": "TRN-COM-TEST-01: Take a raw SHAP value output and process it through the natural language template. Verify the output is grammatically correct and easy to understand for a non-technical user.",
              "step_objective": "To test the effectiveness of the translation layer, providing evidence for the user-facing communication criterion TRN-COM-01."
            },
            {
              "step": "TRN-COM-TEST-02: For a simulated negative outcome (e.g., loan rejection), verify that the generated explanation includes actionable guidance or a counterfactual.",
              "step_objective": "To confirm that explanations are designed to empower users, directly testing a key component of criterion TRN-COM-01."
            },
            {
              "step": "TRN-COM-TEST-03: Generate an explanation for a business user and verify it is displayed as a simple, clear visualization on the internal dashboard.",
              "step_objective": "To ensure explanations for internal stakeholders are effective and easy to interpret, as required by criterion TRN-COM-01."
            },
            {
              "step": "TRN-GOV-TEST-01: Review the project's Model Card and confirm the presence and completeness of the 'Explainability' section.",
              "step_objective": "To audit the formal documentation and ensure it meets the governance standards set by criterion TRN-GOV-01."
            },
            {
              "step": "TRN-GOV-TEST-02: Follow the published user process to request an explanation. Verify that the request is logged and handled according to the documented procedure and timelines.",
              "step_objective": "To perform an end-to-end test of the operational process, confirming compliance with criterion TRN-GOV-01."
            }
          ]
        },
        {
          "FieldType": "plan",
          "FieldName": "(A.9.4) - Purpose Limitation and Prompt Category Enforcement Test Plan",
          "PlanObjective": "To test and validate that the RAG Orchestrator's technical controls are effective in enforcing the AI system's documented intended use by correctly classifying user prompts, rejecting out-of-scope requests, and monitoring for prohibited terms.",
          "PlanCriteria": [
            {
              "criteria": "[PL-TEST-01]: The RAG Orchestrator's Prompt Classification Module must accurately classify user prompts and reject any that are out-of-scope before the retrieval step.",
              "control_objective": "To create a robust, intelligent gatekeeper within the RAG Orchestrator that ensures only user prompts matching the system's approved functionalities are ever processed.",
              "criteria_evidence": "System logs showing both successful classifications for in-scope prompts and explicit rejections for a variety of out-of-category prompts."
            },
            {
              "criteria": "[PL-TEST-02]: The RAG Orchestrator's prompt templating engine must dynamically select the correct category-specific system prompt based on the classified user intent.",
              "control_objective": "To improve the accuracy and relevance of the LLM's response by providing it with context-specific instructions tailored to the user's classified intent.",
              "criteria_evidence": "Logs from the RAG Orchestrator demonstrating that the correct system prompt template (e.g., 'Summarization' template) was selected and applied based on the classified user query."
            },
            {
              "criteria": "[PL-TEST-03]: The RAG Orchestrator must log any user query containing a term from the prohibited terms blacklist.",
              "control_objective": "To create a critical audit trail for attempted misuse of the system, providing the necessary data for user retraining and incident response.",
              "criteria_evidence": "A screenshot from the monitoring dashboard or a sample from the monitoring logs showing that a user query containing a prohibited term was successfully flagged."
            },
            {
              "criteria": "[PL-TEST-04]: The RAG Orchestrator's API response must include the correct category-aware UI directives for consumption by the front-end.",
              "control_objective": "To create a feedback loop where the orchestrator's classification actively guides the user, reinforcing the system's capabilities and improving the user experience.",
              "criteria_evidence": "A sample JSON response from the API containing the correct classified category (e.g., 'category': 'Document Summarization') for a given user prompt."
            }
          ],
          "PlanSteps": [
            {
              "step": "PL-EXEC-01: Develop a test suite of at least 20 user prompts, including a mix of prompts that clearly fall within each permitted category and several prompts that are deliberately out-of-scope or ambiguous.",
              "step_objective": "To create a comprehensive set of test cases to validate the accuracy and robustness of the Prompt Classification Module as required by criterion [PL-TEST-01]."
            },
            {
              "step": "PL-EXEC-02: Execute the test suite against the RAG Orchestrator endpoint and analyze the system logs to verify that all in-scope prompts were correctly classified and all out-of-scope prompts were rejected with the appropriate error message.",
              "step_objective": "To empirically test and gather the evidence needed to confirm that the Prompt Classification Module meets the success criteria defined in [PL-TEST-01]."
            },
            {
              "step": "PL-EXEC-03: For each successfully classified prompt from the test suite, review the RAG Orchestrator's detailed logs to confirm that the corresponding category-specific system prompt template was selected and applied.",
              "step_objective": "To gather the log-based evidence required to validate that the dynamic prompt templating engine is functioning as specified in criterion [PL-TEST-02]."
            },
            {
              "step": "PL-EXEC-04: Submit a series of prompts that each contain a different term from the prohibited terms blacklist (e.g., 'salary', 'performance review').",
              "step_objective": "To create test events that specifically trigger the monitoring control and allow for the validation of criterion [PL-TEST-03]."
            },
            {
              "step": "PL-EXEC-05: Review the monitoring dashboard and system logs to confirm that each prompt submitted in the previous step was successfully flagged and logged with the correct user and session details.",
              "step_objective": "To collect the necessary screenshot and log evidence to prove that the Prohibited Terms Monitoring control is operational as required by [PL-TEST-03]."
            },
            {
              "step": "PL-EXEC-06: For each permitted category, submit a valid prompt and capture the full API JSON response from the RAG Orchestrator. Inspect the response to verify that the 'category' field accurately reflects the prompt's intent.",
              "step_objective": "To confirm that the orchestrator correctly communicates context to the front-end, thereby gathering the evidence needed to satisfy criterion [PL-TEST-04]."
            }
          ]
        }
      ]
    }
  ],
  "Phase Operations": [
    {
      "StepName": "(A.4.3) AI Lifecycle Phase requirements - Secure Logging and Monitoring",
      "Objectives": [
        {
          "Objective": "1. System Performance and Reliability: This is about keeping the AI healthy and efficient. By logging every interaction, we can measure key metrics like response times and success rates. This data is essential for our technical teams to identify performance bottlenecks, troubleshoot errors, and ensure the system remains reliable and responsive for all users."
        },
        {
          "Objective": "2. Transparency and Explainability: To trust the AI, we must be able to understand its thought process. This logging system functions like an airplane's black box, securely recording the user's question, the specific information the AI retrieved to form its answer, and the final response given. This is critical for investigating any incorrect answers and for explaining the AI's decision-making process to auditors or stakeholders."
        },
        {
          "Objective": "3. Fairness and Quality Assurance: This is our primary mechanism for ensuring the AI remains accurate, fair, and safe over time. By analyzing the logged data, we can proactively monitor for potential biases, detect instances where the AI may be hallucinating (straying from the provided facts), and gather the insights needed to continuously improve the quality and safety of the system."
        }
      ],
      "Fields": []
    },
    {
      "StepName": "(A.6.2.6) AI Lifecycle Phase requirements - Operation and Monitoring",
      "Objectives": [
        {
          "Objective": "Define the security requirements for lifecycle phase - Deployment, Maintenance and Updates."
        }
      ],
      "Fields": [
		{
		  "FieldType": "risk",
		  "FieldName": "Insufficient Scalability Management",
		  "question": "Will this AI be used across multiple teams, products, or regions — and is it ready to scale reliably?",
		  "controls": [
			{
			  "control": "[SC][1] - Implement separate horizontal auto-scaling mechanisms for the RAG Orchestrator (CPU-bound) and the privately-hosted LLM inference service (GPU-bound). Scaling policies for each should be based on relevant metrics (e.g., request latency for the orchestrator, GPU utilization for the LLM).",
			  "control_objective": "To independently scale the logic and generation components of the system, ensuring that a bottleneck in one does not unnecessarily consume costly resources in the other.",
			  "control_evidence": "Two distinct Infrastructure-as-Code (IaC) configurations (e.g., separate Kubernetes HPAs or Terraform auto-scaling groups). Load test results demonstrating that orchestrator and LLM instances scale independently under different load patterns. Monitoring dashboards showing separate scaling events."
			},
			{
			  "control": "[SC][2] - Architect the RAG Orchestrator and the LLM inference endpoints as stateless services. Any state required for multi-turn conversations must be managed by the User Interface or externalized to a scalable cache, not held in-memory on the backend instances.",
			  "control_objective": "To enable seamless load balancing and instant failover across all backend components, ensuring that any instance can process any incoming request without being dependent on local session data.",
			  "control_evidence": "System architecture diagram showing stateless services and external state stores. Code review of the orchestrator and LLM service handlers confirming the absence of in-memory session storage. High-availability test report showing user sessions persist after forced restarts of multiple service instances."
			},
			{
			  "control": "[SC][3] - Ensure the Permanent Index (vector database) is deployed on an architecture that supports horizontal scaling to handle high query throughput. This should be achieved by scaling out the number of nodes/pods or by implementing index sharding.",
			  "control_objective": "To prevent the vector search operation from becoming a performance bottleneck as the number of concurrent users and the size of the knowledge base grow.",
			  "control_evidence": "Vector database configuration files (e.g., Helm charts, cloud service settings) showing sharding or replica settings. Performance benchmarks demonstrating query latency remains stable as the query load increases. A documented procedure for scaling the vector database cluster."
			},
			{
			  "control": "[SC][4] - Design the asynchronous Data Processing Pipeline using a queue-based architecture (e.g., RabbitMQ, SQS). This decouples the initial vectorization of proprietary data from the core application, allowing it to handle large-volume data ingestion without impacting real-time query performance.",
			  "control_objective": "To ensure that the creation and updating of the knowledge base is a robust, fault-tolerant, and scalable process that does not degrade the responsiveness of the live chat service.",
			  "control_evidence": "Architecture diagram illustrating the message queue between the data sources and the processing pipeline. Configuration of the queue and the worker pool. Logs showing successful processing of a large batch of documents, with metrics on processing time and throughput."
			}
		  ]
		},
		{
		  "FieldType": "risk",
		  "FieldName": "[C.3.6] - Poor Management of AI System Evolution and Updates",
		  "question": "When the AI system is updated (e.g., retrained model, new features), are there formal processes for testing, documenting, and deploying these changes, including a plan to roll back if the update causes problems?",
		  "controls": [
			{
			  "control": "[C.3.6][BP-10] - Implement a CI/CD pipeline that automates the testing and deployment of AI model updates. The pipeline must enforce a sequence of validation gates (e.g., unit tests, data validation, integration tests, and model performance evaluation on a holdout dataset) before allowing a deployment.",
			  "control_objective": "To automate quality assurance, reduce human error, and ensure that only thoroughly vetted and validated model updates are promoted to production.",
			  "control_evidence": "The CI/CD pipeline configuration file (e.g., `gitlab-ci.yml`, Jenkinsfile). Test reports and logs generated by the pipeline showing successful completion of all gates. A deployment manifest that references the specific model version and code commit hash deployed."
			},
			{
			  "control": "[C.3.6][BP-11] - Utilize a version control system that atomically bundles code, data schemas/references, and model artifacts for each release. Every production deployment must be linked to a single, immutable commit hash or tag.",
			  "control_objective": "To ensure complete reproducibility of any deployed AI system version and enable reliable, one-step rollbacks to a previous stable state.",
			  "control_evidence": "Git repository history showing tagged releases. A `dvc.yaml` or similar data versioning file that pins data versions to specific code commits. Deployment logs explicitly stating the commit hash or tag being deployed for each release."
			},
			{
			  "control": "[C.3.6][BP-13] - All data ingestion pipelines must include an automated data validation gate. This gate must verify data schemas, check for statistical drift in key features, and validate data quality against predefined rules before new data is accepted into the training dataset.",
			  "control_objective": "To prevent model performance degradation caused by upstream data source changes, ensuring data integrity, consistency, and stability across model versions.",
			  "control_evidence": "Configuration files for a data validation tool (e.g., Great Expectations, Pandera). CI/CD logs showing the successful execution of the data validation step. Generated data quality reports and drift analysis dashboards."
			},
			{
			  "control": "[C.3.6][BP-14] - Any modification to the production dataset within the central data repository (as defined in control A.4.2, A.4.3), including additions, deletions, or schema changes, must be executed through a formal change management ticket that requires peer review and explicit approval from a designated data owner.",
			  "control_objective": "To maintain the integrity, traceability, and quality of the training dataset by preventing unauthorized or undocumented changes that could adversely affect model performance and reliability.",
			  "control_evidence": "Documented data change management procedure. Completed change request tickets (e.g., in Jira, ServiceNow) with approval history. Audit logs from the data repository or data pipeline tools confirming that changes were applied post-approval."
			}
		  ]
		},
		{
		  "FieldType": "risk",
		  "FieldName": "Inadequate Disaster Recovery and Business Continuity Planning",
		  "question": "What would happen to your business process if this AI system suddenly stopped working?",
		  "controls": [
			{
			  "control": "[BU][1] - Define the stateless services—specifically the RAG Orchestrator and User Interface—using Infrastructure-as-Code (IaC). The IaC templates must be version-controlled to allow for the rapid, automated deployment of a functional replica in a designated disaster recovery region.",
			  "control_objective": "To ensure the core logic and user-facing components of the RAG system can be redeployed consistently and quickly, minimizing the Recovery Time Objective (RTO) for the application's brain and entry point.",
			  "control_evidence": "Version-controlled IaC files (e.g., Terraform, CloudFormation) for the RAG Orchestrator and UI services. A disaster recovery runbook detailing the execution steps. Logs from a successful DR drill showing these services becoming operational in the failover environment."
			},
			{
			  "control": "[BU][2] - Implement automated, cross-region replication or point-in-time backups for the Permanent Index (vector database). The recovery process for this data store must be tested to ensure the Recovery Point Objective (RPO) is met.",
			  "control_objective": "To prevent catastrophic loss of the vectorized knowledge base, which is costly and time-consuming to recreate, by ensuring a recent copy is always available in a geographically separate location.",
			  "control_evidence": "Cloud console configurations or database settings showing cross-region replication or automated backups are active for the vector database. A log from a recent data restoration test. Monitoring dashboards displaying replication lag or backup completion status."
			},
			{
			  "control": "[BU][3] - Configure a DNS-based failover mechanism for the User Interface's public endpoint. The system must use health checks to monitor the primary RAG Orchestrator's availability and be able to redirect traffic to the replica in the disaster recovery region.",
			  "control_objective": "To provide a seamless transition for end-users during a regional outage by automatically or manually redirecting their requests to the healthy standby environment.",
			  "control_evidence": "DNS record configuration (e.g., AWS Route 53, Azure Traffic Manager) showing the failover routing policy. Health check configuration and status logs. A report from a planned failover test that confirms traffic was successfully redirected with minimal user-facing downtime."
			},
			{
			  "control": "[BU][4] - Ensure the privately-hosted LLM inference endpoint is deployed with a multi-region disaster recovery strategy. This must include either an active-standby deployment defined in IaC or a documented procedure to rapidly redeploy the model from a central, replicated model registry into the failover region.",
			  "control_objective": "To guarantee that the core generative capability of the system is recoverable, preventing a complete service outage if the primary LLM hosting environment becomes unavailable.",
			  "control_evidence": "IaC templates for the LLM endpoint deployment (e.g., SageMaker endpoint config, Kubernetes manifests). A DR runbook with steps for LLM failover/redeployment. Screenshots of the model artifact stored in a resilient registry (e.g., S3 with cross-region replication)."
			}
		  ]
		},
		{
		  "FieldType": "risk",
		  "FieldName": "Insufficient Performance Monitoring and Analysis",
		  "question": "Will the AI’s outputs need to be continuously monitored to ensure they stay accurate and relevant?",
		  "controls": [
			{
			  "control": "[PER][1] - Instrument the RAG Orchestrator with distributed tracing to measure the end-to-end latency of a user query. The trace must capture the duration of each critical operation: retrieval from the vector index, augmentation of the prompt, and response generation by the LLM.",
			  "control_objective": "To pinpoint performance bottlenecks within the live RAG chain, enabling targeted optimization of either the retrieval or generation components to improve user-perceived response time.",
			  "control_evidence": "A distributed tracing dashboard (e.g., Jaeger, Datadog) showing a waterfall diagram of a user request. A metrics dashboard displaying P95/P99 latency for each individual step (retrieval, generation)."
			},
			{
			  "control": "[PER][2] - Implement dedicated monitoring for the privately-hosted LLM inference endpoint. Dashboards must track GPU utilization, memory usage, time-to-first-token, and output tokens per second, with alerts configured for significant degradations.",
			  "control_objective": "To ensure the most resource-intensive component of the stack is operating efficiently and to proactively detect hardware or model issues before they impact the entire system.",
			  "control_evidence": "A monitoring dashboard (e.g., Grafana) with panels for GPU metrics and inference latency. The configuration file for the alerting rules (e.g., in Prometheus/Alertmanager). Load test reports showing performance under stress."
			},
			{
			  "control": "[PER][3] - Establish a feedback loop by logging a sample of user queries, the IDs of the retrieved document chunks, and the final generated response. Implement an offline job to periodically calculate retrieval quality metrics (e.g., hit rate, Mean Reciprocal Rank) against a golden dataset.",
			  "control_objective": "To monitor the relevance and accuracy of the RAG system's outputs over time, detecting concept drift or degradation in retrieval quality as the knowledge base evolves.",
			  "control_evidence": "A sample of the logged data from the production system. A script from a CI/CD pipeline or scheduled job that runs the evaluation. A report or dashboard showing the trend of retrieval quality metrics over time."
			},
			{
			  "control": "[PER][4] - Instrument the asynchronous Data Processing Pipeline to monitor its throughput (documents processed per hour) and error rate. Track the average time spent on each stage: text extraction, chunking, and embedding generation.",
			  "control_objective": "To ensure the knowledge base creation process is scalable and reliable, allowing for timely updates and additions to the system's proprietary data without system degradation.",
			  "control_evidence": "A metrics dashboard for the data pipeline showing throughput and error rates. Logs from pipeline workers detailing the processing time for each stage. An alert that triggers if the pipeline's failure rate exceeds a set threshold."
			}
		  ]
		},
		{
		  "FieldType": "risk",
		  "FieldName": "Inadequate Security Monitoring and Threat Detection",
		  "question": "Do you need visibility into whether this AI system is under attack or being misused?",
		  "controls": [
			{
			  "control": "[STM][1] - Implement logging for all prompts sent to the RAG Orchestrator and the final augmented prompts sent to the LLM. All LLM responses must also be logged before being sent to the user. These logs must be streamed to a centralized SIEM for analysis.",
			  "control_objective": "To create an auditable trail for detecting and investigating prompt injection attacks, data exfiltration attempts, and misuse of the generative model's capabilities.",
			  "control_evidence": "SIEM dashboard showing ingested prompt and response logs. A documented alert rule that triggers on signatures of known prompt injection techniques (e.g., 'ignore previous instructions'). Code review demonstrating the logging calls within the RAG Orchestrator."
			},
			{
			  "control": "[STM][2] - The Data Processing Pipeline must integrate a file scanning mechanism (e.g., ClamAV) to inspect all internal proprietary documents *before* text extraction. Any documents flagged as malicious must be quarantined, and an alert must be generated.",
			  "control_objective": "To prevent the ingestion of weaponized documents that could exploit vulnerabilities in the processing pipeline or poison the permanent vector index with malicious content.",
			  "control_evidence": "Logs from the file scanner showing files being successfully scanned or quarantined. An example security alert generated by a malicious test file. The pipeline's configuration file or code showing the integration of the scanning step."
			},
			{
			  "control": "[STM][3] - Enable and centralize audit logs from the Permanent Index (vector database). Configure alerts for anomalous query patterns, such as an unusually high volume of retrieval requests from a single user or attempts to enumerate large portions of the index.",
			  "control_objective": "To detect attempts to exfiltrate large amounts of proprietary data from the knowledge base or unauthorized attempts to access restricted data segments.",
			  "control_evidence": "Vector database configuration file with auditing enabled. Screenshots of the SIEM dashboard displaying query logs. A documented alert rule that triggers on a high-frequency query threshold from a single source IP or user account."
			},
			{
			  "control": "[STM][4] - Deploy network flow logging for traffic between all internal components (UI, RAG Orchestrator, LLM, Vector DB). Establish a baseline of normal traffic patterns and configure alerts for deviations, such as unexpected connections or unusually large data payloads.",
			  "control_objective": "To detect potential lateral movement by an attacker or compromised components within the architecture, particularly to and from the isolated LLM.",
			  "control_evidence": "VPC flow logs or network monitoring tool dashboards. A documented network traffic baseline report. An active alert that triggers when a service attempts to connect to another service on a non-standard port."
			}
		  ]
		}
      ]
    }
  ],
  "Phase 6": [
    {
      "StepName": "AI Systems approvals",
      "Objectives": [
        {
          "Objective": "Stakeholder Approval and Governance: To obtain formal sign-off from all relevant stakeholders, confirming that the deployment plan is sound and all prerequisites have been satisfied, thereby providing a clear governance gate and accountability for the deployment decision."
        }
      ],
      "Fields": [
        {
          "FieldName": "AI System Security Approver",
          "FieldLabel": "Security Approver",
          "FieldText": "Name/Role of the Security Aprover",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "AI System Security Approval",
          "FieldLabel": "Security Approved",
          "FieldText": "",
          "FieldType": "Option box with values:Yes/No"
        },
        {
          "FieldName": "AI System DPO Approver",
          "FieldLabel": "DPO Approver",
          "FieldText": "Name/Role of the DPO Aprover",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "AI System DPO Approval",
          "FieldLabel": "DPO Approved",
          "FieldText": "",
          "FieldType": "Option box with values:Yes/No"
        },
        {
          "FieldName": "AI System Risk Approver",
          "FieldLabel": "Risk Approver",
          "FieldText": "Name/Role of the Risk Aprover",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "AI System Risk Approval",
          "FieldLabel": "Risk Approved",
          "FieldText": "",
          "FieldType": "Option box with values:Yes/No"
        },
        {
          "FieldName": "AI System Business Approver",
          "FieldLabel": "Business Approver",
          "FieldText": "Name/Role of the Business Approver",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "AI System Business Approval",
          "FieldLabel": "Business Approved",
          "FieldText": "",
          "FieldType": "Option box with values:Yes/No"
        }
      ]
    }
  ]
}