{
    "(A.9.4) AI system's intended use and limitations": {
        "WebFormTitle": "Document the purpose, target users, and intended use cases of the AI system.",
        "Fields": [
            {
                "Control": "A.9.4 – Intended use of the AI system",
                "FieldName": "(A.9.4) - AI System ID",
                "FieldLabel": "AI System ID",
                "FieldText": "AI System Unique Number",
                "FieldType": "Auto generated number"
            },
            {
                "Control": "A.9.4 – Intended use of the AI system",
                "FieldName": "(A.9.4) - Name",
                "FieldLabel": "Name",
                "FieldText": "Name of the AI application",
                "FieldType": "TextBox"
            },
            {
                "Control": "A.9.4 – Intended use of the AI system",
                "FieldName": "(A.9.4) - Business Purpose",
                "FieldLabel": "Business Purpose",
                "FieldText": "What specific business problem or task does this system address?",
                "FieldType": "TextBox"
            },     
            {
                "Control": "A.9.4 – Intended use of the AI system",
                "FieldName": "(A.9.4) - Intended Use",
                "FieldLabel": "Intended Use",
                "FieldText": "Describe the use cases of how the AI system will solve the specific business problem or task",
                "FieldType": "TextBox"
            },   
   			{
                "Control": "A.9.4 – Intended use of the AI system",
                "FieldName": "(A.9.4) - Target Users",
                "FieldLabel": "",
    			"FieldText": "List all job titles whose daily tasks are anticipated to be altered by more than 20% due to the AI system.",
                "FieldType": "Dropdown box with values:/Employees/Customers/Analysts/Customer/Supplier/Partner/Regulator"
            },               
            {
                "FieldName": "AI system impact assessment - Employees - Benefits",
                "FieldLabel": "Employees",
                "FieldText": "Select Potential Benefits:",
                "FieldType": "Option box with values:Faster Service/Reduced Error Rate/Increased Efficiency/Personalized Training or Upskilling/Better Decision-Making Tools"
            },
            {
    			"Control": "A.9.4 – Intended use of the AI system",
    			"FieldName": "HR-WIA-2.3-AugmentationTasks",
    			"FieldLabel": "Augmented/Enhanced Tasks",
    			"FieldText": "List the high-value tasks that will be significantly improved, made more accurate, or accelerated by the AI system's assistance.",
    			"FieldType": "TextBox"
  			}  
        ]
    },
    "Workforce Transition and Adaptation for AI Integration": {
        "WebFormTitle": "Define the workforce adaptation and training strategies to address risks from job role evolution due to AI adoption.",
        "Fields": [
   			{
    			"FieldName": "HR-WIA-1.2-CoreAction",
    			"FieldLabel": "System's Core Action on Tasks",
    			"FieldText": "Identify the primary role of the AI relative to human workers.",
    			"FieldType": "Option box with values:Augmentation (assisting human judgment)/Automation (replacing tasks)/Creation (enabling new tasks)"
  			},  
    		{
    			"FieldName": "HR-WIA-2.2-AutomationTasks",
    			"FieldLabel": "Automated/Eliminated Tasks",
    			"FieldText": "List the specific tasks that will be fully automated or eliminated for the affected roles, and the estimated percentage of work time saved across the department.",
    			"FieldType": "TextBox"
  			},    
  			{
  			    "FieldName": "HR-WIA-4.1-MitigationPlan",
  			    "FieldLabel": "Primary Mitigation Strategy for Displacement",
  			    "FieldText": "If job displacement is identified, select the primary strategy for the affected workers.",
  			    "FieldType": "Option box with values:Internal Re-deployment/Transfer/Managed Attrition (No Backfill)/Voluntary Separation Package/External Layoff"
    		},
    		{
    		    "FieldName": "HR-WIA-4.2-TrainingProgram",
    		    "FieldLabel": "Structured Re-skilling Program in Place",
    		    "FieldText": "Describe the primary strategy to address the affected workers.",
    			"FieldType": "TextBox"
  		    },
    		{
    		    "FieldName": "HR-WIA-4.2-TrainingProgram",
    		    "FieldLabel": "Structured Re-skilling Program Effectiveness",
    		    "FieldText": "Describe the Training Effectiveness measures to evaluate the success of the primary strategy to address the affected workers.",
    			"FieldType": "TextBox"
  		    }  		    
        ]
    },
    "Protection of Vulnerable Populations in AI Deployment": {
        "WebFormTitle": "Define adequate measures to ensure AI deployments do not unfairly impact or discriminate against vulnerable populations, and are controls in place to prevent unethical outcomes.",
        "Fields": [
			{
    			"FieldType": "risk",
    			"FieldName": "Protection of Vulnerable Populations in AI Deployment",
    			"question": "Are there adequate measures to ensure AI deployments do not unfairly impact or discriminate against vulnerable populations, and are controls in place to prevent unethical outcomes?",
    			"controls": [
    				{
        				"control": "RM-EI-01 - 01: Establish and maintain a Vulnerable Population Protection Plan documenting approaches to safeguard vulnerable groups in AI deployments.",
        				"control_objective": "To ensure that AI deployment includes effective measures to protect vulnerable populations from unfair discrimination or unethical considerations."
              		},
              		{
        				"control": "RM-EI-01 - 02: Implement documented Impact Mitigation Strategies to reduce negative effects on vulnerable groups identified during AI system assessments.",
        				"control_objective": "To minimize the adverse impacts of AI systems on vulnerable populations through documented mitigation strategies."
              		},
              		{
        				"control": "RM-EI-01 - 03: Apply a Vulnerability Assessment Protocol to evaluate and address risks for vulnerable populations in AI system deployment.",
        				"control_objective": "To rigorously assess and manage vulnerabilities affecting sensitive groups in AI deployments."
              		},
              		{
        				"control": "RM-EI-01 - 04: Use a Vulnerability Impact Assessment Framework to formally analyze AI system effects on vulnerable communities.",
        				"control_objective": "To ensure systematic assessment of the impacts of AI systems on vulnerable populations."
              		},
              		{
        				"control": "RM-EI-01 - 05: Enforce Child Protection Protocols to safeguard children from harmful impacts of AI systems.",
        				"control_objective": "To protect minors from negative effects and risks posed by AI systems."
              		},
              		{
        				"control": "RM-EI-01 - 06: Maintain Mitigation Strategy Documents detailing plans developed to address risks identified for vulnerable populations.",
        				"control_objective": "To document and implement comprehensive mitigation strategies for vulnerable groups exposed to AI risks."
              		},
              		{
        				"control": "RM-EI-01 - 07: Prepare a Vulnerable Populations Protection Plan including strategies and evidence of safeguarding efforts.",
        				"control_objective": "To ensure robust and transparent safeguarding approaches for vulnerable populations in AI implementation."
              		},
              		{
        				"control": "RM-EI-01 - 08: Document an Adverse Impact Mitigation Report that details actions taken to reduce negative outcomes for vulnerable groups.",
        				"control_objective": "To transparently record mitigation actions and outcomes related to vulnerable populations."
              		},
              		{
        				"control": "RM-EI-01 - 09: Develop an Impact Assessment Toolkit focused on identifying and assessing risks for vulnerable groups including children.",
        				"control_objective": "To provide actionable tools and procedures for identifying risks to vulnerable populations prior to AI system deployment."
              		}
    		]
  		}
    ]
},
 	"Environmental Sustainability of AI Systems": {
        "WebFormTitle": "Define proper environmental assessments, eco-efficient practices, and sustainable lifecycle management controls to minimize ecological impacts.",
        "Fields": [
              {
                "FieldType": "risk",
                "FieldName": "Environmental Sustainability of AI Systems",
                "question": "Do AI systems incorporate proper environmental assessments, eco-efficient practices, and sustainable lifecycle management controls to minimize ecological impacts?",
                "controls": [
                      {
                        "control": "ENV-LCM-01: Measure and document the total energy consumption and associated carbon emissions (Scope 2) for the AI system across the entire lifecycle, including training, inference, and fine-tuning phases.",
                        "control_objective": "To quantify the direct environmental footprint and establish a baseline for performance against sustainability targets."
                      },
                      {
                        "control": "ENV-ALG-02: Mandate the selection of energy-efficient model architectures and computational practices (e.g., model pruning, quantization) to reduce resource intensity during development and operation.",
                        "control_objective": "To minimize computational demand and promote 'frugal AI' by integrating sustainability into the model design phase."
                      },
                      {
                        "control": "ENV-INF-03: Require deployment of AI systems only on certified green data centers (or cloud regions) that rely on renewable energy sources and employ advanced, water-efficient cooling techniques.",
                        "control_objective": "To reduce the indirect environmental impact (carbon emissions and water usage) associated with the underlying computing infrastructure."
                      },
                      {
                        "control": "ENV-HW-04: Establish a protocol for the end-of-life management of AI-specific hardware (GPUs, specialized accelerators), ensuring responsible disposal, refurbishment, or certified recycling to minimize electronic waste (e-waste).",
                        "control_objective": "To address the supply chain and hardware impact by promoting a circular economy approach for resource-intensive components."
                      },
                      {
                        "control": "ENV-DEC-05: Integrate environmental assessment results (ENV-LCM-01) into the final Go/No-Go decision process for the AI system, ensuring that the environmental cost is weighed against the intended societal benefit.",
                        "control_objective": "To embed environmental stewardship into strategic decision-making, ensuring the net effect of AI deployment on the planet is demonstrably positive."
                      }
                ]
              }   		
        ]
},
    "(A.4.4) AI Systems Software and Tooling Resources": {
        "WebFormTitle": "Define the software and tools used by the AI system.",
        "Fields": [
            {
                "FieldName": "(A.4.2, A.4.4) - Tool Name and Version",
                "FieldLabel": "Tool Name and Version",
                "FieldText": "Name of the software, library, or framework and Version number.",
                "FieldType": "TextBox"
            },
            {
                "FieldName": "(A.4.2, A.4.4) - Category",
                "FieldLabel": "",
                "FieldText": "Category of the tool?",
                "FieldType": "Dropdown box with values:/Programming Language/IDE/Data Processing/ML Framework/Version Control/Deployment"
            },
            {
                "FieldName": "(A.4.2, A.4.4) - Purpose/Use Case in Project",
                "FieldLabel": "Purpose/Use Case in Project",
                "FieldText": "Tooling Resources: How this tool will be used in the AI system's lifecycle.",
                "FieldType": "TextBox"
            }
        ]
    },
    "(A.4.5) AI Systems Computing Resources": {
        "WebFormTitle": "Define the computing resources used by the AI system",
        "Fields": [
            {
                "FieldName": "(A.4.2, A.4.5) - Computing Resource Name and Version",
                "FieldLabel": "Computing Resource Name and Version",
                "FieldText": "Name of the software, library, or framework.",
                "FieldType": "TextBox"
            },
            {
                "FieldName": "(A.4.2, A.4.5) - Category",
                "FieldLabel": "",
                "FieldText": "Category of the computing resource.",
                "FieldType": "Dropdown box with values:/Dev Workstation/ML Training Cluster/Inference API Server/Data Lake Storage/EC2/S3/SQL Database"
            },
            {
                "FieldName": "(A.4.2, A.4.5) - Lifecycle Phase(s) Supported",
                "FieldLabel": "",
                "FieldText": "Which part of the AI lifecycle does this resource supports?",
                "FieldType": "Dropdown box with values:/Development/Training/Testing/Staging/Production/Monitoring"
            }
        ]
    },
    "AI Act - Section 2: Requirements for High-Risk AI Systems": {
        "WebFormTitle": "If the AI system is categorised as high-risk according to the AI Acty, define the security requirements for - High-Risk AI Systems.",
        "Fields": [
            {
                "FieldType": "risk",
                "FieldName": "[Article 8] - Non-Compliance with High-Risk AI System Requirements",
                "question": "Is the AI system, or the product incorporating the AI system, subject to other EU regulations, and if so, how is compliance with all applicable regulations, including the AI Act, managed and documented?",
                "controls": [
                 {
                  "control": "[Art-8][Par-1][1] - Establish and maintain a documented process to ensure and demonstrate that high-risk AI systems comply with the requirements of Section 2 of the AI Act, considering the system's intended purpose and the state of the art in AI.",
                  "control_objective": "To ensure that high-risk AI systems meet all legal and technical standards set forth in the AI Act, and to have a clear, auditable trail of compliance."
                 },
                 {
                  "control": "[Art-8][Par-2][2] - For products containing a high-risk AI system that are also subject to other Union harmonisation legislation, ensure the product is fully compliant with all applicable requirements from all relevant regulations.",
                  "control_objective": "To achieve comprehensive legal compliance for products that incorporate AI systems and are subject to multiple regulatory frameworks."
                 },
                 {
                  "control": "[Art-8][Par-2][3] - Where applicable, integrate the testing, reporting, and documentation processes required by the AI Act with existing compliance processes under other EU legislation to ensure consistency, avoid duplication, and minimize administrative burden.",
                  "control_objective": "To streamline compliance activities, improve efficiency, and ensure a coherent approach to regulatory requirements across different legal instruments."
                 }
                ]
              },
              {
                "FieldType": "risk",
                "FieldName": "[Article 9] - Inadequate or Ineffective Risk Management",
                "question": "Does the organization have a documented and continuously updated risk management system for its high-risk AI systems that covers the entire lifecycle of the system?",
                "controls": [
                  {
                    "control": "[Art-9][Par-1][1] - Establish, implement, document, and maintain a comprehensive risk management system for high-risk AI systems.",
                    "control_objective": "To ensure a systematic and ongoing process for identifying, evaluating, and mitigating risks associated with high-risk AI systems throughout their lifecycle."
                  },
                  {
                    "control": "[Art-9][Par-2][2] - Implement a continuous iterative risk management process that includes regular systematic reviews and updates, covering identification, analysis, estimation, and evaluation of foreseeable risks.",
                    "control_objective": "To proactively manage and adapt to evolving risks by maintaining a dynamic and up-to-date risk management framework."
                  },
                  {
                    "control": "[Art-9][Par-2][3] - Adopt appropriate and targeted risk management measures to address identified risks, including those from post-market monitoring.",
                    "control_objective": "To effectively mitigate identified risks through the implementation of specific and relevant control measures."
                  },
                  {
                    "control": "[Art-9][Par-5][4] - Ensure that residual risks, both individual and overall, are acceptable by eliminating or reducing risks as far as technically feasible through design, implementing mitigation measures, and providing information and training.",
                    "control_objective": "To reduce the potential for harm to an acceptable level by employing a multi-layered approach to risk mitigation."
                  },
                  {
                    "control": "[Art-9][Par-6][5] - Conduct testing of high-risk AI systems to identify the most appropriate risk management measures and to ensure consistent performance and compliance with requirements.",
                    "control_objective": "To validate the effectiveness of risk management measures and ensure the AI system operates as intended."
                  },
                  {
                    "control": "[Art-9][Par-9][6] - Consider the potential adverse impact on persons under the age of 18 and other vulnerable groups when implementing the risk management system.",
                    "control_objective": "To provide heightened protection for vulnerable populations who may be disproportionately affected by the AI system."
                  }
                ]
              },
              {
                "FieldType": "risk",
                "FieldName": "[Article 10] - Poor Data Quality and Governance",
                "question": "Are there appropriate data governance and management practices in place for the training, validation, and testing datasets used by the high-risk AI system to ensure they are relevant, representative, and free of errors and biases?",
                "controls": [
                  {
                    "control": "[Art-10][Par-2][1] - Implement and document data governance and management practices covering the entire data lifecycle, including design choices, collection, and preparation processes like annotation and cleaning.",
                    "control_objective": "To ensure that data used for high-risk AI systems is handled systematically and responsibly, maintaining quality and integrity from collection to use."
                  },
                  {
                    "control": "[Art-10][Par-2][2] - Establish a process to examine data sets for possible biases that could negatively impact fundamental rights, health, or safety, and implement measures to detect, prevent, and mitigate these biases.",
                    "control_objective": "To minimize the risk of discriminatory or unfair outcomes and ensure the AI system operates in a manner that is safe and respects fundamental rights."
                  },
                  {
                    "control": "[Art-10][Par-3][3] - Ensure that training, validation, and testing data sets are relevant, sufficiently representative, free of errors, and complete for the system's intended purpose, with appropriate statistical properties.",
                    "control_objective": "To build a robust and reliable AI system by using high-quality data that accurately reflects the operational environment and minimizes performance issues."
                  },
                  {
                    "control": "[Art-10][Par-4][4] - Verify that data sets account for the specific geographical, contextual, behavioral, or functional settings in which the high-risk AI system will be used.",
                    "control_objective": "To ensure the AI system performs effectively and as intended in its specific operational context, reducing the risk of failures due to environmental mismatches."
                  },
                  {
                    "control": "[Art-10][Par-5][5] - Where strictly necessary for bias detection and correction, process special categories of personal data only with appropriate safeguards, technical limitations, and security measures, ensuring data is deleted after use.",
                    "control_objective": "To enable effective bias mitigation while upholding the highest standards of data protection and privacy for sensitive personal information."
                  }
                ]
             },
             {
                "FieldType": "risk",
                "FieldName": "[Article 12] - Inadequate Traceability and Record-Keeping",
                "question": "Does the organization need to trace the operational history of its high-risk AI system to investigate incidents, audit results, or ensure accountability?",
                "controls": [
                 {
                  "control": "[Art-12][Par-1][1] - Implement capabilities for the automatic recording of events (logs) while the high-risk AI system is operating.",
                  "control_objective": "To ensure a level of traceability of the AI system’s functioning throughout its lifecycle that is appropriate to its intended purpose."
                 },
                 {
                  "control": "[Art-12][Par-2][2] - For high-risk AI systems covered by specific points in Annex III (e.g., biometrics, law enforcement), ensure logging capabilities record the period of each use, the reference database, the input data, and the identity of the persons involved in verifying the results.",
                  "control_objective": "To provide detailed operational transparency and accountability for AI systems used in critical public and justice-related applications."
                 },
                 {
                  "control": "[Art-12][Par-4][3] - For AI systems intended for remote biometric identification, ensure logging capabilities record the period of use, reference database, input data, and the identification of the person generating the match.",
                  "control_objective": "To enhance auditability and accountability in the use of sensitive remote biometric identification technologies."
                 }
                ]
              },
              {
                "FieldType": "risk",
                "FieldName": "[Article 13] - Lack of Transparency and Provision of Information to Users",
                "question": "Will users of this AI system need to understand its capabilities, limitations, and the meaning of its outputs to use it safely and effectively?",
                "controls": [
                  {
                    "control": "[Art-13][Par-1] - Ensure the design of high-risk AI systems allows users to interpret outputs and use the system appropriately.",
                    "control_objective": "To enable safe and effective use of the AI system by ensuring user comprehension."
                  },
                  {
                    "control": "[Art-13][Par-2] - Provide clear, complete, and accessible instructions for use with all high-risk AI systems.",
                    "control_objective": "To ensure users have the necessary information to operate the AI system correctly and safely."
                  },
                  {
                    "control": "[Art-13][Par-3a] - Include the identity and contact details of the provider and their authorized representative in the instructions for use.",
                    "control_objective": "To establish clear lines of communication and accountability for the AI system."
                  },
                  {
                    "control": "[Art-13][Par-3b] - Detail the AI system's characteristics, capabilities, limitations, intended purpose, accuracy, robustness, cybersecurity, and performance metrics in the instructions for use.",
                    "control_objective": "To provide a comprehensive understanding of the AI system's operational parameters and performance expectations."
                  },
                  {
                    "control": "[Art-13][Par-3c-e-f] - Specify the necessary hardware resources, expected lifetime, maintenance, and pre-determined changes for operating the AI system in the instructions for use.",
                    "control_objective": "To ensure users have the required infrastructure and information to run and maintain the AI system effectively over its lifecycle."
                  },
                  {
                    "control": "[Art-13][Par-3d] - Detail the human oversight measures from Article 14, including technical aids for interpreting system outputs, in the instructions for use.",
                    "control_objective": "To facilitate effective human oversight and intervention in the AI system's operation."
                  }
                ]
              },
              {
                "FieldType": "risk",
                "FieldName": "[Article 14] - Ineffective or Insufficient Human Oversight",
                "question": "Does the AI system operate in a way that requires human monitoring, intervention, or decision-making to prevent or mitigate risks to health, safety, or fundamental rights?",
                "controls": [
                  {
                    "control": "[Art-14][Par-1] - Design and develop high-risk AI systems with appropriate human-machine interface tools to enable effective oversight by natural persons.",
                    "control_objective": "To ensure that a human can effectively monitor and control the AI system while it is in use."
                  },
                  {
                    "control": "[Art-14][Par-2] - Implement human oversight to prevent or minimize risks to health, safety, or fundamental rights, especially those risks that persist after other requirements have been applied.",
                    "control_objective": "To provide a final layer of risk mitigation through active human involvement."
                  },
                  {
                    "control": "[Art-14][Par-3] - Ensure human oversight measures are built into the AI system by the provider or are appropriate for implementation by the deployer.",
                    "control_objective": "To integrate necessary oversight capabilities either directly into the system or into the operational procedures of the user."
                  },
                  {
                    "control": "[Art-14][Par-4a, 4b, 4c] - Enable assigned human overseers to understand the AI system's capabilities and limitations, monitor for anomalies, and correctly interpret its output, while remaining aware of potential automation bias.",
                    "control_objective": "To empower human overseers with the knowledge and awareness needed to make informed judgments about the system's performance."
                  },
                  {
                    "control": "[Art-14][Par-4d, 4e] - Enable assigned human overseers to have the ability to decide not to use the system, override its output, or interrupt its operation via a 'stop' button or similar procedure.",
                    "control_objective": "To ensure ultimate human control over the AI system's actions and decisions in any given situation."
                  },
                  {
                    "control": "[Art-14][Par-5] - For remote biometric identification systems, ensure that any identification is verified and confirmed by at least two competent, trained, and authorized natural persons before action is taken.",
                    "control_objective": "To increase the reliability and accountability of critical identification tasks performed by AI."
                  }
                ]
              },
              {
                "FieldType": "risk",
                "FieldName": "[Article 15] - Inadequate Accuracy, Robustness, and Cybersecurity",
                "question": "Will poor AI performance (e.g., incorrect or unreliable outputs) cause business disruption, compliance issues, or customer dissatisfaction?",
                "controls": [
                  {
                    "control": "[Art-15][Par-1] - Ensure high-risk AI systems are designed and developed to achieve and maintain an appropriate level of accuracy, robustness, and cybersecurity throughout their lifecycle.",
                    "control_objective": "To maintain the system's trustworthiness and prevent harm from inaccurate or insecure operation."
                  },
                  {
                    "control": "[Art-15][Par-2] - Encourage the development of benchmarks and measurement methodologies for accuracy and robustness in cooperation with relevant stakeholders.",
                    "control_objective": "To establish standardized methods for evaluating and verifying the performance of AI systems."
                  },
                  {
                    "control": "[Art-15][Par-3] - Clearly state the levels of accuracy and the relevant accuracy metrics in the AI system's instructions for use.",
                    "control_objective": "To provide transparency to users about the system's expected performance."
                  },
                  {
                    "control": "[Art-15][Par-4] - Design AI systems to be resilient to errors, faults, or inconsistencies, using technical redundancies and fail-safe plans where appropriate, and mitigate risks from biased feedback loops in learning systems.",
                    "control_objective": "To ensure the system can handle unexpected situations and maintain stable performance without being negatively influenced by its own outputs."
                  },
                  {
                    "control": "[Art-15][Par-5] - Implement cybersecurity measures to protect high-risk AI systems from unauthorized alteration of their use, outputs, or performance by exploiting vulnerabilities.",
                    "control_objective": "To safeguard the AI system against malicious attacks such as data poisoning, model poisoning, and adversarial examples."
                  }
                ]
              }
        ]
    },
    
    
    
    "(A.4.3) AI Lifecycle Phase requirements - Data Collection": {
        "WebFormTitle": "To uphold the principles of data integrity, relevance, currency, and compliance by creating a governed process to identify, review, and approve all proprietary data sources designated for the AI's knowledge base.//1. Data Integrity: This principle is about ensuring the information we use is the official source of truth. It means we select data from authoritative systems and explicitly exclude unofficial sources like draft documents, personal notes, or unverified information. This ensures the AI's answers are not just relevant and current, but are based on correct, approved company data.//2. Data Relevance: This principle ensures that we only feed the AI information related to its designated function. Just as you wouldn't give a pilot a cookbook to fly a plane, we must prevent irrelevant data from polluting the AI's knowledge base. This step establishes a clear scope to keep the AI's answers focused, accurate, and helpful.//3. Data Currency: An AI's answers are only as reliable as the information it has learned from. This step establishes a process to ensure that only up-to-date, current documents are used, and that obsolete information is explicitly excluded. This prevents the AI from providing answers based on outdated policies, which could lead to significant business risks.//4. Data Compliance: This is a critical governance checkpoint to protect our company, employees, and customers. Before any data source is approved, it is reviewed to ensure it adheres to privacy laws (like GDPR) and our internal ethics policies. This prevents sensitive personal information or other inappropriate content from being included in the AI's knowledge base.",
        "Fields": [
            {
                "FieldName": "(A.4.2, A.4.3) - Dataset Name",
                "FieldLabel": "Dataset Name",
                "FieldText": "Common or descriptive name of the dataset.",
                "FieldType": "TextBox"
            },
            {
                "FieldName": "(A.4.2, A.4.3) - Description",
                "FieldLabel": "Description",
                "FieldText": "Brief overview of the dataset's content and general purpose.",
                "FieldType": "TextBox"
            },
            {
                "FieldName": "(A.4.2, A.4.3, A.7.5) - Source",
                "FieldLabel": "",
                "FieldText": "Where does the data originate from?",
                "FieldType": "Dropdown box with values:/Internal database/Third-party vendor/Public repository/Synthetic generation"
            },
            {
                "FieldName": "(A.4.2, A.4.3) - Intended Use",
                "FieldLabel": "",
                "FieldText": "What is the specific purpose(s) of the data?",
                "FieldType": "Dropdown box with values:/Training data/Validation data/Test data/Production data"
            },
            {
                "FieldName": "(A.4.2, A.4.3) - Format",
                "FieldLabel": "",
                "FieldText": "What is the File format or storage system?",
                "FieldType": "Dropdown box with values:/CSV/JSON/Parquet/SQL DB"
            },
            {
                "FieldName": "(A.4.2, A.4.3, A.7.3,A.7.2) - Acquisition of data",
                "FieldLabel": "",
                "FieldText": "How will the Data be acquired and selected?",
                "FieldType": "Dropdown box with values:/Extracted from internal company databases (e.g., CRM, ERP)/Sourced from a publicly available dataset/Purchased or licensed from a third-party data provider/Collected directly from users with explicit consent/Scraped from public websites in compliance with terms of service/Streamed from IoT sensors or application logs/Artificially generated (synthetic data)/Selected based on defined quality and relevance criteria/Manually curated by subject matter experts/Sampled to ensure fair representation of subgroups (stratified sampling)/A combination of multiple sources/methods/Other (requires specific documentation)"
            },
            {
                "FieldName": "(A.4.2, A.4.3) - Retention Schedules",
                "FieldLabel": "",
                "FieldText": "What is the data Retention schedules based on legal and operational requirements?",
                "FieldType": "Dropdown box with values:/1 Year/5 Years/10 Years"
            },
            {
                "FieldName": "(A.4.2, A.4.3) - Secure Disposing",
                "FieldLabel": "Secure Disposing",
                "FieldText": "Descripbe the Secure methods for disposing of obsolete or redundant data.",
                "FieldType": "TextBox"
            },
            {
                "FieldName": "(A.4.2, A.4.3) - Approximate Size",
                "FieldLabel": "Approximate Size",
                "FieldText": "What is the Estimated size of the data (e.g., number of records, GB, TB)?",
                "FieldType": "TextBox"
            },
            {
                "FieldName": "(A.4.2, A.4.3) - Access Method",
                "FieldLabel": "",
                "FieldText": "How will the data be accessed?",
                "FieldType": "Dropdown box with values:/S3 bucket path/API endpoint/Database query/Shared drive"
            },
            {
                "FieldName": "(A.4.2, A.4.3) - Owner/Custodian",
                "FieldLabel": "Owner/Custodian",
                "FieldText": "Person or team responsible for the data?",
                "FieldType": "TextBox"
            },       
			{
    			"FieldType": "risk",
    			"FieldName": "(A.7.4) - Sensitive Data qualtiy requirements",
    			"question": "Data qualtiy requirements for sensitive data?",
    			"controls": [
              		{
        				"control": "DATA-SEN - 01: The accuracy level for sensitive data must be **> 95%** of data points correct when compared to a trusted source.",
        				"control_objective": "To ensure that sensitive data is reliable, precise, and fit for high-stakes decision-making."
              		},
               		 {
         				"control": "DATA-SEN - 02: Sensitive data must have **All critical information present** and all necessary data fields populated.",
       					"control_objective": "To guarantee that all necessary information required for analysis and operations is present in the sensitive dataset."
               		 },
               		 {
        				"control": "DATA-SEN - 03: Sensitive data must have **No contradictory information** and maintain integrity across related datasets.",
        				"control_objective": "To maintain the integrity and trustworthiness of sensitive data by eliminating logical contradictions across related datasets."
              		  },
              		{
        				"control": "DATA-SEN - 04: Sensitive data must be maintained in **Real-time**.",
        				"control_objective": "To ensure that sensitive data is timely and current for its intended use, especially in contexts requiring immediate action or decision."
               		 },
              		  {
        				"control": "DATA-SEN - 04: : The sensitive data's provenance **must be fully traced and verified**.",
        				"control_objective": "To enable full auditing and verification by maintaining a complete, unalterable record of the sensitive data's origin and history (provenance)."
              		  },
              		  {
         				"control": "DATA-SEN - 06: A robust version control system like **Git** or **DVC** must be applied to manage and track sensitive data versions.",
       					"control_objective": "To ensure that changes to sensitive data are tracked, auditable, and reversible, protecting against unauthorized or erroneous modifications."
               		 }
    			]
  			},        
   			{
    			"FieldType": "risk",
    			"FieldName": "(A.7.4) - Internal data Data qualtiy requirements",
    			"question": "Data quality requirements for internal data?",
    			"controls": [
              		  {
        				"control": "DATA-INT - 01: The accuracy level for internal data must be **> 50%** of data points correct when compared to a trusted source.",
        				"control_objective": "To ensure internal data is generally reliable and minimally fit for operational use."
              		  },
              		  {
        				"control": "DATA-INT - 02: Internal data must have **No nulls in essential fields**.",
        				"control_objective": "To guarantee the presence of fundamental data points necessary for business function and reporting."
              		  },
              		  {
        				"control": "DATA-INT - 03: Internal data must have **No logical contradictions**.",
        				"control_objective": "To minimize errors and conflicting information within the internal dataset."
              		  },
              		  {
        				"control": "TDATA-INT - 04: he internal data must be updated at least **Daily**.",
        				"control_objective": "To ensure internal data is updated frequently enough to support timely business processes."
              		  },
              		  {
        				"control": "DATA-INT - 05: The internal data's provenance **should be known but full verification is not strictly required**.",
        				"control_objective": "To establish a clear, documented understanding of the origin of internal data."
              		  },
              		  {
        				"control": "DATA-INT - 06: A system like **Manual** tracking, or preferably DVC/Git, is required for managing and tracking internal data versions.",
        				"control_objective": "To provide a basic ability to audit and revert changes to the internal dataset."
              		  }    			
 
    			]
  			},   
  			{
    			"FieldType": "risk",
    			"FieldName": "(A.7.6) Sensitive Data Preparation Plan",
    			"question": "Data Preparation Plan for Sensitive data?",
    			"controls": [
              		  {
        				"control": "DATA-SEN-PRE-01: Implement masking to replace parts of sensitive data with a generic character (e.g., 'XXX-XX-1234' for a Social Security Number).",
        				"control_objective": "To limit the exposure of sensitive identifiers while retaining data utility for analysis or testing environments."
              		  },
              		  {
        				"control": "DATA-SEN-PRE-02: Implement tokenization to replace a sensitive data element with a non-sensitive equivalent ('token'). The original data must be stored in a secure vault.",
        				"control_objective": "To substitute sensitive values with surrogate non-sensitive values, thereby reducing the risk of a breach while preserving referential integrity."
              		  },
              		  {
        				"control": "DATA-SEN-PRE-03: Use a cryptographic hashing function to convert sensitive data into a fixed-size string of characters.",
        				"control_objective": "To ensure the integrity and verification of sensitive data in a one-way process, preventing the exposure of original values."
              		  },
              		  {
        				"control": "DATA-SEN-PRE-04: Apply generalization to reduce the precision of data (e.g., changing a specific age to an age range or a full date of birth to just the year).",
        				"control_objective": "To reduce the risk of re-identification by making individual sensitive data points less unique."
              		  },
              		  {
        				"control": "DATA-SEN-PRE-05: Create physically or logically separate data stores for sensitive information, secured with strict access controls (Data Segregation).",
        				"control_objective": "To minimize the scope of access and contain potential security breaches affecting sensitive information."
              		  },
              		  {
        				"control": "DATA-SEN-PRE-06: Apply Attribute-Based Access Control (ABAC) rules so that only users with specific attributes (e.g., role, department) can access certain data fields.",
        				"control_objective": "To enforce granular, dynamic access policies based on specific user and data characteristics."
              		  },
              		  {
        				"control": "DATA-SEN-PRE-07: Perform Feature Pruning to remove any data fields or columns that are not strictly necessary for the AI model's purpose.",
        				"control_objective": "To adhere to the principle of data minimization by eliminating superfluous sensitive information."
              		  },
              		  {
        				"control": "DATA-SEN-PRE-08: Remove records of individuals who have not given consent for their data to be used for AI training (Data Minimization).",
        				"control_objective": "To ensure sensitive data processing complies with legal and ethical requirements regarding individual consent."
              		  },
              		  {
        				"control": "DATA-SEN-PRE-09: Perform Integrity Checks after de-identification to verify that relationships between anonymized fields (e.g., a tokenized user ID and their records) remain intact.",
        				"control_objective": "To ensure that de-identification methods do not compromise the data's utility or internal relationships required for analysis."
              		  },
              		  {
        				"control": "DATA-SEN-PRE-10: Implement a strategy for Handling Missing Values, such as removing empty or null fields, filling them with a mean/median/mode, or removing the row/column.",
        				"control_objective": "To improve the quality and robustness of the data prior to processing by managing gaps in the dataset systematically."
              		  },
              		  {
        				"control": "DATA-SEN-PRE-11: Implement a process for Correcting Inaccuracies by identifying and fixing demonstrably incorrect data points (e.g., typos, out-of-range values).",
        				"control_objective": "To enhance the accuracy of the data by correcting known factual errors or anomalies."
              		  },
              		  {
        				"control": "DATA-SEN-PRE-12: Implement a method for Removing Duplicates by identifying and deleting redundant records from the dataset.",
        				"control_objective": "To prevent data skewing and ensure the dataset is clean by eliminating redundant records."
              		  } 			
    			]
  			},
 			{
    			"FieldType": "risk",
    			"FieldName": "(A.7.6) Internal Data Preparation Plan",
    			"question": "Data Preparation Plan for internal data?",
    			"controls": [
              		  {
        				"control": "DATA-INT-PRE-01: Implement masking to replace non-essential sensitive portions of data with a generic character (e.g., 'XXX-XX-1234' for internal IDs).",
        				"control_objective": "To limit the exposure of non-essential identifiers while retaining utility for analysis or testing environments."
              		  },
              		  {
        				"control": "DATA-INT-PRE-02: Apply generalization to reduce the precision of data where necessary for internal reporting flexibility (e.g., changing a specific age to an age range).",
        				"control_objective": "To simplify data points for better aggregation and basic analysis without focusing on individual uniqueness."
              		  },
              		  {
        				"control": "DATA-INT-PRE-03: Create logically separate data stores for high-priority internal information, secured with appropriate access controls (Data Segregation).",
        				"control_objective": "To logically separate and manage access to high-value internal data assets."
              		  },
              		  {
        				"control": "DATA-INT-PRE-04: Apply Attribute-Based Access Control (ABAC) rules so that only authorized users (e.g., based on role, department) can access certain data fields.",
        				"control_objective": "To enforce granular access policies based on the principle of least privilege."
              		  },
              		  {
        				"control": "DATA-INT-PRE-05: Perform Feature Pruning to remove any data fields or columns that are not necessary for the intended internal business process or model.",
        				"control_objective": "To apply the principle of data minimization by eliminating superfluous information, reducing storage, and improving processing efficiency."
              		  },
              		  {
        				"control": "DATA-INT-PRE-06: Perform Integrity Checks after any data transformation to verify that internal relationships between fields remain intact.",
        				"control_objective": "To ensure that data preparation methods do not compromise the data's utility or internal relationships required for analysis."
              		  },
              		  {
        				"control": "DATA-INT-PRE-07: Implement a strategy for Handling Missing Values, such as filling them with a mean/median/mode, or removing the row/column, based on documented methodology.",
        				"control_objective": "To improve the quality and robustness of the data prior to processing by managing gaps in the dataset systematically."
              		  },
              		  {
        				"control": "DATA-INT-PRE-08: Implement a process for Correcting Inaccuracies by identifying and fixing demonstrably incorrect data points (e.g., typos, out-of-range values).",
        				"control_objective": "To enhance the accuracy of the data by correcting known factual errors or anomalies."
              		  },
              		  {
        				"control": "DATA-INT-PRE-09: Implement a method for Removing Duplicates by identifying and deleting redundant records from the dataset.",
        				"control_objective": "To prevent data skewing and ensure the dataset is clean by eliminating redundant records."
              		  }	
    			]
  			},    			     
            {
  			    "FieldType": "risk",
  			    "FieldName": "Knowledge Base Preparation and Integration",
  			    "question": "Are AI systems provided internal company documents in a specialized, machine-readable format, creating a searchable knowledge base that a generic LLM can use to answer user questions accurately and without internet access.?",
  			    "controls": [
   			  {
        				"control": "RAG-PREP-01: Establish automated connections to internal data sources and extract raw content from various file formats (PDF, DOCX, HTML, etc.).",
        				"control_objective": "To ensure all required information is brought into the processing pipeline, forming the complete knowledge foundation for the AI system."
   			    },
   			    {
        				"control": "RAG-PREP-02: Clean extracted raw text by removing irrelevant artifacts (e.g., HTML tags, document headers/footers) and performing OCR on scanned documents.",
        				"control_objective": "To improve the quality and reliability of the AI's final answers by providing clean, high-fidelity source text for processing."
   			    },
   			    {
        				"control": "RAG-PREP-03: Implement a strategy to break down large documents into smaller, semantically coherent text chunks (e.g., paragraphs, sections).",
        				"control_objective": "To prepare the data for efficient retrieval and to fit within the model's context window, ensuring only the most relevant information is used to answer a query."
   			    },
   			    {
        				"control": "RAG-PREP-04: Use an embedding model to convert all text chunks into numerical vector embeddings that represent their semantic meaning.",
        				"control_objective": "To enable semantic search capabilities, allowing the system to retrieve information based on conceptual meaning rather than just keyword matching."
   			    },
   			    {
        				"control": "RAG-PREP-05: Store all generated vector embeddings and their corresponding text chunks in a specialized Vector Database.",
        				"control_objective": "To create a highly optimized and rapidly searchable index of the knowledge base, enabling real-time retrieval of relevant information at query time."
   			    }
   			]
  		    }
  		 ]
    }, 
    
    "(A.4.3) AI Lifecycle Phase requirements - Vectorise proprietary data.": {
        "WebFormTitle": "To uphold the integrity and trustworthiness of the AI's knowledge base by ensuring all source data is accurately extracted, thoroughly cleaned of irrelevant artifacts, and logically chunked for optimal processing.//1. Accurate Extraction: This is the first and most crucial step to ensure the AI learns from the correct information. It involves carefully reading the source documents, including complex formats like scanned PDFs, to create a perfect digital text copy. An error here could be like the AI misreading a word, which could alter the meaning of a key fact.//2. Thorough Cleaning: Documents often contain digital noise that isn't part of the core knowledge, such as page numbers, headers, footers, or website navigation links. This step acts as a filter, removing this irrelevant information so the AI can focus purely on the valuable content, leading to clearer and more relevant answers.//3. Logical Chunking: An AI cannot process an entire 100-page document at once to answer a single question. This step intelligently breaks down long documents into smaller, bite-sized, and contextually complete paragraphs or ideas. This is like creating a perfectly indexed and bookmarked version of the knowledge, making it possible for the AI to quickly find the single most relevant piece of information later on.",
        "Fields": []
    },     
    
    "(A.4.3) AI Lifecycle Phase requirements - Indexing and storing companiy's proprietary data": {
        "WebFormTitle": "To uphold the principles of data confidentiality, integrity, and availability for all information stored in the AI's knowledge base by implementing comprehensive encryption, strict access controls, and robust disaster recovery protocols.//1. Data Confidentiality (Encryption): This ensures that the company's proprietary information, which has been converted into an AI-readable numerical format (vectorized), is unreadable to unauthorized parties. Think of this as storing the AI's knowledge in a digital safe (encryption at rest) and using a secure, armored courier when moving it between systems (encryption in transit).//2. Data Integrity and Access Control: This establishes strict controls over who can add, modify, or delete information in the AI's knowledge base. It prevents both accidental corruption and malicious tampering (data poisoning) by ensuring only authorized personnel or automated processes can manage the data, with all actions logged for auditing.//3. Availability and Resilience: This guarantees that the AI's knowledge base, now a critical business asset, is protected against loss. It involves implementing robust backup and disaster recovery plans, ensuring the system can be restored quickly and reliably in the event of a technical failure, cyber-attack, or other disruption.",
        "Fields": []
    },   
    
    "(A.4.3) AI Lifecycle Phase requirements - User Query": {
        "WebFormTitle": "To uphold system integrity at the point of user interaction by validating user identity, sanitizing all submitted queries to neutralize potential threats, and ensuring non-repudiation through detailed audit logs. //1. Identity and Access: Establishes that the system must know who is making the request, which is the foundation for access control.//2. Data Integrity: Covers the need to inspect the incoming data itself for malicious content, such as prompt injection, code, or other attacks that could compromise the system.//3. Audibility and Non-repudiation: Creating an immutable record that proves a specific user performed a specific action, and they cannot later deny it. This is critical for security investigations and accountability.",
        "Fields": []
    },      
       
    "(A.4.3) AI Lifecycle Phase requirements - Secure Context Retrieval and Prompt Augmentation": {
        "WebFormTitle": "To uphold core data security principles by enforcing access controls, ensuring the integrity of the prompt through sanitization, and applying the principle of least privilege before providing context to the LLM.//1. Access Controls: This ensures the system acts like a responsible librarian, checking a user's library card (their permissions) before retrieving any information. It guarantees that the AI can only pull answers from documents and data that the specific user is actually authorized to view, preventing a junior employee from accessing confidential executive files.//2. Prompt Integrity and Sanitization: This is a protective measure for the AI itself. The system inspects all the information it retrieves before showing it to the Large Language Model. This step filters out any hidden malicious instructions or confusing data that could be used to trick or hijack the AI, ensuring the integrity of the final prompt.//3. Principle of Least Privilege: This is a need-to-know security measure. Even if a retrieved document contains a lot of information (e.g., a person's project status, salary, and home address), this step ensures that only the absolute minimum data required to answer the user's specific question is provided to the AI. This dramatically reduces the risk of accidentally exposing sensitive information.",
        "Fields": []
    },      
     
    "(A.4.3) AI Lifecycle Phase requirements - Secure Generation and Response Delivery": {
        "WebFormTitle": "To uphold the principles of responsible and trustworthy AI generation by applying content safety guardrails, filtering all model outputs to prevent sensitive data leakage, and delivering a final, verifiable answer with citations.//1. Content Safety Guardrails: This is the AI's code of conduct. We have safety measures in place to ensure the model's response, even when based on factual data, is helpful, professional, and free of biased or inappropriate content. These guardrails ensure the AI's output aligns with our company values.//2. Filtering to Prevent Data Leakage: This is the final security checkpoint before an answer reaches the user. The system automatically scans the AI's generated text to ensure it hasn't accidentally included any sensitive information (like personal data or confidential numbers) that might have been present in the source documents. It acts as a final privacy filter.//3. Delivering a Verifiable Answer: To build user trust, the AI doesn't just provide an answer—it shows its work. Every response is delivered with clear references or citations pointing back to the original source document(s) used. This allows users to verify the information for themselves and have confidence that the answer is based on approved, internal facts.",
        "Fields": []
    },  
        
    "(A.4.3) AI Lifecycle Phase requirements - Secure Logging and Monitoring": {
        "WebFormTitle": "To uphold the principles of AI transparency, accountability, and continuous improvement by implementing a comprehensive logging system that securely captures all interaction data, enabling performance monitoring, quality assurance, and detailed audits.//1. System Performance and Reliability: This is about keeping the AI healthy and efficient. By logging every interaction, we can measure key metrics like response times and success rates. This data is essential for our technical teams to identify performance bottlenecks, troubleshoot errors, and ensure the system remains reliable and responsive for all users.//2. Transparency and Explainability: To trust the AI, we must be able to understand its thought process. This logging system functions like an airplane's black box, securely recording the user's question, the specific information the AI retrieved to form its answer, and the final response given. This is critical for investigating any incorrect answers and for explaining the AI's decision-making process to auditors or stakeholders.//3. Fairness and Quality Assurance: This is our primary mechanism for ensuring the AI remains accurate, fair, and safe over time. By analyzing the logged data, we can proactively monitor for potential biases, detect instances where the AI may be hallucinating (straying from the provided facts), and gather the insights needed to continuously improve the quality and safety of the system.",
        "Fields": []
    },  
                 
    "(6.2.1) AI Lifecycle Phase requirements - Component Design and Development": {
        "WebFormTitle": "Define the security requirements for lifecycle phase - Design and Development.",
        "Fields": [
          {
  			"FieldType": "risk",
  			"FieldName": "Fairness and Bias in AI Systems",
  			"question": "Are AI systems designed, developed, and tested with defined fairness objectives to prevent discriminatory or inequitable outcomes for protected or vulnerable groups?",
   			"controls": [
    			    {
        				"control": "DATA-CTX-01: Document the intended purpose and operational context of the dataset (e.g., screening job applicants, predicting loan defaults).",
        				"control_objective": "To establish the specific fairness context, which dictates the relevance of different biases and informs the selection of appropriate fairness metrics."
    			    },
    			    {
        				"control": "DATA-CTX-02: Identify and document the 'favorable outcome' variable and its values within the dataset (e.g., loan_approved = 1, hired = TRUE).",
        				"control_objective": "To provide a clear, measurable target for performing historical bias analysis and calculating outcome rates across groups."
    			    },
    			    {
        				"control": "DATA-CTX-03: List the protected attributes relevant to the system's context and jurisdiction (e.g., Gender, Age, Ethnicity, Disability per GDPR in Luxembourg).",
        				"control_objective": "To define the specific demographic and sensitive groups against which all subsequent fairness and bias tests will be conducted."
    			    },
    			    {
        				"control": "DATA-REP-01: For each protected attribute, calculate and visualize the distribution of individuals across all subgroups (e.g., bar charts showing counts for each gender).",
        				"control_objective": "To quantify the demographic composition of the dataset and identify any underrepresentation of specific groups at a glance."
    			    },
    			    {
        				"control": "DATA-REP-02: Compare the dataset's demographic proportions against relevant real-world population benchmarks (e.g., national census data, applicant pool statistics).",
        				"control_objective": "To formally assess for representation bias by determining if the dataset is a skewed or accurate sample of the target population."
    			    },
    			    {
        				"control": "DATA-HIST-01: Calculate and compare the rate of the 'favorable outcome' for each subgroup within a protected attribute (e.g., calculate the loan approval rate for males vs. females).",
        				"control_objective": "To measure for historical bias by identifying disparities in past outcomes between different demographic groups recorded in the data."
    			    },
    			    {
        				"control": "DATA-HIST-02: Apply a quantitative disparity threshold, such as the 'Four-Fifths (80%) Rule', to determine if the observed differences in outcome rates are statistically significant.",
        				"control_objective": "To provide a concrete, defensible criterion for flagging potential discriminatory patterns and escalating them for review."
    			    },
    			    {
        				"control": "DATA-PRX-01: Conduct a correlation analysis to identify non-protected features that are strong predictors of protected attributes (e.g., check correlation between zip code and ethnicity).",
        				"control_objective": "To uncover hidden sources of bias by identifying proxy variables that could enable the model to discriminate even if protected attributes are removed."
    			    },
    			    {
        				"control": "DATA-GOV-01: Compile all findings into a formal 'Dataset Bias Report' detailing representation biases, historical biases, and identified proxies.",
        				"control_objective": "To create a comprehensive, auditable record of the fairness assessment, ensuring transparency and accountability for the data's state."
    			    },
    			    {
        				"control": "DATA-GOV-02: Based on the report, select and document a clear mitigation strategy (e.g., collect more data, apply re-sampling/re-weighting, remove proxies, or formally accept the risk).",
        				"control_objective": "To ensure that identified biases are not ignored and that a deliberate, documented action plan is put in place to address them before model development."
    			    }
    			
    			]
  		 },
         {
  			"FieldType": "risk",
  			"FieldName": "Transparency and Explainability in AI Systems",
  			"question": "Are AI systems designed, developed and tested to provide understandable and sufficient information about its decisions to affected individuals.",
   			"controls": [  	
   			    {
        				"control": "TRN-REQ-01: Identify and document all audiences for explanations, categorizing them into groups such as Developers/Auditors, Business Owners, and Affected End-Users.",
        				"control_objective": "To ensure that explanations are tailored and appropriate for the technical understanding and informational needs of each distinct stakeholder group."
   			    },
   			    {
        				"control": "TRN-REQ-02: For each identified audience, define and document the primary question the explanation needs to answer (e.g., debugging, business logic validation, or personal recourse).",
        				"control_objective": "To focus the design of the explanation system on providing relevant, useful, and actionable information, rather than generic technical output."
   			    },
   			    {
        				"control": "TRN-REQ-03: Conduct and document a review of legal and regulatory obligations for explainability, such as those related to GDPR in Luxembourg.",
        				"control_objective": "To ensure the system's transparency features are designed to meet or exceed legal compliance requirements for user rights and information."
   			    },
   			    {
        				"control": "TRN-TEC-01: Mandate the use of inherently interpretable models (e.g., Logistic Regression, Decision Trees) when they meet performance requirements.",
        				"control_objective": "To reduce complexity and ambiguity by favoring models that are transparent by design, making the need for post-hoc explanation techniques secondary."
   			    },
   			    {
        				"control": "TRN-TEC-02: For complex models, implement a global explanation technique (e.g., Feature Importance) to describe the model's overall behavior.",
        				"control_objective": "To provide high-level transparency into the model's general logic, enabling business owners and auditors to validate its alignment with expectations."
   			    },
   			    {
        				"control": "TRN-TEC-03: For complex models, implement a local explanation technique (e.g., SHAP or LIME) to explain individual predictions.",
        				"control_objective": "To enable the system to provide specific reasons for each decision, which is essential for debugging, user trust, and fulfilling regulatory rights."
   			    },
   			    {
        				"control": "TRN-IMP-01: Technically integrate selected XAI libraries (e.g., SHAP) into the prediction pipeline to generate explanations alongside decisions.",
        				"control_objective": "To operationalize explainability, ensuring that the capability to generate an explanation is a core, reliable feature of the production system."
   			    },
   			    {
        				"control": "TRN-IMP-02: Conduct a sanity check of generated explanations for a sample of test cases, reviewed by a domain expert to ensure they are plausible and logical.",
        				"control_objective": "To validate that the explanation outputs are not just technically correct but also contextually meaningful and aligned with real-world knowledge."
   			    },
   			    {
        				"control": "TRN-IMP-03: Test the stability of the explanation method to ensure that minor, irrelevant changes to an input do not produce disproportionately large changes in its explanation.",
        				"control_objective": "To build trust in the explanation system by verifying that its outputs are robust and reliable."
   			    },
   			    {
        				"control": "TRN-USR-01: Develop templates to translate raw technical explanation outputs (e.g., SHAP values) into clear, non-technical natural language for end-users.",
        				"control_objective": "To make explanations accessible and understandable to a non-expert audience, fulfilling the primary goal of user-facing transparency."
   			    },
   			    {
        				"control": "TRN-USR-02: For negative outcomes, design explanations to be actionable by including guidance or counterfactuals on what factors a user could potentially change.",
        				"control_objective": "To empower affected individuals by providing them with not just a reason, but also a potential path for recourse or a better future outcome."
   			    },
   			    {
        				"control": "TRN-USR-03: For internal dashboards and business users, create simple visualizations (e.g., bar charts) summarizing the key factors influencing a decision.",
        				"control_objective": "To improve the speed and clarity of comprehension for internal stakeholders who need to understand model behavior quickly."
   			    },
   			    {
        				"control": "TRN-GOV-01: Document the system's explanation capabilities, including the methods used and audiences served, in a dedicated section of the Model Card.",
        				"control_objective": "To provide a central, standardized source of truth regarding the system's transparency features for governance and auditing purposes."
   			    },
   			    {
        				"control": "TRN-GOV-02: Define and publish a formal process for end-users to request a detailed explanation for a decision, including timelines and responsible parties.",
        				"control_objective": "To operationalize the right to explanation, ensuring that user requests are handled consistently, efficiently, and in compliance with policy and regulations."
   			    }
   			]
  		 },	 
  		 {
  			"FieldType": "risk",
  			"FieldName": "Transparency and Explainability in AI Systems",
  			"question": "Are AI system's designed and developed to prevent, withstand, and recover from disruptions that could impact its functionality and availability is assessed (e.g., cyber-attacks, infrastructure failures, data corruption).",
   			"controls": [ 
    			    {
        				"control": "RES-RSK-01: Map and document the entire AI pipeline and its components to identify the system's attack surface and potential single points of failure.",
        				"control_objective": "To create a comprehensive system overview that serves as the foundation for identifying vulnerabilities and understanding potential failure scenarios."
    			    },
    			    {
        				"control": "RES-RSK-02: Document and analyze AI-specific threats, including Data Poisoning, Adversarial Attacks, Model Extraction, and Data/Model Drift.",
        				"control_objective": "To expand risk assessment beyond traditional IT threats to include unique vulnerabilities inherent in machine learning systems."
    			    },
    			    {
        				"control": "RES-RSK-03: Assess the business impact and likelihood of each identified threat and create a prioritized risk register to guide mitigation efforts.",
        				"control_objective": "To ensure that resilience resources are focused on addressing the most critical and probable threats to the AI system's functionality."
    			    },
    			    {
        				"control": "RES-DEV-01: Implement automated data validation and anomaly detection at the ingestion stage to identify and quarantine suspicious or corrupt data.",
        				"control_objective": "To establish a first line of defense against data poisoning attacks and ensure the integrity of the data entering the training pipeline."
    			    },
    			    {
        				"control": "RES-DEV-02: Apply model hardening techniques such as Adversarial Training and Input Sanitization at the inference endpoint to make the model more robust.",
        				"control_objective": "To reduce the model's vulnerability to malicious inputs and prevent attackers from easily manipulating its predictions."
    			    },
    			    {
        				"control": "RES-DEV-03: Define all system infrastructure as code (IaC) and subject it to version control and automated security scanning.",
        				"control_objective": "To ensure that the underlying infrastructure is secure, reproducible, and can be rapidly rebuilt in a consistent manner after a failure."
    			    },
    			    {
        				"control": "RES-OPS-01: Deploy inference services across multiple, geographically separate availability zones with automated load balancing and failover.",
        				"control_objective": "To ensure high availability and withstand infrastructure failures by eliminating single points of failure in the hosting environment."
    			    },
    			    {
        				"control": "RES-OPS-02: Implement and configure real-time monitoring and alerting for Data Drift, Model Performance Drift, and anomalous input patterns.",
        				"control_objective": "To enable the rapid detection of both silent performance degradation and active security threats, moving from reactive to proactive system management."
    			    },
    			    {
        				"control": "RES-REC-01: Create a fully automated CI/CD pipeline for the AI model to enable rapid, reliable redeployment from the model registry.",
        				"control_objective": "To minimize downtime and ensure a fast recovery to a known good state in the event of model corruption or critical failure."
    			    },
    			    {
        				"control": "RES-REC-02: Implement and regularly test automated backup and restoration procedures for all critical components, including training data, models, and configurations.",
        				"control_objective": "To ensure data and system integrity can be restored after a catastrophic event, such as data corruption or infrastructure loss."
    			    },
    			    {
        				"control": "RES-REC-03: Develop and maintain an AI-specific incident response playbook that details procedures and responsibilities for handling events like data poisoning or severe model drift.",
        				"control_objective": "To provide clear, actionable guidance during a crisis, ensuring a swift, coordinated, and effective response to unique AI system failures."
    			    },
    			    {
        				"control": "RES-TST-01: Schedule and conduct regular penetration tests that specifically target the AI model with simulated adversarial attacks (evasion, poisoning).",
        				"control_objective": "To proactively identify and remediate vulnerabilities in the model's defenses before they can be exploited by malicious actors."
    			    },
    			    {
        				"control": "RES-TST-02: Conduct controlled chaos engineering drills by intentionally injecting failures into the system to test automated failover and recovery mechanisms.",
        				"control_objective": "To build confidence in the system's resilience by verifying that its automated recovery and redundancy features work as designed under real-world stress."
    			    },
    			    {
        				"control": "RES-TST-03: Establish a formal process to review and update threat models, risk assessments, and response plans on at least an annual basis.",
        				"control_objective": "To ensure the system's resilience strategy remains effective and relevant by adapting to new threats, technologies, and business contexts."
    			    }   			
   			]
  		 }
        ]
    },
    "(6.2.1) AI Lifecycle Phase requirements - Component Package": {
        "WebFormTitle": "Define the security requirements for lifecycle phase - Package.",
        "Fields": [
      {
        "FieldType": "risk",
        "FieldName": "Insecure AI component Packaging",
        "question": "Are any components of the AI system be packaged in containers?",
        "controls": [
          {
            "control": "PROTE.01 Ensure masking and anonymisation of sensitive or confidential data.",
            "control_objective": "To protect sensitive data from unauthorised exposure during the packaging process."
          },
          {
            "control": "PROTE.02 Configure development tools, orchestrators, and container runtimes to exclusively use encrypted channels when connecting to registries.",
            "control_objective": "To safeguard the integrity and confidentiality of container images and code during transit to and from registries."
          },
          {
            "control": "PROTE.03 Implement time-triggered pruning of registries to remove unsafe or vulnerable container images.",
            "control_objective": "To maintain the security and integrity of container images in registries by eliminating outdated and vulnerable images."
          },
          {
            "control": "PROTE.04 Enforce read/write access control for registries containing proprietary or sensitive container images.",
            "control_objective": "To restrict unauthorised access and modifications to container images stored in registries."
          },
          {
            "control": "PROTE.05 Control access to cluster-wide administrative accounts using strong authentication methods like multifactor authentication and single sign-on to existing directory systems where applicable.",
            "control_objective": "To ensure secure and controlled access to administrative accounts within the cluster."
          },
          {
            "control": "PROTE.06 Implement network isolation protocols that configure orchestrators to segregate network traffic based on sensitivity levels.",
            "control_objective": "To maintain distinct network environments for different levels of data sensitivity, enhancing overall network security."
          },
          {
            "control": "PROTE.07 Deploy policies that configure orchestrators to isolate deployments to specific sets of hosts based on security requirements or sensitivity levels.",
            "control_objective": "To ensure that deployments are conducted on secure, appropriate hosts in alignment with their security needs."
          },
          {
            "control": "PROTE.08 Establish stringent trust mechanisms for orchestrator nodes, including\na) secure introduction to the cluster\nb) persistent node identity\nc) inventory of nodes and their connectivity states\nd) resilience to individual node compromise\ne) mutual authentication of network connections with end-to-end encryption of intracluster traffic.",
            "control_objective": "To build a secure and resilient orchestrator environment with trusted nodes and secure communications."
          },
          {
            "control": "PROTE.09 Implement mechanisms for identifying Common Vulnerabilities and Exposures (CVEs) in the runtimes deployed.",
            "control_objective": "To promptly identify and address known vulnerabilities in deployed runtimes, enhancing system security."
          },
          {
            "control": "PROTE.10 Implement robust controls at network borders for monitoring and regulating egress network traffic originating from containers.",
            "control_objective": "To control and monitor the outbound network traffic from containers, enhancing network security."
          },
          {
            "control": "PROTE.11 Develop mechanisms to automatically enforce compliance requirements and, where applicable, prevent the execution of noncompliant container images.",
            "control_objective": "To ensure that only compliant container images are deployed, maintaining a secure and regulated environment."
          },
          {
            "control": "PROTE.12 Implement mechanisms to reduce Host Operating System (OS) attack surfaces, including\na) using container-specific OSs with unnecessary services disabled (e.g., print spooler)\nb) employing read-only file systems\nc) regularly updating and patching OSs and lower-level components like the kernel\nd) validating versioning of components for base OS management and functionality.",
            "control_objective": "To minimise vulnerabilities and enhance the security of the host operating systems used in containerised environments."
          },
          {
            "control": "PROTE.13 Establish mechanisms to prevent the mixing of containerised and non-containerised workloads on the same host instance.",
            "control_objective": "To segregate containerised workloads from non-containerised ones, reducing the risk of cross-contamination and attacks."
          },
          {
            "control": "PROTE.14 Implement mechanisms to enforce minimal file system permissions for all containers, ensuring that they cannot mount sensitive directories on the host's file system.",
            "control_objective": "To restrict container access to the host's file system, preventing unauthorised access or manipulation of sensitive data."
          },
          {
            "control": "PROTE.15 Implement mechanisms for continuous updating, centralised reporting, and monitoring of image compliance to identify weaknesses and risks.",
            "control_objective": "To maintain up-to-date compliance and security status of container images, enabling proactive risk management."
          },
          {
            "control": "PROTE.16 Ensure that only images from trusted image stores and registries are permitted to run in the environment.",
            "control_objective": "To safeguard the environment from untrusted or potentially harmful container images."
          },
          {
            "control": "PROTE.17 Utilise network policies and firewall rules to restrict container network access and isolate sensitive workloads.",
            "control_objective": "To enhance network security by controlling container access and isolating sensitive workloads."
          },
          {
            "control": "PROTE.18 Adopt the use of immutable containers, which cannot be altered post-deployment, wherever feasible.",
            "control_objective": "To prevent runtime attacks by ensuring container configurations remain unchanged after deployment."
          },
          {
            "control": "PROTE.19 Implement security measures for APIs, including robust API authentication mechanisms (e.g., OAuth 2.0, API keys), fine-grained access controls, and rate limiting to protect against abuse.",
            "control_objective": "To ensure the secure operation of APIs"
          },
          {
            "control": "PROTE.20 Images should be configured to run as non-privileged users.",
            "control_objective": "To enhance security by minimising the potential impact of a security breach from a containerised environment."
          },
          {
            "control": "PROTE.21 Secrets should be stored outside of images and provided dynamically at runtime as needed.",
            "control_objective": "To protect sensitive information like credentials and keys by managing them securely and separately from container images."
          },
          {
            "control": "PROTE.22 Implement security policies and access controls at both the container and host levels to restrict unauthorised access and privilege escalation.",
            "control_objective": "To enhance container and host security by limiting access and preventing unauthorised privilege escalation."
          },
          {
            "control": "PROTE.23 Utilise built-in security features of your containerisation platform.",
            "control_objective": "To leverage platform-specific security features to enhance the security posture of containerised applications."
          },
          {
            "control": "PROTE.24 Mechanisms exist to implement resource limitations to prevent containers from consuming excessive resources and potentially causing a Denial of Service (DoS) attack.",
            "control_objective": "To prevent containers from over-utilising system resources, thereby safeguarding against resource exhaustion and DoS attacks."
          }
        ]
      }
        ]
    },
    "(6.2.1) AI Lifecycle Phase requirements - Deployment, Maintenance and Updates": {
        "WebFormTitle": "Define the security requirements for lifecycle phase - Deployment, Maintenance and Updates.",
        "Fields": [
      {
        "FieldType": "risk",
        "FieldName": "[C.3.6] - Mismanaged AI System Deployment and Transition",
        "question": "Is the plan to deploy the AI system a 'big bang' launch without sufficient user training, a pilot phase, or a clear transition plan from the legacy system?",
        "controls": [
          {
            "control": "[C.3.6][BP-4] - Develop and deliver comprehensive training programs for all users and operators on the AI system's capabilities, limitations, and proper operational procedures before deployment.",
            "control_objective": "To minimize misuse, build user trust, and ensure the AI system is operated correctly and effectively."
          },
          {
            "control": "[C.3.6][BP-5] - Implement a phased rollout or pilot program in a controlled environment to identify and resolve unforeseen issues before a full-scale, system-wide launch.",
            "control_objective": "To reduce the risk of widespread disruption and failure by validating system performance in a live but limited setting."
          },
          {
            "control": "[C.3.6][BP-6] - Establish a clear transition plan that includes running the new AI system in parallel with any legacy system to verify outputs and ensure a smooth handover without creating an operational gap.",
            "control_objective": "To guarantee operational continuity and validate the new AI system's performance against established benchmarks before decommissioning the previous system."
          }
        ]
      },
      {
        "FieldType": "risk",
        "FieldName": "[C.3.6] - Inadequate Post-Deployment Maintenance and Monitoring",
        "question": "Once the AI system is live, is there a risk it will be treated with a 'set and forget' mentality, without a dedicated team or process for continuous performance monitoring and regular maintenance?",
        "controls": [
          {
            "control": "[C.3.6][BP-7] - Assign clear ownership for the deployed AI system and implement a continuous monitoring framework with predefined KPIs (e.g., for accuracy, data drift, fairness) and automated alerts to detect performance degradation.",
            "control_objective": "To ensure proactive oversight and rapid detection of issues, preventing the AI system from making flawed decisions over extended periods."
          },
          {
            "control": "[C.3.6][BP-8] - Establish a formal maintenance schedule for the AI system, including periodic model retraining with new data, recalibration, and updates to adapt to changing operational environments.",
            "control_objective": "To prevent model obsolescence and ensure the AI system remains accurate, relevant, and effective throughout its operational life."
          },
          {
            "control": "[C.3.6][BP-9] - Implement a formal feedback loop for users to easily report issues, anomalies, or incorrect outputs, and ensure this feedback is systematically reviewed and used to improve the system.",
            "control_objective": "To leverage end-user experience as a critical source for identifying subtle performance issues and driving continuous improvement."
          }
        ]
      },
      {
        "FieldType": "risk",
        "FieldName": "Insufficient Scalability Management",
        "question": "Will this AI be used across multiple teams, products, or regions — and is it ready to scale reliably?",
        "controls": [
          {
            "control": "SC[1] -  Develop and implement a structured scalability management process for AI infrastructure to ensure efficient handling of increasing workloads and data volumes.",
            "control_objective": "To ensure that the AI infrastructure can scale effectively and efficiently, meeting the growing demands of the organisation while maintaining performance, stability, and security."
          },
          {
            "control": "SC[2] -  Deploy comprehensive technical measures to ensure the AI infrastructure can scale seamlessly to accommodate varying workloads and future growth.",
            "control_objective": "To guarantee that the AI infrastructure is equipped with the necessary technical capabilities to support scalability, ensuring uninterrupted performance and resource availability as demands increase."
          }
        ]
      },
      {
        "FieldType": "risk",
        "FieldName": "[C.3.6] - Poor Management of AI System Evolution and Updates",
        "question": "When the AI system is updated (e.g., retrained model, new features), are there formal processes for testing, documenting, and deploying these changes, including a plan to roll back if the update causes problems?",
        "controls": [
          {
            "control": "[C.3.6][BP-10] - Enforce a strict change management process for all AI system updates, requiring thorough testing (e.g., A/B testing, shadow mode), impact analysis, and a documented rollback plan before deployment.",
            "control_objective": "To prevent system degradation caused by faulty updates and ensure that changes improve, rather than harm, system performance and reliability."
          },
          {
            "control": "[C.3.6][BP-11] - Maintain comprehensive and up-to-date documentation for the AI system, including design rationale, model parameters, data sources, and a version history of all changes made.",
            "control_objective": "To ensure knowledge continuity, facilitate troubleshooting, and support consistent and informed management of the AI system over time."
          },
          {
            "control": "[C.3.6][BP-12] - Implement version control for all components of the AI system, including data, code, and models, to ensure traceability and the ability to rapidly revert to a previously stable version in case of an incident.",
            "control_objective": "To provide a safety net against failed updates and enable quick recovery of system functionality."
          }
        ]
      },
      {
        "FieldType": "risk",
        "FieldName": "Inadequate Disaster Recovery and Business Continuity Planning",
        "question": "What would happen to your business process if this AI system suddenly stopped working?",
        "controls": [
          {
            "control": "BU[1] -  Establish a comprehensive disaster recovery and business continuity planning process to ensure preparedness and resilience in the face of disruptions to AI infrastructure.",
            "control_objective": "To develop and implement administrative controls that ensure the organisation is prepared for and can effectively respond to disruptions, thereby maintaining the continuity of AI operations and minimising downtime."
          },
          {
            "control": "BU[2] -  Implement technical measures to support disaster recovery and business continuity, ensuring the resilience and availability of AI infrastructure.",
            "control_objective": "To deploy comprehensive technical controls that enable rapid recovery and continuity of AI operations in the event of a disruption, minimising data loss and operational downtime."
          }
        ]
      },
      {
        "FieldType": "risk",
        "FieldName": "Insufficient Performance Monitoring and Analysis",
        "question": "Will the AI’s outputs need to be continuously monitored to ensure they stay accurate and relevant?",
        "controls": [
          {
            "control": "PER[1] -  Establish a structured performance monitoring and analysis process to ensure continuous oversight and optimization of AI infrastructure performance.",
            "control_objective": "To implement robust administrative controls that define roles, responsibilities, and procedures for performance monitoring and analysis, ensuring that performance issues are identified and addressed proactively."
          },
          {
            "control": "PER[2] -  Implement advanced technical measures to ensure continuous and effective performance monitoring and analysis of AI infrastructure.",
            "control_objective": "To deploy comprehensive technical controls that provide real-time visibility into AI system performance, enabling prompt detection and resolution of performance issues to maintain optimal operation."
          }
        ]
      },
      {
        "FieldType": "risk",
        "FieldName": "Inadequate Security Monitoring and Threat Detection",
        "question": "Do you need visibility into whether this AI system is under attack or being misused?",
        "controls": [
          {
            "control": "STM[1] -  Establish a security monitoring and threat detection process to ensure continuous oversight and protection of AI infrastructure.",
            "control_objective": "To implement robust administrative controls that define roles, responsibilities, policies, and procedures for effective security monitoring and threat detection, ensuring proactive identification and mitigation of security threats."
          },
          {
            "control": "STM[2] -  Implement advanced technical measures to ensure continuous and effective security monitoring and threat detection within AI infrastructure.",
            "control_objective": "To deploy comprehensive technical controls that provide real-time visibility into security threats and enable prompt detection and response to ensure the protection and integrity of AI systems."
          }
        ]
      },
      {
        "FieldType": "risk",
        "FieldName": "Ineffective Incident Management",
        "question": "Do you have a process in place if the AI system causes an issue (e.g., incorrect output, outage, or data leak)?",
        "controls": [
          {
            "control": "IM[1] -  Establish an incident management capability to ensure effective detection, response, and mitigation of AI-related incidents.",
            "control_objective": "To establish incident management processes that define roles, responsibilities, and procedures for managing AI-related incidents, ensuring timely and effective response to maintain AI system integrity and security."
          },
          {
            "control": "IM[2] -  Implement advanced technical measures to detect, prevent, and mitigate AI system hallucinations and other incidents, ensuring the reliability and accuracy of AI outputs.",
            "control_objective": "To deploy comprehensive technical controls that provide real-time visibility into AI system performance and security, enabling prompt detection and resolution of incidents to maintain optimal operation."
          }
        ]
      }
        ]
    },
    "(6.2.1) AI Lifecycle Phase requirements - Use and Decommissioning": {
        "WebFormTitle": "Define the security requirements for - Use and Decommissioning.",
        "Fields": [
      {
        "FieldType": "risk",
        "FieldName": "Insufficient Audit Trails and Logging",
        "question": "Will you need to track who accessed the AI system, what it did, and when — for accountability or regulatory purposes?",
        "controls": [
          {
            "control": "AT[1] -  Record detailed logs of all data pre-processing activities, including data cleaning, normalisation, transformation, and any outlier removal or imputation techniques applied. Include timestamps, user IDs, and descriptions of actions taken.",
            "control_objective": "To enable traceability and reconstruction of the entire data pre-processing workflow, providing a clear audit trail for identifying any issues or anomalies that may arise during this phase."
          },
          {
            "control": "AT[2] -  Store logs in a secure and tamper-proof manner, ensuring their integrity and availability for future audits or investigations.",
            "control_objective": "To ensure the availability of audit logs for a sufficient period to allow for thorough analysis and investigation of potential security or compliance issues. Protect logs from unauthorised modification or deletion to maintain their evidentiary value."
          }
        ]
      },
      {
        "FieldType": "risk",
        "FieldName": "Insecure Data Sharing and Transfer",
        "question": "Will this AI system exchange data across teams, systems, or external vendors that may not have the same security controls?",
        "controls": [
          {
            "control": "DT[1] -  Implement secure data transfer protocols and encryption mechanisms for proprietary datasets during collection, storage, and fine-tuning processes.",
            "control_objective": "To protect the confidentiality and integrity of sensitive data throughout its lifecycle, preventing unauthorised access or tampering during data sharing and transfer activities. [PR.DS-02]"
          },
          {
            "control": "DT[2] -  Establish data sharing agreements and protocols with internal and external stakeholders involved in the data collection and fine-tuning processes.",
            "control_objective": "To define clear guidelines, roles, and responsibilities for secure data sharing, ensuring compliance with data protection regulations and ethical standards. [GV.SC-02, GV.SC-05]"
          }
        ]
      },
      {
        "FieldType": "risk",
        "FieldName": "Inadequate User Training and Awareness",
        "question": "Will non-technical users rely on this AI — and have they been trained on its limitations and correct usage?",
        "controls": [
          {
            "control": "[Art-9][Par-5][5c] -  Provide comprehensive information and training to AI system users within the organisation, considering their technical knowledge, experience, and education.",
            "control_objective": "To ensure safe and responsible use of the adopted AI system by equipping users with necessary knowledge and skills."
          }
        ]
      },
      {
        "FieldType": "risk",
        "FieldName": "LLM01 Prompt Injection",
        "question": "Will external users or other systems provide text prompts or instructions to the AI model?",
        "controls": [
          {
            "control": "[LLM01][1] - All user inputs within the UI must be validated to prevent the injection of malicious code.",
            "control_objective": "Prevent attackers from exploiting vulnerabilities in the UI to inject malicious code and compromise the AI system."
          },
          {
            "control": "[LLM01][2] - Implement input sanitization techniques to remove harmful characters from user inputs.",
            "control_objective": "Further mitigate the risk of malicious code injection attempts through the UI."
          },
          {
            "control": "[LLM01][4] - Encrypt all sensitive data transmitted through APIs.",
            "control_objective": "Protect sensitive data from unauthorised interception or tampering during communication through APIs."
          },
          {
            "control": "[LLM01][5] - Enforce privilege control on LLM access to backend systems.",
            "control_objective": "To restrict the LLM to the minimum level of access necessary, mitigating the risk of prompt injection leading to unauthorised operations."
          },
          {
            "control": "[LLM01][8] - Treat the LLM as an untrusted component, ensuring user oversight over decision-making processes.",
            "control_objective": "To treat the LLM as an untrusted component, ensuring user oversight over decision-making processes."
          }
        ]
      },
      {
        "FieldType": "risk",
        "FieldName": "LLM02 Insecure Output Handling",
        "question": "Will the AI output be used in other systems, documents, or communications without being verified first?",
        "controls": [
          {
            "control": "[LLM02][1] - Ensure that sensitive or IP data is not exposed in AI system outputs.",
            "control_objective": "To mitigate the risk of insecure output handling by treating LLM-generated outputs as potentially untrusted."
          }
        ]
      },
      {
        "FieldType": "risk",
        "FieldName": "LLM04 Model Denial of Service",
        "question": "Could this AI system be overwhelmed or misused in a way that impacts availability?",
        "controls": [
          {
            "control": "[LLM04][3] - Enforce API rate limits to restrict the number of requests an individual user or IP address can make within a specific timeframe.",
            "control_objective": "To control the rate of requests and prevent overwhelming the LLM with a high volume of concurrent requests."
          },
          {
            "control": "[LLM04][4] - Limit the number of queued actions and the number of total actions in a system reacting to LLM responses.",
            "control_objective": "To prevent the accumulation of excessive workload and ensure that the system can effectively process LLM responses without becoming overwhelmed."
          },
          {
            "control": "[LLM04][5] - Continuously monitor the resource utilisation of the LLM to identify abnormal spikes or patterns that may indicate a DoS attack.",
            "control_objective": "To detect and respond to anomalous resource usage patterns indicative of a denial of service attack on the LLM."
          }
        ]
      },
      {
        "FieldType": "risk",
        "FieldName": "LLM05: Supply Chain Vulnerabilities",
        "question": "Does the AI system depend on third-party libraries, tools, or models you haven’t vetted?",
        "controls": [
          {
            "control": "[LLM05][2] - Only use reputable plugins that have been tested for application requirements.",
            "control_objective": "Minimise plugin-related vulnerabilities."
          },
          {
            "control": "[LLM05][4] - Maintain an up-to-date inventory using a Software Bill of Materials (SBOM).",
            "control_objective": "Track and manage components."
          },
          {
            "control": "[LLM05][6] - Implement anomaly detection and adversarial robustness tests.",
            "control_objective": "Detect tampering and poisoning."
          },
          {
            "control": "[LLM05][7] - Implement sufficient monitoring and a robust patching policy.",
            "control_objective": "Maintain system security and component currency."
          },
          {
            "control": "[LLM05][8] - Regularly review and audit supplier security and access controls.",
            "control_objective": "Verify supplier compliance and security."
          }
        ]
      },
      {
        "FieldType": "risk",
        "FieldName": "LLM08 Excessive Agency",
        "question": "Will the AI be allowed to take actions or make decisions automatically on behalf of a person or system?",
        "controls": [
          {
            "control": "[LLM08][4] - Limit the permissions that LLM plugins/tools are granted to other systems to the minimum necessary.",
            "control_objective": "To restrict the scope of actions that LLM agents can perform on other systems, thereby minimising the potential for harmful activities."
          },
          {
            "control": "[LLM08][5] - Track user authorization and security scope to ensure actions taken on behalf of a user are executed with the minimum privileges necessary.",
            "control_objective": "To enforce proper user authentication and authorization, limiting the potential impact of actions performed by LLM agents."
          },
          {
            "control": "[LLM08][8] - Log and monitor the activity of LLM plugins/tools and downstream systems to identify and respond to undesirable actions.",
            "control_objective": "To detect and mitigate unauthorised or harmful activities by monitoring system activity and responding promptly to incidents."
          }
        ]
      },
      {
        "FieldType": "risk",
        "FieldName": "LLM10: Model Theft",
        "question": "Is this AI system considered a valuable business asset — and could losing it expose intellectual property or trade secrets?",
        "controls": [
          {
            "control": "[LLM10][4] - Restrict the LLMs access to network resources, internal services, and APIs.",
            "control_objective": "Control what the LLM application has access to."
          },
          {
            "control": "[LLM10][6] - Automate MLOps deployment with governance and tracking and approval workflows.",
            "control_objective": "Tighten access and deployment controls."
          }
        ]
      },
      {
        "FieldType": "risk",
        "FieldName": "[C.3.6] - Unsafe Decommissioning of AI Systems",
        "question": "When this AI system is eventually retired, is there a formal plan to handle its data and models securely and to manage the transition to a replacement system without creating operational gaps?",
        "controls": [
          {
            "control": "[C.3.6][BP-13] - Establish a formal decommissioning plan that specifies procedures for the secure archiving or certified destruction of the AI model and all associated data, in compliance with data retention policies and privacy regulations.",
            "control_objective": "To prevent data breaches and the misuse of sensitive information or obsolete models after the system is retired."
          },
          {
            "control": "[C.3.6][BP-14] - Ensure a managed transition when replacing an AI system by maintaining operational continuity, potentially through a period of overlap with the new system, to prevent any gaps in critical functionality.",
            "control_objective": "To avoid creating vulnerabilities or disruptions to business processes during the transition from an old AI system to its replacement."
          },
          {
            "control": "[C.3.6][BP-15] - Have an incident response plan for AI-related failures, including procedures for safe shutdown, reversion to manual processes, and clear communication to stakeholders.",
            "control_objective": "To minimize harm and operational disruption when an AI incident occurs and ensure a structured recovery process."
          }
        ]
      }   
        ]
    },
    "(A.6.2.4) AI Systems verifications": {
        "WebFormTitle": "Document verification and validation measures for the AI system and specify criteria for their use.",
        "Fields": [
            {
                "Control": "A.6.2.4 – AI system verification and validation",
                "FieldName": "(6.2.4) - Performance and Fairness Monitoring V&V",
                "FieldLabel": "Performance Monitoring Verification",
                "FieldText": "Verify the implementation of monitoring tools and dashboards. Test the system's ability to detect simulated performance degradation (e.g., accuracy drop >5%) and fairness metric violations (e.g., disparate impact ratio <0.8). Confirm that automated alerts are configured and successfully trigger notifications to the designated personnel.",
                "FieldType": "Option box with values:Applicable/Not Applicable"
            },
            {
                "Control": "A.6.2.4 – AI system verification and validation",
                "FieldName": "(6.2.4) - Human Oversight V&V",
                "FieldLabel": "Human Oversight Validation",
                "FieldText": "Review and approve the documented human oversight procedures. Validate that the user interface provides the necessary information and controls for a human to intervene, override, or stop the system's process. Conduct user acceptance testing (UAT) with designated overseers to confirm they can effectively perform their roles.",
                "FieldType": "Option box with values:Applicable/Not Applicable"
            },
            {
                "Control": "A.6.2.4 – AI system verification and validation",
                "FieldName": "(6.2.4) - Adherence to Intended Use V&V",
                "FieldLabel": "Intended Use Adherence Verification",
                "FieldText": "Verify that the 'Intended Use' documentation is clear, complete, and accessible to all users. Test any implemented technical controls (e.g., access restrictions, input validation) designed to prevent use outside the defined scope. Confirm user training materials explicitly cover approved use cases and limitations.",
                "FieldType": "Option box with values:Applicable/Not Applicable"
            },
            {
                "Control": "A.6.2.4 – AI system verification and validation",
                "FieldName": "(6.2.4) - Incident Management V&V",
                "FieldLabel": "Incident Management Validation",
                "FieldText": "Verify that an AI-specific incident management plan exists. Conduct a tabletop exercise or simulation of a critical AI failure (e.g., major bias detection, system outage). Validate that reporting channels, escalation paths, and response roles function as documented and meet defined SLAs.",
                "FieldType": "Option box with values:Applicable/Not Applicable"
            },
            {
                "Control": "A.6.2.4 – AI system verification and validation",
                "FieldName": "(6.2.4) - Operational Transparency V&V",
                "FieldLabel": "Operational Transparency Validation",
                "FieldText": "Validate that all user interfaces clearly disclose that an AI system is in use. Review system-generated notifications and explanations for clarity, accuracy, and completeness. Verify that mechanisms for users to access more detailed information or provide feedback are implemented as specified in the requirements.",
                "FieldType": "Option box with values:Applicable/Not Applicable"
            },
            {
                "Control": "A.6.2.4 – AI system verification and validation",
                "FieldName": "(6.2.4) - Data Management V&V",
                "FieldLabel": "Operational Data Management Verification",
                "FieldText": "Verify that data handling processes align with the Data Privacy Impact Assessment (DPIA). Conduct penetration testing and vulnerability scans on data storage and transmission channels. Validate that data minimization techniques (e.g., anonymization) are correctly implemented and that data retention policies are automatically enforced.",
                "FieldType": "Option box with values:Applicable/Not Applicable"
            },
            {
                "Control": "A.6.2.4 – AI system verification and validation",
                "FieldName": "(6.2.4) - System Maintenance V&V",
                "FieldLabel": "System Maintenance Plan Verification",
                "FieldText": "Verify the existence of a comprehensive system maintenance plan, including schedules for performance reviews and security patching. Validate the documented procedure for model retraining and redeployment. Review and approve the decommissioning plan to ensure it addresses data archival/deletion and stakeholder communication.",
                "FieldType": "Option box with values:Applicable/Not Applicable"
            }
        ]
    },
    
       
    "(A.8.4) AI System Documentation and User Information": {
        "WebFormTitle": "Define and document the information, reporting, and incident communication plan for all internal and external interested parties.",
        "Fields": [
            {
                "FieldName": "(A.8.4) - Incident Communication Plan - Data Breach",
                "FieldLabel": "Data Breach",
                "FieldText": "Describe how incidents related to \"Unintended exposure of training data\" will be comunicated.",
                "FieldType": "TextBox"
            },
            {
                "FieldName": "(A.8.4) - Incident Communication Plan - Model Misuse",
                "FieldLabel": "Model Misuse",
                "FieldText": "Describe how incidents related to \"AI model used outside intended scope\" will be comunicated.",
                "FieldType": "TextBox"
            },
            {
                "FieldName": "(A.8.4) - Incident Communication Plan - Model Failure",
                "FieldLabel": "Model Failure",
                "FieldText": "Describe how incidents related to \"False predictions causing harm\" will be comunicated.",
                "FieldType": "TextBox"
            }
        ]
    },
    
    
    
    "AI Systems approvals": {
        "WebFormTitle": "Document AI system approvals",
        "Fields": [
            {
                "FieldName": "AI System Security Approver",
                "FieldLabel": "Security Approver",
                "FieldText": "Name/Role of the Security Aprover",
                "FieldType": "TextBox"
            },
            {
                "FieldName": "AI System Security Approval",
                "FieldLabel": "Security Approved",
                "FieldText": "",
                "FieldType": "Option box with values:Yes/No"
            },
            {
                "FieldName": "AI System DPO Approver",
                "FieldLabel": "DPO Approver",
                "FieldText": "Name/Role of the DPO Aprover",
                "FieldType": "TextBox"
            },
            {
                "FieldName": "AI System DPO Approval",
                "FieldLabel": "DPO Approved",
                "FieldText": "",
                "FieldType": "Option box with values:Yes/No"
            },
            {
                "FieldName": "AI System Risk Approver",
                "FieldLabel": "Risk Approver",
                "FieldText": "Name/Role of the Risk Aprover",
                "FieldType": "TextBox"
            },
            {
                "FieldName": "AI System Risk Approval",
                "FieldLabel": "Risk Approved",
                "FieldText": "",
                "FieldType": "Option box with values:Yes/No"
            },
            {
                "FieldName": "AI System Business Approver",
                "FieldLabel": "Business Approver",
                "FieldText": "Name/Role of the Business Approver",
                "FieldType": "TextBox"
            },
            {
                "FieldName": "AI System Business Approval",
                "FieldLabel": "Business Approved",
                "FieldText": "",
                "FieldType": "Option box with values:Yes/No"
            }
        ]
    }
}
