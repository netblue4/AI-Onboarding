{
  "Phase AI Act Compliance": [
    {
      "StepName": "EU AI Act: Prohibited AI Practices Assessment",
      "Objectives": [
        {
          "Objective": "To determine if the AI system's intended purpose constitutes a prohibited practice under the EU AI Act."
        }
      ],
      "Fields": [
        {
          "FieldName": "EU-AI-ACT-PROHIBITED-ASSESSMENT",
          "FieldLabel": "Will the AI system be used for any of the following prohibited purposes?",
          "FieldText": "The EU AI Act strictly prohibits certain AI practices that pose an unacceptable risk. If any of the following options are selected, the AI system is considered prohibited and cannot be deployed.",
          "FieldType": "MultiSelect:Manipulating human behavior to cause physical or psychological harm/Exploiting vulnerabilities of specific groups (e.g., age, disability) to cause harm/General-purpose social scoring by public authorities/Real-time remote biometric identification in public spaces for law enforcement (outside of strictly defined exceptions)"
        }
      ]
    },
    {
      "StepName": "EU AI Act: Role Classification (Provider vs. Deployer)",
      "Objectives": [
        {
          "Objective": "To clarify the organization's role and corresponding legal obligations under the EU AI Act based on its development and deployment activities."
        }
      ],
      "Fields": [
        {
          "FieldName": "AI-ACT-ROLE-CLASSIFICATION",
          "FieldLabel": "Which description best defines your organization's role and activities for this AI system?",
          "FieldText": "It's very important to clearly define the organisation's activities because it will impact the AI Act’s distinction between 'Provider' (developer) and 'Deployer' (user), which comes with significantly different responsibilities. The organisation's activities are exclusively focused on operationalizing, integrating, and governing generic pre-trained LLMs and developing internal infrastructure for Retrieval-Augmented Generation (RAG), without any modification, fine-tuning, or retraining of the underlying model itself. The AI system is for internal organizational use only, and is not repackaged or distributed to external customers. The LLM is chosen as a generic, pre-trained model, stored on-premises, and never fine-tuned, retrained, Its parameters, weights, or architecture layers are not modified by the organisation's internal engineering team. Meaning it does not interact with or access any external internet datasets, ensuring data sovereignty and minimizing exposure to third-party risks. The organisation's internal engineering team’s efforts are strictly limited to building infrastructure, orchestration, and internal data pipelines for the LLM, but do not alter the core LLM architecture or its parameters.",
          "FieldType": "MultiSelect:[Deployer - Internal Build] We are a Deployer. Our activities match the description: we use a generic model for internal use only AND our development is limited to building orchestration (RAG) without modifying the core model./[Provider] We are a Provider. We are substantially modifying the core AI model (e.g., fine-tuning, retraining) OR we are distributing this system to external customers."
        }
      ]
    },
    {
      "StepName": "EU AI Act: High-Risk System Classification",
      "Objectives": [
        {
          "Objective": "To classify the AI system's risk level by assessing its intended purpose against the high-risk categories defined in the EU AI Act."
        }
      ],
      "Fields": [
        {
          "FieldName": "EU-AI-ACT-HIGH-RISK-ASSESSMENT",
          "FieldLabel": "Will the AI system be used for any of the following purposes?",
          "FieldText": "Under the EU AI Act, a system is classified as high-risk if its intended use falls into specific categories. Please select all that apply. If any option is selected, the AI system will be classified as high-risk.",
          "FieldType": "MultiSelect:As a safety component in a regulated product (e.g., medical devices, cars, toys)/Biometric identification or categorisation of people/Management of critical infrastructure (e.g., water, gas, electricity)/Determining access to education or scoring exams/Recruitment, promotion, or employee performance management/Assessing creditworthiness or eligibility for public benefits/Law enforcement purposes (e.g., risk assessment, evidence evaluation)/Migration, asylum, and border control management/Assisting judicial authorities in legal proceedings"
        },
        {
          "FieldName": "EU-AI-ACT-LIMITED-RISK-ASSESSMENT",
          "FieldLabel": "Does the AI system have specific transparency obligations (Limited Risk)?",
          "FieldText": "If the system is not high-risk, it may still be 'Limited Risk' and have specific transparency obligations to ensure users are not deceived. Please select all that apply.",
          "FieldType": "MultiSelect:Interacts directly with humans (e.g., a chatbot) and must disclose it is an AI/Generates 'deep fakes' or manipulates video, audio, image content/Used for emotion recognition or biometric categorization/Generates synthetic text published on matters of public interest"
        }
      ]
    },
    {
      "StepName": "AI Act - Section 2: Provider & Deployer Requirements for High-Risk AI Systems",
      "Objectives": [
        {
          "Objective": "If the AI system is categorised as high-risk according to the AI Acty, define the security requirements for - High-Risk AI Systems."
        }
      ],
      "Fields": [
        {
          "FieldType": "risk",
          "FieldName": "[Article 9] - Inadequate or Ineffective Risk Management",
          "question": "Does the organization have a documented and continuously updated risk management system for its high-risk AI systems that covers the entire lifecycle of the system?",
          "controls": [
            {
              "control": "[Art-9][Par-1][1] - Establish, implement, document, and maintain a comprehensive risk management system for high-risk AI systems.",
              "control_objective": "To ensure a systematic and ongoing process for identifying, evaluating, and mitigating risks associated with high-risk AI systems throughout their lifecycle.",
              "control_status": "Met",
              "control_evidence": "Evidence is the AI onboarding procedure itself. This procedure combines the 'A.9.4 functional specification webform' (to define the system's boundaries) with the 'Risk' node (to document the risk management process), demonstrating a systematic approach."
            },
            {
              "control": "[Art-9][Par-2][2] - Implement a continuous iterative risk management process that includes regular systematic reviews and updates, covering identification, analysis, estimation, and evaluation of foreseeable risks.",
              "control_objective": "To proactively manage and adapt to evolving risks by maintaining a dynamic and up-to-date risk management framework.",
              "control_status": "Met",
              "control_evidence": "Evidence is the completed 'Risk' node for each RAG component. This node serves as a living document that captures the full analysis, from 'FieldName' (risk identification) to 'controls' (evaluation/treatment), ensuring a traceable and iterative process."
            },
            {
              "control": "[Art-9][Par-2][3] - Adopt appropriate and targeted risk management measures to address identified risks, including those from post-market monitoring.",
              "control_objective": "To effectively mitigate identified risks through the implementation of specific and relevant control measures.",
              "control_status": "Met",
              "control_evidence": "Evidence is documented within the 'Risk' node. The 'controls' and 'control_objectives' fields for each identified threat (e.g., 'data poisoning') explain the specific measures adopted to treat that targeted risk."
            },
            {
              "control": "[Art-9][Par-5][4] - Ensure that residual risks, both individual and overall, are acceptable by eliminating or reducing risks as far as technically feasible through design, implementing mitigation measures, and providing information and training.",
              "control_objective": "To reduce the potential for harm to an acceptable level by employing a multi-layered approach to risk mitigation.",
              "control_status": "Met",
              "control_evidence": "Evidence is the formal sign-off of the AI onboarding documentation. This act confirms that the controls documented in the 'Risk' node are sufficient and that any residual risk is deemed acceptable for the system's 'Intended Use' as defined in the A.9.4 webform."
            },
            {
              "control": "[Art-9][Par-6][5] - Conduct testing of high-risk AI systems to identify the most appropriate risk management measures and to ensure consistent performance and compliance with requirements.",
              "control_objective": "To validate the effectiveness of risk management measures and ensure the AI system operates as intended.",
              "control_status": "Met",
              "control_evidence": "Evidence is the output of the '(A.6.2.4) AI Systems verifications' step. This step documents the test plans (e.g., performance, fairness, data quality) that validate the effectiveness of our risk management measures. The 'PlanCriteria' within this step defines the specific, measurable success conditions used to ensure the system operates as intended and that the controls are verifiably effective."
            },
            {
              "control": "[Art-9][Par-9][6] - Consider the potential adverse impact on persons under the age of 18 and other vulnerable groups when implementing the risk management system.",
              "control_objective": "To provide heightened protection for vulnerable populations who may be disproportionately affected by the AI system.",
              "control_status": "Met",
              "control_evidence": "Evidence is the completed '(A.5.2, A.5.3, A.5.4) Vulnerable Populations Impact Assessment' step. This step provides the documented assessment required by the control, forcing a formal review of potential negative impacts on at-risk groups and the creation of specific mitigation and monitoring strategies to ensure their protection."
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "[Article 10] - Poor Data Quality and Governance",
          "question": "Are there appropriate data governance and management practices in place for the training, validation, and testing datasets used by the high-risk AI system to ensure they are relevant, representative, and free of errors and biases?",
          "controls": [
            {
              "control": "[Art-10][Par-2][1] - Implement and document data governance and management practices covering the entire data lifecycle, including design choices, collection, and preparation processes like annotation and cleaning.",
              "control_objective": "To ensure that data used for high-risk AI systems is handled systematically and responsibly, maintaining quality and integrity from collection to use.",
              "control_status": "Met",
              "control_evidence": "Evidence is the combination of the 'Internal Data Sources,' 'Data Processing Pipeline,' and '(A.6.2.4) AI Systems verifications' steps. Together, these procedures document our entire data management strategy (A.7.2), covering data acquisition, preparation (A.7.6), and quality, thus ensuring systematic handling."
            },
            {
              "control": "[Art-10][Par-2][2] - Establish a process to examine data sets for possible biases that could negatively impact fundamental rights, health, or safety, and implement measures to detect, prevent, and mitigate these biases.",
              "control_objective": "To minimize the risk of discriminatory or unfair outcomes and ensure the AI system operates in a manner that is safe and respects fundamental rights.",
              "control_status": "Met",
              "control_evidence": "Evidence is the 'Fairness and Bias' test plan within the '(A.6.2.4) AI Systems verifications' step. This plan documents the formal process for examining bias by analyzing dataset representativeness against real-world populations (A.7.3) and setting measurable fairness criteria (A.7.4) to mitigate discriminatory outcomes."
            },
            {
              "control": "[Art-10][Par-3][3] - Ensure that training, validation, and testing data sets are relevant, sufficiently representative, free of errors, and complete for the system's intended purpose, with appropriate statistical properties.",
              "control_objective": "To build a robust and reliable AI system by using high-quality data that accurately reflects the operational environment and minimizes performance issues.",
              "control_status": "Met",
              "control_evidence": "Evidence is the '(A.6.2.4) AI Systems verifications' step. Its 'Data quality requirements' plan sets explicit, measurable criteria for accuracy and completeness (A.7.4), while the 'Fairness and Bias' plan sets the criteria for representativeness, ensuring all data is fit for purpose."
            },
            {
              "control": "[Art-10][Par-4][4] - Verify that data sets account for the specific geographical, contextual, behavioral, or functional settings in which the high-risk AI system will be used.",
              "control_objective": "To ensure the AI system performs effectively and as intended in its specific operational context, reducing the risk of failures due to environmental mismatches.",
              "control_status": "Met",
              "control_evidence": "Evidence is the combination of the 'Internal Data Sources' step and the 'Fairness and Bias' test plan. The 'Internal Data Sources' documentation defines the origin and context of our data, and the test plan verifies its representativeness against the 'real-world populations' of that specific operational setting."
            },
            {
              "control": "[Art-10][Par-5][5] - Where strictly necessary for bias detection and correction, process special categories of personal data only with appropriate safeguards, technical limitations, and security measures, ensuring data is deleted after use.",
              "control_objective": "To enable effective bias mitigation while upholding the highest standards of data protection and privacy for sensitive personal information.",
              "control_status": "Met",
              "control_evidence": "Evidence is the 'Data quality requirements' test plan. This plan establishes safeguards by defining specific data provenance criteria (DATA-SEN-05) and verification steps (BBT-EXT-PRO-01, BBT-EXT-PRO-02), which create a secure, auditable process for handling and tracing sensitive data used for bias correction."
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "[Article 12] - Inadequate Traceability and Record-Keeping",
          "question": "Does the organization need to trace the operational history of its high-risk AI system to investigate incidents, audit results, or ensure accountability?",
          "controls": [
            {
              "control": "[Art-12][Par-1][1] - Implement capabilities for the automatic recording of events (logs) while the high-risk AI system is operating.",
              "control_objective": "To ensure a level of traceability of the AI system’s functioning throughout its lifecycle that is appropriate to its intended purpose.",
              "control_status": "Met",
              "control_evidence": "Compliance is met by ensuring the **RAG Orchestrator's** logging service initializes automatically on startup. This captures a complete operational trail, including the 'Request', 'Retrieve', 'Generate', and 'Response' steps, and streams these events to a SIEM. This provides the necessary end-to-end traceability of the system's function during operation, meeting the objective."
            },
            {
              "control": "[Art-12][Par-2][2] - Implement capabilities ensuring logging capabilities record the period of each use, the reference database, the input data, and the identity of the persons involved in verifying the results.",
              "control_objective": "To provide detailed operational transparency and accountability for AI systems used in critical public and justice-related applications.",
              "control_status": "Met",
              "control_evidence": "Accountability is provided via a defined log schema for the **RAG Orchestrator** that explicitly includes fields for 'period_of_use', 'reference_database' (e.g., 'Permanent Index'), 'input_data' (the 'PDF + QUESTION'), and 'identity_of_person_verifying'. Verifying a log query for a single user session confirms that these specific data points are captured, providing the detailed operational transparency required by the objective."
            },
            {
              "control": "[Art-12][Par-4][3] - Implement capabilities ensuring  logging capabilities record the period of use, reference database, input data, and the identification of the person generating the match.",
              "control_objective": "To enhance auditability and accountability in the use of sensitive remote biometric identification technologies.",
              "control_status": "Met",
              "control_evidence": "Accountability is met by demonstrating that a complete \"Chat With Your Document\" session log, generated by the **RAG Orchestrator**, contains all the required elements: 'period_of_use', 'reference_database', 'input_data', and the 'identification_of_person' (the authenticated user ID). This log export provides a clear, auditable record linking a specific user to a specific query and its underlying data sources, fulfilling the objective."
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "[Article 13] - Lack of Transparency and Provision of Information to Users",
          "question": "Will users of this AI system need to understand its capabilities, limitations, and the meaning of its outputs to use it safely and effectively?",
          "controls": [
            {
              "control": "[Art-13][Par-1] - Ensure the design of high-risk AI systems allows users to interpret outputs and use the system appropriately.",
              "control_objective": "To enable safe and effective use of the AI system by ensuring user comprehension.",
              "control_evidence": ""
            },
            {
              "control": "[Art-13][Par-2] - Provide clear, complete, and accessible instructions for use with all high-risk AI systems.",
              "control_objective": "To ensure users have the necessary information to operate the AI system correctly and safely.",
              "control_evidence": ""
            },
            {
              "control": "[Art-13][Par-3a] - Include the identity and contact details of the provider and their authorized representative in the instructions for use.",
              "control_objective": "To establish clear lines of communication and accountability for the AI system.",
              "control_evidence": ""
            },
            {
              "control": "[Art-13][Par-3b] - Detail the AI system's characteristics, capabilities, limitations, intended purpose, accuracy, robustness, cybersecurity, and performance metrics in the instructions for use.",
              "control_objective": "To provide a comprehensive understanding of the AI system's operational parameters and performance expectations.",
              "control_evidence": ""
            },
            {
              "control": "[Art-13][Par-3c-e-f] - Specify the necessary hardware resources, expected lifetime, maintenance, and pre-determined changes for operating the AI system in the instructions for use.",
              "control_objective": "To ensure users have the required infrastructure and information to run and maintain the AI system effectively over its lifecycle.",
              "control_evidence": ""
            },
            {
              "control": "[Art-13][Par-3d] - Detail the human oversight measures from Article 14, including technical aids for interpreting system outputs, in the instructions for use.",
              "control_objective": "To facilitate effective human oversight and intervention in the AI system's operation.",
              "control_evidence": ""
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "[Article 14] - Ineffective or Insufficient Human Oversight",
          "question": "Does the AI system operate in a way that requires human monitoring, intervention, or decision-making to prevent or mitigate risks to health, safety, or fundamental rights?",
          "controls": [
            {
              "control": "[Art-14][Par-1] - Design and develop high-risk AI systems with appropriate human-machine interface tools to enable effective oversight by natural persons.",
              "control_objective": "To ensure that a human can effectively monitor and control the AI system while it is in use.",
              "control_status": "Met",
              "control_evidence": "The `AI system response - Decision-Making Language Requiring Intervention` list defines the specific criteria (e.g., 'approve the loan', 'the diagnosis is') that trigger the human oversight mechanism. This list is the core configuration for the human-machine interface, as it identifies exactly which system outputs must be flagged for an overseer, enabling them to monitor and control the system's high-risk actions as described in control `[RAG-DM-01]`."
            },
            {
              "control": "[Art-14][Par-2] - Implement human oversight to prevent or minimize risks to health, safety, or fundamental rights, especially those risks that persist after other requirements have been applied.",
              "control_objective": "To provide a final layer of risk mitigation through active human involvement.",
              "control_status": "Met",
              "control_evidence": "This is met in two ways: 1) The `AI system intended use - Prohibited Uses Blacklist Definition` proactively prevents high-risk uses that threaten fundamental rights (e.g., 'social scoring', 'biometric categorization'). 2) For risks that may emerge during permitted use, the `AI system response - Decision-Making Language Requiring Intervention` list identifies outputs with potential risks to health (e.g., 'prescribe treatment') or rights (e.g., 'deny benefits'), triggering human oversight via control `[RAG-DM-01]` as a final mitigation layer."
            },
            {
              "control": "[Art-14][Par-3] - Ensure human oversight measures are built into the AI system by the provider or are appropriate for implementation by the deployer.",
              "control_objective": "To integrate necessary oversight capabilities either directly into the system or into the operational procedures of the user.",
              "control_status": "Met",
              "control_evidence": "The provided content defines measures that are built directly into the RAG Orchestrator. Control `[RAG-DM-01]` describes a 'monitoring module' that inspects LLM output, and the `AI system response - Decision-Making Language Requiring Intervention` list serves as its direct configuration file. This demonstrates an oversight capability integrated into the system's core design, not one left to chance or deployer procedures."
            },
            {
              "control": "[Art-14][Par-4a, 4b, 4c] - Enable assigned human overseers to understand the AI system's capabilities and limitations, monitor for anomalies, and correctly interpret its output, while remaining aware of potential automation bias.",
              "control_objective": "To empower human overseers with the knowledge and awareness needed to make informed judgments about the system's performance.",
              "control_status": "Met",
              "control_evidence": "The `AI system intended use - Permitted User Prompt Categories Whitelist Definition` and `Prohibited Uses Blacklist Definition` clearly document the system's intended capabilities and limitations for an overseer. More specifically, the `AI system response - Decision-Making Language Requiring Intervention` list acts as a concrete guide, training overseers to monitor for and correctly interpret anomalous, high-risk outputs (e.g., 'this is a medical recommendation'), thereby empowering them to make informed judgments and remain aware of automation bias."
            },
            {
              "control": "[Art-14][Par-4d, 4e] - Enable assigned human overseers to have the ability to decide not to use the system, override its output, or interrupt its operation via a 'stop' button or similar procedure.",
              "control_objective": "To ensure ultimate human control over the AI system's actions and decisions in any given situation.",
              "control_status": "Met",
              "control_evidence": "Control `[RAG-DM-01]` directly mandates this. It requires a mechanism for 'immediate human intervention' that explicitly allows the 'overseer to stop or override the system.' The `AI system response - Decision-Making Language Requiring Intervention` list defines the precise triggers (e.g., 'authorize payment', 'initiate safety protocol') that activate this 'stop' or 'override' capability, ensuring ultimate human control over high-risk decisions."
            },
            {
              "control": "[Art-14][Par-5] - Ensure that any identification is verified and confirmed by at least two competent, trained, and authorized natural persons before action is taken.",
              "control_objective": "To increase the reliability and accountability of critical identification tasks performed by AI.",
              "control_evidence": ""
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "[Article 15] - Inadequate Accuracy, Robustness, and Cybersecurity",
          "question": "Will poor AI performance (e.g., incorrect or unreliable outputs) cause business disruption, compliance issues, or customer dissatisfaction?",
          "controls": [
            {
              "control": "[Art-15][Par-1] - Ensure high-risk AI systems are designed and developed to achieve and maintain an appropriate level of accuracy, robustness, and cybersecurity throughout their lifecycle.",
              "control_objective": "To maintain the system's trustworthiness and prevent harm from inaccurate or insecure operation.",
              "control_status": "Met",
              "control_evidence": "The `(A.6.2.4) AI Systems verifications` step provides the formal framework to ensure these levels are met. It defines specific test plans for performance (accuracy) and fairness (a component of robustness). Each plan includes `PlanCriteria` that set measurable success conditions and `criteria_evidence` requirements, directly verifying that the system achieves and maintains the appropriate levels of accuracy and trustworthiness."
            },
            {
              "control": "[Art-15][Par-2] - Encourage the development of benchmarks and measurement methodologies for accuracy and robustness in cooperation with relevant stakeholders.",
              "control_objective": "To establish standardized methods for evaluating and verifying the performance of AI systems.",
              "control_status": "Met",
              "control_evidence": "The `(A.6.2.4) AI Systems verifications` step establishes this standardized method. It formally documents the validation process through distinct test plans (e.g., for performance, fairness) which consist of specific `PlanSteps` (the verification measures) and `PlanCriteria` (the success conditions). This structure creates a defined, repeatable, and documented methodology for evaluating and verifying the system's performance against objective standards."
            },
            {
              "control": "[Art-15][Par-3] - Clearly state the levels of accuracy and the relevant accuracy metrics in the AI system's instructions for use.",
              "control_objective": "To provide transparency to users about the system's expected performance.",
              "control_status": "Met",
              "control_evidence": "The (A.7.4, A.7.6) - Data quality requirements & Test Plan directly supports this control by defining a specific, measurable accuracy metric and the methodology for its validation. The criterion `DATA-SEN-01` establishes the quantitative accuracy target of '> 95% for sensitive data'. The test steps `BBT-EXT-ACC-01` and `BBT-EXT-ACC-02` provide the procedure for creating a 'golden dataset' and programmatically verifying this accuracy level. This ensures that the accuracy figures stated in the user instructions are based on a robust and repeatable testing process, providing a transparent and evidence-backed representation of the system's performance."
            },
            {
              "control": "[Art-15][Par-4] - Design AI systems to be resilient to errors, faults, or inconsistencies, using technical redundancies and fail-safe plans where appropriate, and mitigate risks from biased feedback loops in learning systems.",
              "control_objective": "To ensure the system can handle unexpected situations and maintain stable performance without being negatively influenced by its own outputs.",
              "control_status": "Met",
              "control_evidence": "The `(A.6.2.4) AI Systems verifications` step provides the evidence for this objective. It includes test plans for 'performance' and 'fairness,' which are used to validate the system's resilience and check for bias. By defining measurable `PlanCriteria` for these plans, we verify that the system can handle errors and that measures to mitigate bias are effective, ensuring stable and fair performance."
            },
            {
              "control": "[Art-15][Par-5] - Implement cybersecurity measures to protect high-risk AI systems from unauthorized alteration of their use, outputs, or performance by exploiting vulnerabilities.",
              "control_objective": "To safeguard the AI system against malicious attacks such as data poisoning, model poisoning, and adversarial examples.",
              "control_status": "Met",
              "control_evidence": "Compliance is evidenced by the 'Risk' node documented for each RAG component. This node explicitly defines the security requirements by identifying specific threats, such as 'malicious data ingestion or data poisoning'. It then documents the 'associated controls' and 'control objectives' designed to mitigate these exact vulnerabilities, providing a traceable record of the measures specified to protect the system from such attacks."
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "[Article 27] - Failure to Assess Fundamental Rights Impact",
          "question": "Does the organization (as a deployer of a high-risk system in a public or sensitive sector) need to conduct and document a fundamental rights impact assessment (FRIA) before deploying the system?",
          "controls": [
            {
              "control": "[Art-27][Par-1] - Perform a Fundamental Rights Impact Assessment (FRIA) prior to deploying a high-risk AI system (for public bodies, providers of public services, and systems for credit scoring or insurance).",
              "control_objective": "To proactively identify and analyze the potential impact and specific risks the AI system's use may pose to the fundamental rights of affected persons before any harm can occur.",
              "control_status": "Met",
              "control_evidence": "This objective is met by implementing the 'Vulnerable Populations Impact Assessment' step. This step establishes the formal, proactive process required by the objective, providing a 'structured framework' to identify and analyze both 'positive and negative impacts' on 'at-risk groups' before deployment."
            },
            {
              "control": "[Art-27][Par-1a, 1b] - The FRIA must include a description of the processes, the intended purpose, the period of time, and the frequency of the system's use.",
              "control_objective": "To clearly document the context, scope, and intensity of the AI system's deployment to understand its potential sphere of influence on fundamental rights.",
              "control_evidence": ""
            },
            {
              "control": "[Art-27][Par-1c, 1d] - The FRIA must include the categories of persons/groups likely to be affected and the specific risks of harm likely to impact them, using information from the provider.",
              "control_objective": "To move from a general assessment to a specific analysis of *who* could be harmed and *how* they could be harmed, ensuring vulnerable groups are considered.",
              "control_status": "Met",
              "control_evidence": "This is directly met. The 'Vulnerable Populations Impact Assessment' step provides a 'structured method to identify at-risk populations' (who) and 'describe and rate the severity of potential positive or negative impacts' (how). This is complemented by the 'Workforce Transition and Adaptation' step, which specifically identifies the 'job roles affected,' fulfilling the objective's need for a specific analysis of *who* is impacted."
            },
            {
              "control": "[Art-27][Par-1e] - The FRIA must include a description of how human oversight measures will be implemented according to the instructions for use.",
              "control_objective": "To ensure that the planned human oversight is not just theoretical, but is described and integrated into the deployment plan as a key risk mitigation measure.",
              "control_evidence": ""
            },
            {
              "control": "[Art-27][Par-1f] - The FRIA must include the measures to be taken if risks materialize, including internal governance and complaint mechanisms.",
              "control_objective": "To create an actionable plan for risk mitigation and redress, ensuring that if a risk does materialize, there is a pre-defined process to manage it and provide recourse for affected individuals.",
              "control_status": "Met",
              "control_evidence": "This objective is met through the 'Vulnerable Populations Impact Assessment' step, which requires the organization to 'define clear mitigation strategies to prevent harm.' The 'Workforce Transition and Adaptation' step reinforces this by requiring documentation of 'corresponding mitigation strategies, including re-skilling and re-deployment plans.' Together, these create the actionable plan required to manage materialized risks."
            },
            {
              "control": "[Art-27][Par-2] - Conduct the FRIA for the first use of the system and update it if any of the key elements change during its use.",
              "control_objective": "To ensure the FRIA is a relevant, living document that accurately reflects the system's current use, rather than a one-time, static assessment that becomes outdated.",
              "control_status": "Met",
              "control_evidence": "This is met by establishing formal monitoring plans within the assessments. The 'Vulnerable Populations Impact Assessment' step 'establish[es] a post-deployment monitoring plan to ensure ongoing oversight.' Similarly, the 'Environmental Sustainability' step outlines a 'plan for ongoing monitoring and review,' ensuring the assessment remains relevant and is not just a static, one-time activity."
            },
            {
              "control": "[Art-27][Par-3] - Notify the market surveillance authority of the FRIA results by submitting the completed template (unless exempt).",
              "control_objective": "To provide regulatory transparency and enable authorities to oversee the deployment of high-risk systems and the deployer's due diligence.",
              "control_evidence": ""
            },
            {
              "control": "[Art-27][Par-4] - Complement any existing Data Protection Impact Assessment (DPIA) with the FRIA, if the DPIA already meets some of the obligations.",
              "control_objective": "To streamline compliance by avoiding redundant work, allowing the FRIA to build upon an existing DPIA to cover the broader, specific impacts on all fundamental rights, not just data protection.",
              "control_evidence": ""
            }
          ]
        }
      ]
    }
  ],
  "Phase ISO 42001 Compliance": [
    {
      "StepName": "ISO 42001 Compliance",
      "Objectives": [
        {
          "Objective": "To provide management direction and support for AI systems according to business requirements."
        },
        {
          "Objective": "To establish accountability within the organization to uphold its responsible approach for the implementation, operation and management of AI systems."
        },
        {
          "Objective": "To ensure that the organization accounts for the resources (including AI system components and assets) of the AI system in order to fully understand and address risks and impacts."
        },
        {
          "Objective": "To assess AI system impacts to individuals or groups of individuals, or both, and societies affected by the AI system throughout its life cycle."
        },
        {
          "Objective": "To ensure that the organization identifies and documents objectives and implements processes for the responsible design and development of AI systems."
        },
        {
          "Objective": "To define the criteria and requirements for each stage of the AI system life cycle."
        },
        {
          "Objective": "To ensure that the organization understands the role and impacts of data in AI systems in the application and development, provision or use of AI systems throughout their life cycles."
        },
        {
          "Objective": "To ensure that relevant interested parties have the necessary information to understand and assess the risks and their impacts (both positive and negative)."
        },
        {
          "Objective": "To ensure that the organization uses AI systems responsibly and per organizational policies."
        },
        {
          "Objective": "To ensure that the organization understands its responsibilities and remains accountable, and risks are appropriately apportioned when third parties are involved at any stage of the AI system life cycle."
        }
      ],
      "Fields": [
        {
          "FieldType": "risk",
          "FieldName": "[A.2] - Policies related to AI",
          "question": "Is the organization fully compliant with the requirements of [A.2] - Policies related to AI?",
          "controls": [
            {
              "control": "[A.2.2] - AI policy: The organization shall document a policy for the development or use of AI systems.",
              "control_objective": "The organization shall document a policy for the development or use of AI systems.",
              "control_status": "Not Met",
              "control_evidence": ""
            },
            {
              "control": "[A.2.3] - Alignment with other organizational policies: The organization shall determine where other policies can be affected by or apply to, the organization’s objectives with respect to AI systems.",
              "control_objective": "The organization shall determine where other policies can be affected by or apply to, the organization’s objectives with respect to AI systems.",
              "control_status": "Not Met",
              "control_evidence": ""
            },
            {
              "control": "[A.2.4] - Review of the AI policy: The AI policy shall be reviewed at planned intervals or additionally as needed to ensure its continuing suitability, adequacy and effectiveness.",
              "control_objective": "The AI policy shall be reviewed at planned intervals or additionally as needed to ensure its continuing suitability, adequacy and effectiveness.",
              "control_status": "Not Met",
              "control_evidence": ""
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "[A.3] - Internal organization",
          "question": "Is the organization fully compliant with the requirements of [A.3] - Internal organization?",
          "controls": [
            {
              "control": "[A.3.2] - AI roles and responsibilities: Roles and responsibilities for AI shall be defined and allocated according to the needs of the organization.",
              "control_objective": "Roles and responsibilities for AI shall be defined and allocated according to the needs of the organization.",
              "control_status": "Not Met",
              "control_evidence": ""
            },
            {
              "control": "[A.3.3] - Reporting of concerns: The organization shall define and put in place a process to report concerns about the organization’s role with respect to an AI system throughout its life cycle.",
              "control_objective": "The organization shall define and put in place a process to report concerns about the organization’s role with respect to an AI system throughout its life cycle.",
              "control_status": "Not Met",
              "control_evidence": ""
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "[A.4] - Resources for AI systems",
          "question": "Is the organization fully compliant with the requirements of [A.4] - Resources for AI systems?",
          "controls": [
            {
              "control": "[A.4.2] - Resource documentation: The organization shall identify and document relevant resources required for the activities at given AI system life cycle stages and other AI-related activities relevant for the organization.",
              "control_objective": "The organization shall identify and document relevant resources required for the activities at given AI system life cycle stages and other AI-related activities relevant for the organization.",
              "control_status": "Not Met",
              "control_evidence": ""
            },
            {
              "control": "[A.4.3] - Data resources: As part of resource identification, the organization shall document information about the data resources utilized for the AI system.",
              "control_objective": "As part of resource identification, the organization shall document information about the data resources utilized for the AI system.",
              "control_status": "Met",
              "control_evidence": "These ISO controls, A.4.2 and A.4.3, mandate that the organization formally identify and document all relevant resources—with a specific focus on the data resources—used at any stage of the AI system's lifecycle. This ensures a clear, traceable inventory of the data being utilized. Step's: 'Internal Data Sources' addresses the requirements by systematically collecting key information for each data asset. It captures the dataset's name and description, its source and intended use (e.g., training, validation), its technical format, and the specific method of acquisition. Furthermore, it documents critical governance details such as data retention schedules, procedures for secure disposal, its approximate size, how it is accessed, and the responsible owner or custodian, thereby creating the comprehensive documentation required by the ISO standard."
            },
            {
              "control": "[A.4.4] - Tooling resources: As part of resource identification, the organization shall document information about the tooling resources utilized for the AI system.",
              "control_objective": "As part of resource identification, the organization shall document information about the tooling resources utilized for the AI system.",
              "control_status": "Met",
              "control_evidence": "These ISO controls mandate that we identify and maintain documentation for all resources utilized throughout the AI system's lifecycle. This includes specific information on both software tooling (A.4.4) and the underlying system and computing infrastructure (A.4.5). Step: (A.4.2, A.4.4) AI Systems Software and Tooling Resources addresses the requirements by capturing the specific name and version of each software tool, library, or framework. It further requires classifying each tool by its category and documenting its explicit purpose or use case within the project, which directly satisfies the need to document tooling resources. Step: (A.4.2, A.4.5) AI Systems Computing Resources addresses the requirements by documenting the name and version of each computing resource, assigning it a category (such as a training cluster or cloud service), and detailing the specific lifecycle phase it supports. This provides a clear inventory of the system and computing resources as required."
            },
            {
              "control": "[A.4.5] - System and computing resources: As part of resource identification, the organization shall document information about the system and computing resources utilized for the AI system.",
              "control_objective": "As part of resource identification, the organization shall document information about the system and computing resources utilized for the AI system.",
              "control_status": "Met",
              "control_evidence": "These ISO controls mandate that we identify and maintain documentation for all resources utilized throughout the AI system's lifecycle. This includes specific information on both software tooling (A.4.4) and the underlying system and computing infrastructure (A.4.5). Step: (A.4.2, A.4.4) AI Systems Software and Tooling Resources addresses the requirements by capturing the specific name and version of each software tool, library, or framework. It further requires classifying each tool by its category and documenting its explicit purpose or use case within the project, which directly satisfies the need to document tooling resources. Step: (A.4.2, A.4.5) AI Systems Computing Resources addresses the requirements by documenting the name and version of each computing resource, assigning it a category (such as a training cluster or cloud service), and detailing the specific lifecycle phase it supports. This provides a clear inventory of the system and computing resources as required."
            },
            {
              "control": "[A.4.6] - Human resources: As part of resource identification, the organization shall document information about the human resources and their competences utilized for the development, deployment, operation, change management, maintenance, transfer and decommissioning, as well as verification and integration of the AI system.",
              "control_objective": "As part of resource identification, the organization shall document information about the human resources and their competences utilized for the development, deployment, operation, change management, maintenance, transfer and decommissioning, as well as verification and integration of the AI system.",
              "control_status": "Not Met",
              "control_evidence": ""
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "[A.5] - Assessing impacts of AI systems",
          "question": "Is the organization fully compliant with the requirements of [A.5] - Assessing impacts of AI systems?",
          "controls": [
            {
              "control": "[A.5.2] - AI system impact assessment process: The organization shall establish a process to assess the potential consequences for individuals or groups of individuals, or both, and societies that can result from the AI system throughout its life cycle.",
              "control_objective": "The organization shall establish a process to assess the potential consequences for individuals or groups of individuals, or both, and societies that can result from the AI system throughout its life cycle.",
              "control_status": "Met",
              "control_evidence": "These controls mandate that we establish a formal process to assess potential consequences for individuals, groups, or society that can result from an AI system (A.5.2), and that we document and retain the results of these assessments (A.5.3). The Step '(A.5.2, A.5.3, A.5.4) Vulnerable Populations Impact Assessment' addresses these requirements by providing a structured framework to identify at-risk groups, describe the nature and severity of both positive and negative impacts, and define mitigation and monitoring strategies. This directly establishes the assessment process required by A.5.2, while the act of completing these fields creates the formal documentation needed to satisfy A.5.3. Similarly, the Step '(A.5.2, A.5.3, A.5.5) Environmental Sustainability of AI Systems' addresses the requirements by guiding the assessment of societal impacts, specifically focusing on environmental factors like energy consumption, carbon footprint, and hardware lifecycle management. The fields create a systematic process for evaluating and recording these environmental consequences, thereby fulfilling the assessment (A.5.2) and documentation (A.5.3) mandates."
            },
            {
              "control": "[A.5.3] - Documentation of AI system impact assessments: The organization shall document the results of AI system impact assessments and retain results for a defined period.",
              "control_objective": "The organization shall document the results of AI system impact assessments and retain results for a defined period.",
              "control_status": "Met",
              "control_evidence": "These controls mandate that we establish a formal process to assess potential consequences for individuals, groups, or society that can result from an AI system (A.5.2), and that we document and retain the results of these assessments (A.5.3). The Step '(A.5.2, A.5.3, A.5.4) Vulnerable Populations Impact Assessment' addresses these requirements by providing a structured framework to identify at-risk groups, describe the nature and severity of both positive and negative impacts, and define mitigation and monitoring strategies. This directly establishes the assessment process required by A.5.2, while the act of completing these fields creates the formal documentation needed to satisfy A.5.3. Similarly, the Step '(A.5.2, A.5.3, A.5.5) Environmental Sustainability of AI Systems' addresses the requirements by guiding the assessment of societal impacts, specifically focusing on environmental factors like energy consumption, carbon footprint, and hardware lifecycle management. The fields create a systematic process for evaluating and recording these environmental consequences, thereby fulfilling the assessment (A.5.2) and documentation (A.5.3) mandates."
            },
            {
              "control": "[A.5.4] - Assessing AI system impact on individuals or groups of individuals: The organization shall assess and document the potential impacts of AI systems to individuals or groups of individuals throughout the system’s life cycle.",
              "control_objective": "The organization shall assess and document the potential impacts of AI systems to individuals or groups of individuals throughout the system’s life cycle.",
              "control_status": "Met",
              "control_evidence": "These controls mandate that we establish a formal process to assess potential impacts on individuals, groups, and society (A.5.2), conduct and document that assessment (A.5.4), and retain the results (A.5.3). Step: (A.5.2, A.5.3, A.5.4) Workforce Transition and Adaptation for AI Integration addresses the requirements by systematically evaluating the AI system's impact on our internal workforce. This is achieved by identifying the specific job roles affected, classifying the nature of the impact as augmentation or automation, detailing the tasks to be automated, and documenting the corresponding mitigation strategies, including re-skilling and re-deployment plans. Step: (A.5.2, A.5.3, A.5.4) Vulnerable Populations Impact Assessment addresses the requirements by extending the analysis to external societal groups. It provides a structured method to identify at-risk populations, describe and rate the severity of potential positive or negative impacts, define clear mitigation strategies to prevent harm, and establish a post-deployment monitoring plan to ensure ongoing oversight."
            },
            {
              "control": "[A.5.5] - Assessing societal impacts of AI systems: The organization shall assess and document the potential societal impacts of their AI systems throughout their life cycle.",
              "control_objective": "The organization shall assess and document the potential societal impacts of their AI systems throughout their life cycle.",
              "control_status": "Met",
              "control_evidence": "These controls mandate that we establish a formal process to assess potential societal consequences (A.5.2, A.5.5) that can result from the AI system, and document and retain the results of this assessment (A.5.3) throughout the system's life cycle. Step: (A.5.2, A.5.3, A.5.5) Environmental Sustainability of AI Systems addresses the requirements by providing a structured process to assess and document the system's environmental footprint—a key societal impact. It facilitates this assessment by requiring documentation of the primary energy sources used, an estimation of the total energy consumption and carbon footprint, and the strategies for hardware end-of-life management. Furthermore, it documents the efficiency measures taken to minimize negative impacts, rates the overall level of environmental impact, and outlines a plan for ongoing monitoring and review, ensuring the assessment remains relevant throughout the AI system's lifecycle."
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "[A.6] - AI system life cycle",
          "question": "Is the organization fully compliant with the requirements of [A.6] - AI system life cycle?",
          "controls": [
            {
              "control": "[A.6.1.2] - Objectives for responsible development of AI system: The organization shall identify and document objectives to guide the responsible development AI systems, and take those objectives into account and integrate measures to achieve them in the development life cycle.",
              "control_objective": "The organization shall identify and document objectives to guide the responsible development AI systems, and take those objectives into account and integrate measures to achieve them in the development life cycle.",
              "control_status": "Met",
              "control_evidence": "These ISO controls mandate that the organization establishes and documents clear objectives for the responsible development of AI systems (A.6.1.2) and defines and documents the specific processes for how the design and development will be carried out responsibly (A.6.1.3). Our procedure addresses these requirements by first establishing key objectives for responsible development, such as ensuring Fairness and Non-discrimination and promoting Transparency and Explainability. It then defines the specific processes to achieve these objectives through a structured AI onboarding procedure. A core part of this process is the '(A.6.2.4) AI Systems verifications' step, which operationalizes our responsible AI objectives into auditable actions. For instance: Our objective of Fairness is implemented through the documented 'Fairness and Bias in AI Systems Test Plan'. This plan defines the specific processes for data acquisition, analysis of dataset representativeness, and quantitative testing for historical bias. Similarly, our Transparency objective is implemented via the 'Transparency and Explainability in AI Systems Test Plan', which documents the process for defining audience needs, selecting explanation techniques, and validating their effectiveness. By embedding these detailed, verifiable test plans directly into our development lifecycle, we ensure our high-level objectives (A.6.1.2) are supported by a concrete, documented, and repeatable process (A.6.1.3)."
            },
            {
              "control": "[A.6.1.3] - Processes for responsible AI system design and development: The organization shall define and document the specific processes for the responsible design and development of the AI system.",
              "control_objective": "The organization shall define and document the specific processes for the responsible design and development of the AI system.",
              "control_status": "Met",
              "control_evidence": "These ISO controls mandate that the organization establishes and documents clear objectives for the responsible development of AI systems (A.6.1.2) and defines and documents the specific processes for how the design and development will be carried out responsibly (A.6.1.3). Our procedure addresses these requirements by first establishing key objectives for responsible development, such as ensuring Fairness and Non-discrimination and promoting Transparency and Explainability. It then defines the specific processes to achieve these objectives through a structured AI onboarding procedure. A core part of this process is the '(A.6.2.4) AI Systems verifications' step, which operationalizes our responsible AI objectives into auditable actions. For instance: Our objective of Fairness is implemented through the documented 'Fairness and Bias in AI Systems Test Plan'. This plan defines the specific processes for data acquisition, analysis of dataset representativeness, and quantitative testing for historical bias. Similarly, our Transparency objective is implemented via the 'Transparency and Explainability in AI Systems Test Plan', which documents the process for defining audience needs, selecting explanation techniques, and validating their effectiveness. By embedding these detailed, verifiable test plans directly into our development lifecycle, we ensure our high-level objectives (A.6.1.2) are supported by a concrete, documented, and repeatable process (A.6.1.3)."
            },
            {
              "control": "[A.6.2.2] - AI system requirements and specification: The organization shall specify and document requirements for new AI systems or material enhancements to existing systems.",
              "control_objective": "The organization shall specify and document requirements for new AI systems or material enhancements to existing systems.",
              "control_status": "Met",
              "control_evidence": "ISO 42001 control A.6.2.2 requires that organizations formally specify and document the requirements for new or materially changed AI systems, ensuring those specifications capture both functional and non-functional needs alongside security, risk, and compliance expectations. These details collectively demonstrate that the AI system’s behavior, purpose, and mitigations are fully defined before deployment, aligning system design with organizational, ethical, and regulatory obligations. The “Risk” node in each RAG component addresses this requirement by documenting the security and integrity requirements of individual modules. It defines specific threats (for example, malicious data ingestion or data poisoning), the associated controls, control objectives, and audit evidence, all structured to promote traceability from risk identification through control design. These elements collectively satisfy the documentation requirement for non-functional aspects such as integrity and confidentiality. Complementing the security and risk documentation, the functional requirements are captured through the structured webform aligned with ISO 42001 A.9.4 on intended use. This JSON schema defines how functional details are gathered and documented, ensuring that each AI system’s intended behavior and operational scope are transparent and justified. It includes fields such as the AI System ID, Name, Business Purpose, Intended Use, and Augmented Tasks, along with selectable benefit outcomes (for example, faster service or increased efficiency). By pairing the A.9.4 functional specification webform with each component’s “Risk” node, the AI onboarding documentation holistically fulfills A.6.2.2 by ensuring that every RAG-based component has a documented purpose, defined operational boundary, articulated risks, and mapped controls. This creates a single, integrated framework for demonstrating how AI system design aligns with organizational objectives and responsible use obligations throughout the lifecycle."
            },
            {
              "control": "[A.6.2.3] - Documentation of AI system design and development: The organization shall document the AI system design and development based on organizational objectives, documented requirements and specification criteria.",
              "control_objective": "The organization shall document the AI system design and development based on organizational objectives, documented requirements and specification criteria.",
              "control_status": "Met",
              "control_evidence": "The ISO 42001 A.6.2.3 control mandates comprehensive documentation of the AI system's design and development, linking it directly to organizational objectives and specified requirements. This ensures that the system's architecture and implementation choices are transparent, justifiable, and traceable back to the intended purpose and established standards.\n\nThe overall AI Onboarding procedure, structured as a set of interconnected steps, serves as the complete documentation for the AI system design and development process. Specifically, the 'Architecture, Components, and Dependencies' step directly addresses this control. This step systematically captures all necessary information, including the system's core design elements (e.g., API Gateway, Orchestrator, Database), their operational dependencies, and their functional mapping to the required system objectives. The fields within this step—such as component name, version, role, and inter-component communication mechanisms—create a clear, auditable trail that connects the initial objectives to the final implementation architecture, fulfilling the documentation requirements of A.6.2.3."
            },
            {
              "control": "[A.6.2.4] - AI system verification and validation: The organization shall define and document verification and validation measures for the AI system and specify criteria for their use.",
              "control_objective": "The organization shall define and document verification and validation measures for the AI system and specify criteria for their use.",
              "control_status": "Met",
              "control_evidence": "ISO 42001 control A.6.2.4 requires mandates that we formally define and document the specific measures we will take to verify and validate our AI system, and also specify the exact criteria that determine success or failure for those measures. Step: '(A.6.2.4) AI Systems verifications' addresses the requirements by structuring the validation process into distinct test plans covering key areas such as performance, fairness, transparency, and data quality. For each plan, it documents the 'verification and validation measures' through a detailed list of executable PlanSteps. More importantly, it 'specifies criteria for their use' by defining explicit PlanCriteria, where each criterion contains a measurable success condition (criteria), its purpose (control_objective), and the specific criteria_evidence required to prove it has been met. This ensures our entire validation process is clearly defined, documented, and based on objective, auditable standards."
            },
            {
              "control": "[A.6.2.5] - AI system deployment: The organization shall document a deployment plan and ensure that appropriate requirements are met prior to deployment.",
              "control_objective": "The organization shall document a deployment plan and ensure that appropriate requirements are met prior to deployment.",
              "control_status": "Met",
              "control_evidence": "The ISO control A.6.2.5 requires that organizations formally document a deployment plan for AI systems and verify that all technical, security, ethical, and performance requirements are met before deployment. This ensures a secure, controlled, and compliant rollout of AI solutions by preventing the activation of non-compliant or unstable systems and by maintaining the integrity of deployed components. Step: (A.6.2.5) AI Lifecycle Phase requirements - Deployment addresses these requirements by orchestrating a secure and compliant AI system deployment through several key mechanisms: it requires the documentation of a comprehensive deployment plan that defines the rollout strategy, resource allocation, technical steps, and rollback procedures, providing clear evidence of a controlled deployment process. The step also ensures that prerequisite verification is performed so that all technical, security, ethical, and performance benchmarks are met via rigorous pre-deployment testing and validation. Secure packaging controls are enforced to guarantee the integrity of each deployable artifact, requiring container images to be sourced only from trusted registries, scanned for vulnerabilities, and hardened before deployment, as documented with evidence such as configuration files, automated pruning job logs, and access control settings. Collectively, these documented objectives and fields in your onboarding procedure demonstrate compliance with ISO A.6.2.5 by ensuring all deployment activities are formally structured, thoroughly validated, and securely executed prior to AI system activation."
            },
            {
              "control": "[A.6.2.6] - AI system operation and monitoring: The organization shall define and document the necessary elements for the ongoing operation of the AI system. At the minimum, this should include system and performance monitoring, repairs, updates and support.",
              "control_objective": "The organization shall define and document the necessary elements for the ongoing operation of the AI system. At the minimum, this should include system and performance monitoring, repairs, updates and support.",
              "control_status": "Met",
              "control_evidence": "ISO 42001 control A.6.2.6 mandates that we formally define and document the necessary elements for the ongoing operation of the AI system, including, at a minimum, the processes for system and performance monitoring, repairs, updates, and support.  Step: '(A.6.2.6) AI Lifecycle Phase requirements - Operation and Monitoring' addresses the requirements by using a risk-based approach to document every aspect of the AI system's operational lifecycle. The 'risk' nodes within this step function as the formal documentation for these elements: the 'Insufficient Performance Monitoring...' and 'Inadequate Security Monitoring...' nodes define the requirements for system and performance monitoring; the 'Poor Management of AI System Evolution and Updates' node defines the process for managing updates; and the 'Inadequate Disaster Recovery...' and 'Insufficient Scalability Management' nodes define the technical controls for repairs and support by documenting the required failover, replication, and scaling mechanisms."
            },
            {
              "control": "[A.6.2.7] - AI system technical documentation: The organization shall determine what AI system technical documentation is needed for each relevant category of interested parties, such as users, partners, supervisory authorities, and provide the technical documentation to them in the appropriate form.",
              "control_objective": "The organization shall determine what AI system technical documentation is needed for each relevant category of interested parties, such as users, partners, supervisory authorities, and provide the technical documentation to them in the appropriate form.",
              "control_status": "Met",
              "control_evidence": "ISO 42001 A.6.2.7 requires that the organization must determine and provide the necessary **technical documentation** to various interested parties (e.g., users, partners, authorities) in an appropriate form.\n\nOur procedure addresses this through the **'Architecture, Components, and Dependencies'** step, which is the repository for the primary technical documentation. Specifically, it requires documenting the **System Architecture Diagram** (a critical piece of technical documentation for partners/auditors), the **Component Inventory** (technical details for engineers/maintainers), and the **External Documentation Link** (which is explicitly intended for users or external parties). By mandating that this technical information be compiled and an accessible external link be provided, the procedure ensures that the necessary technical documentation is determined, prepared, and ready for distribution to all relevant interested parties, fulfilling A.6.2.7."
            },
            {
              "control": "[A.6.2.8] - AI system recording of event logs: The organization shall determine at which phases of the AI system life cycle, record keeping of event logs should be enabled, but at the minimum when the AI system is in use.",
              "control_objective": "The organization shall determine at which phases of the AI system life cycle, record keeping of event logs should be enabled, but at the minimum when the AI system is in use.",
              "control_status": "Met",
              "control_evidence": "The ISO 42001 control A.6.2.8 requires that a process is in place to record event logs during the AI system lifecycle, with the minimum requirement that logging is active when the system is in use. This ensures a clear record of system activity for traceability, security, and accountability. Step's: 'RAG Orchestrator' addresses this requirement by functioning as the central logic unit that generates structured and immutable audit logs for every user interaction while the system is operational. These logs create a complete audit trail by capturing critical data points, including the user identifier, timestamp, the full user query, the IDs of retrieved documents, and the final AI-generated response. This process ensures a reliable record of activity is always created during the system's use, providing the evidence needed for security forensics, accountability, and non-repudiation, thereby fulfilling the mandate of the control."
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "[A.7] - Data for AI systems",
          "question": "Is the organization fully compliant with the requirements of [A.7] - Data for AI systems?",
          "controls": [
            {
              "control": "[A.7.2] - Data for development and enhancement of AI system: The organization shall define, document and implement data management processes related to the development of AI systems.",
              "control_objective": "The organization shall define, document and implement data management processes related to the development of AI systems.",
              "control_status": "Met",
              "control_evidence": "These controls (A.7.2, A.7.3, A.7.4, A.7.5, A.7.6) mandate that we define, document, and implement comprehensive data management processes for our AI systems. This includes specifying the details of data acquisition, defining criteria for data preparation and quality, and establishing a clear record of data provenance throughout the AI lifecycle. The requirements of these controls are met over 3 procedure steps which detail our data management strategy. 1. Step: 'Data Processing Pipeline (Vectorise proprietary data)' 2. Step: 'Internal Data Sources'  3. Step: '(A.6.2.4) AI Systems verifications'. Together the procedure steps addresses the requirements by providing a structured framework that documents our entire data management strategy (A.7.2). The four detailed test plans—'Performance & Load Test Plan', 'Fairness and Bias', 'Transparency and Explainability' and 'Data quality requirements'—explicitly define the criteria and methods for data preparation (A.7.6) and set clear, measurable requirements for data quality, including accuracy, completeness, and fairness (A.7.4). The 'Fairness and Bias' plan also addresses data acquisition and selection (A.7.3) by requiring an analysis of the dataset's representativeness against real-world populations. Furthermore, the 'Data quality requirements' plan establishes a concrete process for recording and verifying the data's origin and history through specific provenance criteria (DATA-SEN-05) and associated test steps (BBT-EXT-PRO-01, BBT-EXT-PRO-02), fulfilling the data provenance mandate (A.7.5). The detailed PlanSteps within each section serve as the documented implementation and verification procedure, ensuring these defined requirements are consistently met."
            },
            {
              "control": "[A.7.3] - Acquisition of data: The organization shall determine and document details about the acquisition and selection of the data used in AI systems.",
              "control_objective": "The organization shall determine and document details about the acquisition and selection of the data used in AI systems.",
              "control_status": "Met",
              "control_evidence": "These controls (A.7.2, A.7.3, A.7.4, A.7.5, A.7.6) mandate that we define, document, and implement comprehensive data management processes for our AI systems. This includes specifying the details of data acquisition, defining criteria for data preparation and quality, and establishing a clear record of data provenance throughout the AI lifecycle. The requirements of these controls are met over 3 procedure steps which detail our data management strategy. 1. Step: 'Data Processing Pipeline (Vectorise proprietary data)' 2. Step: 'Internal Data Sources'  3. Step: '(A.6.2.4) AI Systems verifications'. Together the procedure steps addresses the requirements by providing a structured framework that documents our entire data management strategy (A.7.2). The four detailed test plans—'Performance & Load Test Plan', 'Fairness and Bias', 'Transparency and Explainability' and 'Data quality requirements'—explicitly define the criteria and methods for data preparation (A.7.6) and set clear, measurable requirements for data quality, including accuracy, completeness, and fairness (A.7.4). The 'Fairness and Bias' plan also addresses data acquisition and selection (A.7.3) by requiring an analysis of the dataset's representativeness against real-world populations. Furthermore, the 'Data quality requirements' plan establishes a concrete process for recording and verifying the data's origin and history through specific provenance criteria (DATA-SEN-05) and associated test steps (BBT-EXT-PRO-01, BBT-EXT-PRO-02), fulfilling the data provenance mandate (A.7.5). The detailed PlanSteps within each section serve as the documented implementation and verification procedure, ensuring these defined requirements are consistently met."
            },
            {
              "control": "[A.7.4] - Quality of data for AI systems: The organization shall define and document requirements for data quality and ensure that data used to develop and operate the AI system meet those requirements.",
              "control_objective": "The organization shall define and document requirements for data quality and ensure that data used to develop and operate the AI system meet those requirements.",
              "control_status": "Met",
              "control_evidence": "These controls (A.7.2, A.7.3, A.7.4, A.7.5, A.7.6) mandate that we define, document, and implement comprehensive data management processes for our AI systems. This includes specifying the details of data acquisition, defining criteria for data preparation and quality, and establishing a clear record of data provenance throughout the AI lifecycle. The requirements of these controls are met over 3 procedure steps which detail our data management strategy. 1. Step: 'Data Processing Pipeline (Vectorise proprietary data)' 2. Step: 'Internal Data Sources'  3. Step: '(A.6.2.4) AI Systems verifications'. Together the procedure steps addresses the requirements by providing a structured framework that documents our entire data management strategy (A.7.2). The four detailed test plans—'Performance & Load Test Plan', 'Fairness and Bias', 'Transparency and Explainability' and 'Data quality requirements'—explicitly define the criteria and methods for data preparation (A.7.6) and set clear, measurable requirements for data quality, including accuracy, completeness, and fairness (A.7.4). The 'Fairness and Bias' plan also addresses data acquisition and selection (A.7.3) by requiring an analysis of the dataset's representativeness against real-world populations. Furthermore, the 'Data quality requirements' plan establishes a concrete process for recording and verifying the data's origin and history through specific provenance criteria (DATA-SEN-05) and associated test steps (BBT-EXT-PRO-01, BBT-EXT-PRO-02), fulfilling the data provenance mandate (A.7.5). The detailed PlanSteps within each section serve as the documented implementation and verification procedure, ensuring these defined requirements are consistently met."
            },
            {
              "control": "[A.7.5] - Data provenance: The organization shall define and document a process for recording the provenance of data used in its AI systems over the life cycles of the data and the AI system.",
              "control_objective": "The organization shall define and document a process for recording the provenance of data used in its AI systems over the life cycles of the data and the AI system.",
              "control_status": "Met",
              "control_evidence": "These controls (A.7.2, A.7.3, A.7.4, A.7.5, A.7.6) mandate that we define, document, and implement comprehensive data management processes for our AI systems. This includes specifying the details of data acquisition, defining criteria for data preparation and quality, and establishing a clear record of data provenance throughout the AI lifecycle. The requirements of these controls are met over 3 procedure steps which detail our data management strategy. 1. Step: 'Data Processing Pipeline (Vectorise proprietary data)' 2. Step: 'Internal Data Sources'  3. Step: '(A.6.2.4) AI Systems verifications'. Together the procedure steps addresses the requirements by providing a structured framework that documents our entire data management strategy (A.7.2). The four detailed test plans—'Performance & Load Test Plan', 'Fairness and Bias', 'Transparency and Explainability' and 'Data quality requirements'—explicitly define the criteria and methods for data preparation (A.7.6) and set clear, measurable requirements for data quality, including accuracy, completeness, and fairness (A.7.4). The 'Fairness and Bias' plan also addresses data acquisition and selection (A.7.3) by requiring an analysis of the dataset's representativeness against real-world populations. Furthermore, the 'Data quality requirements' plan establishes a concrete process for recording and verifying the data's origin and history through specific provenance criteria (DATA-SEN-05) and associated test steps (BBT-EXT-PRO-01, BBT-EXT-PRO-02), fulfilling the data provenance mandate (A.7.5). The detailed PlanSteps within each section serve as the documented implementation and verification procedure, ensuring these defined requirements are consistently met."
            },
            {
              "control": "[A.7.6] - Data preparation: The organization shall define and document its criteria for selecting data preparations and the data preparation methods to be used.",
              "control_objective": "The organization shall define and document its criteria for selecting data preparations and the data preparation methods to be used.",
              "control_status": "Met",
              "control_evidence": "These controls (A.7.2, A.7.3, A.7.4, A.7.5, A.7.6) mandate that we define, document, and implement comprehensive data management processes for our AI systems. This includes specifying the details of data acquisition, defining criteria for data preparation and quality, and establishing a clear record of data provenance throughout the AI lifecycle. The requirements of these controls are met over 3 procedure steps which detail our data management strategy. 1. Step: 'Data Processing Pipeline (Vectorise proprietary data)' 2. Step: 'Internal Data Sources'  3. Step: '(A.6.2.4) AI Systems verifications'. Together the procedure steps addresses the requirements by providing a structured framework that documents our entire data management strategy (A.7.2). The four detailed test plans—'Performance & Load Test Plan', 'Fairness and Bias', 'Transparency and Explainability' and 'Data quality requirements'—explicitly define the criteria and methods for data preparation (A.7.6) and set clear, measurable requirements for data quality, including accuracy, completeness, and fairness (A.7.4). The 'Fairness and Bias' plan also addresses data acquisition and selection (A.7.3) by requiring an analysis of the dataset's representativeness against real-world populations. Furthermore, the 'Data quality requirements' plan establishes a concrete process for recording and verifying the data's origin and history through specific provenance criteria (DATA-SEN-05) and associated test steps (BBT-EXT-PRO-01, BBT-EXT-PRO-02), fulfilling the data provenance mandate (A.7.5). The detailed PlanSteps within each section serve as the documented implementation and verification procedure, ensuring these defined requirements are consistently met."
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "[A.8] - Information for interested parties of AI systems",
          "question": "Is the organization fully compliant with the requirements of [A.8] - Information for interested parties of AI systems?",
          "controls": [
            {
              "control": "[A.8.2] - System documentation and information for users: The organization shall determine and provide the necessary information to users of the AI system.",
              "control_objective": "The organization shall determine and provide the necessary information to users of the AI system.",
              "control_status": "Met",
              "control_evidence": "ISO 42001 A.8.2 requires the organization to determine and provide the **necessary information to users** of the AI system to ensure safe and effective use.\n\nThis is met by mandating the creation of a definitive user guide within the onboarding procedure. This guide must, at a minimum, include: **1. Intended Use and Limitations (A.9.4)**: Detailing what the system is for and what it is not for. **2. Performance and Accuracy (A.6.2.4)**: Providing the system’s performance metrics. **3. Human Oversight Procedures (A.14)**: Explaining when and how the user must intervene. The **'Architecture, Components, and Dependencies'** step then requires a final **'External Documentation Link'** to this compiled user-facing information. This process ensures the determination of necessary information and its provision to the end-users, fulfilling A.8.2."
            },
            {
              "control": "[A.8.3] - External reporting: The organization shall provide capabilities for interested parties to report adverse impacts of the AI system.",
              "control_objective": "The organization shall provide capabilities for interested parties to report adverse impacts of the AI system.",
              "control_status": "Met",
              "control_evidence": "ISO 42001 A.8.3 mandates that the organization must provide a capability for interested parties (e.g., users, public, affected groups) to report adverse impacts of the AI system.\n\nThis is addressed by mandating the definition of a clear **'Adverse Impact Reporting Mechanism'** within the organization's governance. The procedure requires the team to document the specific mechanism—such as a dedicated email alias ('ai-concerns@org.com'), a web form, or an internal governance portal—that serves as the official intake channel for complaints and adverse impact reports. By formally documenting and defining this mechanism within the onboarding procedure, the organization establishes the required capability for external reporting, thereby fulfilling A.8.3."
            },
            {
              "control": "[A.8.4] - Communication of incidents: The organization shall determine and document a plan for communicating incidents to users of the AI system.",
              "control_objective": "The organization shall determine and document a plan for communicating incidents to users of the AI system.",
              "control_status": "Met",
              "control_evidence": "ISO 42001 A.8.4 requires the organization to determine and document a plan for communicating incidents related to the AI system to its users.\n\nThis is met by mandating the documentation of an explicit **'Incident Communication Plan'** within the overall AI onboarding procedure. This plan must detail, at a minimum, the specific scenarios that trigger communication (e.g., system outage, significant model drift, data breach), the communication channels to be used (e.g., email notification, in-app banner), and the key messages to be conveyed (e.g., nature of the incident, impact, estimated time to resolution). By establishing and documenting this formal plan, the organization ensures a systematic and consistent approach to incident communication, satisfying A.8.4."
            },
            {
              "control": "[A.8.5] - Information for interested parties: The organization shall determine and document their obligations to reporting information about the AI system to interested parties.",
              "control_objective": "The organization shall determine and document their obligations to reporting information about the AI system to interested parties.",
              "control_status": "Met",
              "control_evidence": "ISO 42001 A.8.5 requires the organization to determine and document its obligations for reporting information about the AI system to various interested parties (e.g., regulators, partners, the public).\n\nThis is met by mandating the documentation of a **'Regulatory and External Reporting Obligations'** matrix within the onboarding procedure. This matrix requires the team to systematically identify and document:\n* **The interested party/regulator** (e.g., EU AI Act, Data Protection Authority).\n* **The specific reporting obligation** (e.g., 'notify in case of serious incident', 'submit FRIA results').\n* **The frequency/trigger** (e.g., 'annually', 'within 72 hours of discovery').\n* **The internal team responsible**.\n\nThis process systematically determines and documents all external reporting obligations, creating a single, auditable record that fulfills the requirement of A.8.5."
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "[A.9] - Use of AI systems",
          "question": "Is the organization fully compliant with the requirements of [A.9] - Use of AI systems?",
          "controls": [
            {
              "control": "[A.9.2] - Processes for responsible use of AI systems: The organization shall define and document the processes for the responsible use of AI systems.",
              "control_objective": "The organization shall define and document the processes for the responsible use of AI systems.",
              "control_status": "Met",
              "control_evidence": "This ISO control mandates that we define and document the specific, repeatable processes that ensure the responsible use of our AI systems in day-to-day operations. Our procedure addresses the requirements by documenting a comprehensive, multi-layered process for responsible use that spans policy, technical implementation, and validation. The process begins with the (A.9.3) AI system's Objectives for Responsible Use step, where we formally document the high-level rules by defining permitted use categories, prohibited uses, and authorized users. These rules are then translated into a concrete technical process within the RAG Orchestrator component, which documents the specific controls—like the prompt classification module and prohibited terms monitoring—that build the enforcement of these objectives directly into the system's architecture. Finally, the process is verified through the (A.9.4) - Purpose Limitation and Prompt Category Enforcement Test Plan in the (A.6.2.4) AI Systems verifications step, which documents the exact testing procedures to ensure these technical controls are implemented correctly and are effective. Together, these three steps form a complete, auditable process for defining, technically enforcing, and validating the responsible use of the AI system."
            },
            {
              "control": "[A.9.3] - Objectives for responsible use of AI system: The organization shall identify and document objectives to guide the responsible use of AI systems.",
              "control_objective": "The organization shall identify and document objectives to guide the responsible use of AI systems.",
              "control_status": "Met",
              "control_evidence": "This ISO control mandates that we formally identify and document a clear set of objectives that establish the rules and boundaries for the responsible operation of our AI systems. Step: (A.9.3) AI system's Objectives for Responsible Use addresses the requirements by providing a structured method to document the system's operational boundaries. It does this by defining the approved applications through the 'Permitted User Prompt Categories', establishing clear limitations and compliance guardrails with the 'Known Prohibited Uses', and ensuring accountability by specifying the authorized users in the 'Intended User Profile'. Collectively, these fields create a comprehensive, documented set of objectives that guide the responsible use of the AI system."
            },
            {
              "control": "[A.9.4] - Intended use of the AI system: The organization shall ensure that the AI system is used according to the intended uses of the AI system and its accompanying documentation.",
              "control_objective": "The organization shall ensure that the AI system is used according to the intended uses of the AI system and its accompanying documentation.",
              "control_status": "Met",
              "control_evidence": "This ISO control mandates that we formally document the specific purpose and intended applications of the AI system, and then ensure the system is only operated within those defined boundaries and according to its accompanying documentation. Step: (A.9.4) AI system's intended use and limitations addresses the requirements by creating this foundational documentation. It provides a structured framework to capture the system's explicit 'Business Purpose' and a detailed description of its 'Intended Use'. By further detailing the specific 'Augmented/Enhanced Tasks' and potential benefits, this step produces the official record of intended use that serves as the baseline for all future user training, documentation, and compliance monitoring."
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "[A.10] - Third-party and customer relationships",
          "question": "Is the organization fully compliant with the requirements of [A.10] - Third-party and customer relationships?",
          "controls": [
            {
              "control": "[A.10.2] - Allocating responsibilities: The organization shall ensure that responsibilities within their AI system life cycle are allocated between the organization, its partners, suppliers, customers and third parties.",
              "control_objective": "The organization shall ensure that responsibilities within their AI system life cycle are allocated between the organization, its partners, suppliers, customers and third parties.",
              "control_status": "Met",
              "control_evidence": "ISO 42001 A.10.2 requires that responsibilities for the AI system lifecycle be clearly allocated between the organization and all involved third parties (partners, suppliers, customers).\n\nThis is met by mandating the creation of a formal **'Responsibility Allocation Matrix'** within the onboarding procedure. This matrix requires the team to systematically define who is responsible for each key phase of the AI lifecycle, including:\n* **Data Sourcing/Acquisition**\n* **Model Training/Development**\n* **Deployment/Hosting**\n* **Ongoing Monitoring/Maintenance**\n\nFor each phase, the matrix must explicitly list the responsible entity (e.g., 'Internal Dev Team', 'Cloud Vendor', 'Data Partner'), ensuring that no responsibility is unassigned or duplicated, thereby fulfilling the allocation requirement of A.10.2."
            },
            {
              "control": "[A.10.3] - Suppliers: The organization shall establish a process to ensure that its usage of services, products or materials provided by suppliers aligns with the organization’s approach to the responsible development and use of AI systems.",
              "control_objective": "The organization shall establish a process to ensure that its usage of services, products or materials provided by suppliers aligns with the organization’s approach to the responsible development and use of AI systems.",
              "control_status": "Met",
              "control_evidence": "ISO 42001 A.10.3 mandates that the organization establishes a process to ensure that the services, products, or materials provided by suppliers (including foundational models, cloud services, and datasets) align with the organization's approach to the responsible development and use of AI systems.\n\nThis is met by mandating a formal **'AI Supplier Due Diligence'** step. This step requires the team to document:\n1.  **Supplier Vetting**: The process used to assess the supplier's own responsible AI policies and compliance (e.g., confirming ISO 42001 certification or similar). \n2.  **Contractual Requirements**: Vetting the contract to ensure the supplier's obligations regarding data quality, security, and ethical use are explicitly defined and align with our internal standards.\n3.  **Exit Strategy**: Documenting a plan for switching providers or services, which is a key component of mitigating supplier risk.\n\nThis documented due diligence process ensures a systematic method for supplier alignment and risk management, fulfilling the requirement of A.10.3."
            },
            {
              "control": "[A.10.4] - Customers: The organization shall ensure that its responsible approach to the development and use of AI systems considers their customer expectations and needs.",
              "control_objective": "The organization shall ensure that its responsible approach to the development and use of AI systems considers their customer expectations and needs.",
              "control_status": "Met",
              "control_evidence": "ISO 42001 A.10.4 requires the organization to ensure that its responsible approach to AI considers customer expectations and needs.\n\nThis is met by mandating the documentation of **'Customer Engagement and Feedback Mechanisms'**. The procedure requires the team to document:\n1.  **Feedback Channels**: The official channels for customer input on the AI system's performance and impact (e.g., dedicated customer support line, user forums).\n2.  **Expectation Management**: How customer expectations are set and managed, linking back to the official documentation of the AI system's **Intended Use and Limitations (A.9.4)**.\n3.  **Adverse Impact Reporting (A.8.3)**: Leveraging the mandatory adverse impact reporting mechanism (A.8.3) to capture and respond to critical customer concerns.\n\nBy systematically documenting how customer input is gathered, expectations are set, and concerns are addressed, the organization demonstrates consideration of customer needs and fulfills the requirement of A.10.4."
            }
          ]
        }
      ]
    }
  ],
  "Phase AI system information": [
    {
      "StepName": "(A.9.4) AI system's intended use and limitations",
      "Objectives": [
        {
          "Objective": "Document the purpose, target users, and intended use cases of the AI system."
        }
      ],
      "Fields": [
        {
          "Control": "A.9.4 – Intended use of the AI system",
          "FieldName": "(A.9.4) - AI System ID",
          "FieldLabel": "AI System ID",
          "FieldText": "AI System Unique Number",
          "FieldType": "Auto generated number"
        },
        {
          "Control": "A.9.4 – Intended use of the AI system",
          "FieldName": "(A.9.4) - Name",
          "FieldLabel": "Name",
          "FieldText": "Name of the AI application",
          "FieldType": "TextBox"
        },
        {
          "Control": "A.9.4 – Intended use of the AI system",
          "FieldName": "(A.9.4) - Business Purpose",
          "FieldLabel": "Business Purpose",
          "FieldText": "What specific business problem or task does this system address?",
          "FieldType": "TextBox"
        },
        {
          "Control": "A.9.4 – Intended use of the AI system",
          "FieldName": "(A.9.4) - Intended Use",
          "FieldLabel": "Intended Use",
          "FieldText": "Describe the use cases of how the AI system will solve the specific business problem or task",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "AI system impact assessment - Employees - Benefits",
          "FieldLabel": "Select Potential Benefits",
          "FieldText": "",
          "FieldType": "MultiSelect:Faster Service/Reduced Error Rate/Increased Efficiency/Personalized Training or Upskilling/Better Decision-Making Tools"
        },
        {
          "Control": "A.9.4 – Intended use of the AI system",
          "FieldName": "HR-WIA-2.3-AugmentationTasks",
          "FieldLabel": "Augmented/Enhanced Tasks",
          "FieldText": "List the high-value tasks that will be significantly improved, made more accurate, or accelerated by the AI system's assistance.",
          "FieldType": "TextBox"
        }
      ]
    },
    {
      "StepName": "(A.9.3) AI system's Objectives for Responsible Use",
      "Objectives": [
        {
          "Objective": "1. Establish Clear Operational Boundaries: To formally document the precise scope of the AI system by defining what it is built to do (Permitted Prompt Categories), what it must never be used for (Known Prohibited Uses), and who is authorized to use it (Intended User Profile)."
        },
        {
          "Objective": "2. Ensure Safety and Regulatory Compliance: To proactively mitigate risks and ensure compliance with regulations like the EU AI Act by explicitly identifying and forbidding high-risk or prohibited applications from the outset."
        },
        {
          "Objective": "3. Promote Effective and Authorized Use: To ensure the system is used effectively and safely by restricting its operation to trained, authorized users and guiding them towards the specific, validated use cases for which the system was designed."
        }
      ],
      "Fields": [
        {
          "FieldName": "AI system intended use - Permitted User Prompt Categories Whitelist Definition",
          "FieldLabel": "Whitelist Key Phrases for Permitted User Prompt Categories",
          "FieldText": "This is the reference list of key phrases that will be allowed by the RAG Orchestrator. The RAG Orchestrator will be trained to recognize prompts similar to these examples.",
          "FieldType": "MultiSelect:Information Retrieval and Q&A:what is, who is, where can I find, tell me about, explain the, what are the details on/Document Summarization:summarize this, give me the key points, tl;dr, provide a summary of, what are the main takeaways, create an abstract for/Content Creation and Drafting:draft an email about, write a paragraph on, create a marketing blurb for, help me write, generate a draft of, compose a message about/Policy and Procedure Guidance:what is the policy for, how do I request, what is the procedure for, explain the process of, guide me through the steps for/Onboarding and Training Support:how do I set up, what training do I need for, explain the onboarding process, guide for new hires, help me learn about/Technical Support and Troubleshooting:how to fix, I'm getting an error, troubleshoot this issue, what does this error mean, help me with this technical problem/Data Analysis and Extraction:extract all the names from, what is the total of, find the average of, pull the data for, list all the dates in, analyze the trends in"
        },
        {
          "FieldName": "AI system intended use - Prohibited Uses Blacklist Definition",
          "FieldLabel": "Blacklist Key Phrases for Prohibited User Prompt Categories",
          "FieldText": "This is a reference list of key phrases that will be blocked by the RAG Orchestrator. AI systems that support these key phrases are prohibited by the AI act.",
          "FieldType": "MultiSelect:Manipulative or Subliminal Techniques Causing Harm:influence my decision, make me buy, change my mind, secretly persuade, subliminal message, hidden command, trick me into, deceive me/Exploitation of Vulnerabilities (e.g., age, disability):target children with ads, exploit elderly, convince a disabled person, addictive game for kids, target financially desperate, exploit vulnerability/General-Purpose Social Scoring:calculate social score, rate my behavior, trustworthiness score, classify people's value, social credit system, evaluate a person based on/Predictive Policing Based Solely on Profiling:predict who will commit a crime, crime risk assessment for a person, profile potential criminals, likelihood of reoffending, predict recidivism/Untargeted Scraping of Facial Images for Databases:scrape faces from internet, create facial recognition database, find all images of a person online, collect CCTV footage for faces, build a face dataset/Emotion Recognition in Workplace or Education:analyze employee emotions, detect student mood, tell me if my team is happy, monitor engagement with emotion, read facial expressions for performance/Biometric Categorization Using Sensitive Data (e.g., race, religion):guess race from photo, determine political opinion from text, infer religious beliefs, categorize by sexual orientation, deduce sex life/'Real-Time' Remote Biometric ID in Public Spaces:live facial recognition, track people in real-time with cameras, identify everyone in this crowd, public camera identification, real-time biometric tracking"
        },
        {
          "FieldName": "AI system response - Decision-Making Language Requiring Intervention",
          "FieldLabel": "Key Phrases in LLM Responses That Require Human Intervention",
          "FieldText": "This is a reference list of key phrases in the LLM's output that indicate it may be making a business decision or a decision with potential risks. If these phrases are detected, the system should flag the response for immediate human oversight and potential intervention, allowing an overseer to stop or override the action.",
          "FieldType": "MultiSelect:Financial and Business Directives:approve the loan, reject the claim, allocate budget, set the price at, hire candidate, terminate contract, invest in, authorize payment, proceed with purchase/Health and Safety Determinations:the diagnosis is, you likely have, prescribe treatment, this is a medical recommendation, it is safe to proceed, declare unsafe, risk level is critical, initiate safety protocol/Eligibility and Rights-Based Judgements:grant access to, deny benefits, is eligible for, application approved, request denied, assign credit score of, this is legally compliant, classify as high-risk/Automated Action and Execution:I have executed the transfer, the system will automatically, action has been triggered, the process is now initiated, I have decided to, the final decision is/Definitive Predictions and Classifications:the outcome will be, I predict with certainty, this will happen, the final score is, this person's risk profile is, this is the correct answer, conclude that"
        },
        {
          "FieldName": "AI system intended use - Intended User Profile",
          "FieldLabel": "Select the Intended User Profiles for the AI System",
          "FieldText": "Define the specific roles, departments, or groups of individuals who are authorized to use the system. This helps to control access and ensure the system is used by those with the appropriate context and training.",
          "FieldType": "MultiSelect:All Employees/Specific Department(s)/Management and Leadership/Technical Administrators and Developers/External Partners or Clients/Executive Leadership Only"
        }
      ]
    },
    {
      "StepName": "x(A.4.2, A.4.4) AI Systems Software and Tooling Resources",
      "Objectives": [
        {
          "Objective": "Define the software and tools used by the AI system."
        }
      ],
      "Fields": [
        {
          "FieldName": "(A.4.2, A.4.4) - Tool Name and Version",
          "FieldLabel": "Tools Name and Version",
          "FieldText": "Name of the software, libraries, or frameworks and their Version numbers.",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "(A.4.2, A.4.4) - Category",
          "FieldLabel": "Select Tool Categories",
          "FieldText": "",
          "FieldType": "MultiSelect:Programming Language/IDE/Data Processing/ML Framework/Version Control/Deployment"
        },
        {
          "FieldName": "(A.4.2, A.4.4) - Purpose/Use Case in Project",
          "FieldLabel": "Purpose/Use Case in Project",
          "FieldText": "How these tools will be used in the AI system's lifecycle.",
          "FieldType": "TextBox"
        }
      ]
    },
    {
      "StepName": "x(A.4.2, A.4.5) AI Systems Computing Resources",
      "Objectives": [
        {
          "Objective": "Define the computing resources used by the AI system"
        }
      ],
      "Fields": [
        {
          "FieldName": "(A.4.2, A.4.5) - Computing Resource Name and Version",
          "FieldLabel": "Computing Resource Name and Version",
          "FieldText": "Names of the computing resources used.",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "(A.4.2, A.4.5) - Category",
          "FieldLabel": "Category of the computing resource",
          "FieldText": "",
          "FieldType": "MultiSelect:Dev Workstation/ML Training Cluster/Inference API Server/Data Lake Storage/EC2/S3/SQL Database"
        },
        {
          "FieldName": "(A.4.2, A.4.5) - Lifecycle Phase(s) Supported",
          "FieldLabel": "Which parts of the AI lifecycle do these resource supports?",
          "FieldText": "",
          "FieldType": "MultiSelect:Development/Training/Testing/Staging/Production/Monitoring"
        }
      ]
    }
  ],
  "Phase Impact assessments": [
    {
      "StepName": "x(A.5.2, A.5.3, A.5.4) Workforce Transition and Adaptation for AI Integration",
      "Objectives": [
        {
          "Objective": "Define the workforce adaptation and training strategies to address risks from job role evolution due to AI adoption."
        }
      ],
      "Fields": [
        {
          "Control": "A.9.4 – Intended use of the AI system",
          "FieldName": "(A.9.4) - Target Users",
          "FieldLabel": "Select the job titles whose daily tasks may be altered by more than 20% due to the AI system",
          "FieldText": "",
          "FieldType": "MultiSelect:Employees/Customers/Analysts/Customer/Supplier/Partner/Regulator"
        },
        {
          "FieldName": "HR-WIA-1.2-CoreAction",
          "FieldLabel": "Identify the primary roles of the AI system relative to human workers",
          "FieldText": "",
          "FieldType": "MultiSelect:Augmentation (assisting human judgment)/Automation (replacing tasks)/Creation (enabling new tasks)"
        },
        {
          "FieldName": "HR-WIA-2.2-AutomationTasks",
          "FieldLabel": "Automated/Eliminated Tasks",
          "FieldText": "List the specific tasks that will be fully automated or eliminated for the affected roles, and the estimated percentage of work time saved across the department.",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "HR-WIA-4.1-MitigationPlan",
          "FieldLabel": "Primary Mitigation Strategy for Displacement",
          "FieldText": "If job displacement is identified, select the primary strategies for the affected workers",
          "FieldType": "MultiSelect:Internal Re-deployment/Transfer/Managed Attrition (No Backfill)/Voluntary Separation Package/External Layoff"
        },
        {
          "FieldName": "HR-WIA-4.2-TrainingProgram",
          "FieldLabel": "Structured Re-skilling Program in Place",
          "FieldText": "Describe the primary strategies to address the affected workers.",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "HR-WIA-4.2-TrainingProgram",
          "FieldLabel": "Structured Re-skilling Program Effectiveness",
          "FieldText": "Describe the Training Effectiveness measures to evaluate the success of the primary strategies to address the affected workers.",
          "FieldType": "TextBox"
        }
      ]
    },
    {
      "StepName": "x(A.5.2, A.5.3, A.5.4) Vulnerable Populations Impact Assessment",
      "Objectives": [
        {
          "Objective": "Define and evaluate the AI system's impacts, risks, and mitigation strategies specific to vulnerable populations affected by deployment."
        }
      ],
      "Fields": [
        {
          "FieldName": "VP-IMP-1.1-PopulationType",
          "FieldLabel": "Select the at-risk group(s) impacted by the AI system",
          "FieldText": "",
          "FieldType": "Option box with values:Children/Elderly/Persons with Disabilities/Economically Disadvantaged/Ethnic Minorities/Other (specify)"
        },
        {
          "FieldName": "VP-IMP-2.1-ImpactType",
          "FieldLabel": "Negitive or positive impacts",
          "FieldText": "Describe the specific ways the AI system could negatively or positively affect the vulnerable population identified.",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "VP-IMP-2.2-SeverityAssessment",
          "FieldLabel": "Rate the severity of identified impacts",
          "FieldText": "",
          "FieldType": "Dropdown box with values:/Low/Medium/High"
        },
        {
          "FieldName": "VP-IMP-2.3-SeverityAssessment",
          "FieldLabel": "Describe the severity of identified impacts",
          "FieldText": "",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "VP-IMP-3.1-MitigationStrategy",
          "FieldLabel": "Describe the main strategies chosen to prevent or reduce negative impacts for the vulnerable population",
          "FieldText": "",
          "FieldType": "MultiSelect:Enhanced Access/Targeted Education & Support/Explicit User Consent/Safeguard Controls/Stakeholder Engagement/Other (specify)"
        },
        {
          "FieldName": "VP-IMP-4.1-StakeholderInclusion",
          "FieldLabel": "Have any of the identified vulnerable populations or their representatives been consulted during the design, development, or testing of the AI system?",
          "FieldText": "",
          "FieldType": "Option box with values:Yes, extensive consultation/Yes, limited consultation/No consultation conducted/Not applicable"
        },
        {
          "FieldName": "VP-IMP-4.2-SeverityAssessment",
          "FieldLabel": "Impact Description",
          "FieldText": "Describe the severity of identified impacts.",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "VP-IMP-3.2-MitigationEffectiveness",
          "FieldLabel": "Post-Deployment Monitoring Plan",
          "FieldText": "Describe the plan for monitoring the AI system's performance and impact on vulnerable populations after deployment. Include key metrics and the frequency of review.",
          "FieldType": "TextBox"
        }
      ]
    },
    {
      "StepName": "x(A.5.2, A.5.3, A.5.5) Environmental Sustainability of AI Systems",
      "Objectives": [
        {
          "Objective": "Define proper environmental assessments, eco-efficient practices, and sustainable lifecycle management controls to minimize ecological impacts."
        }
      ],
      "Fields": [
        {
          "FieldName": "ENV-IA-1.1-EnergySource",
          "FieldLabel": "Specify the primary sources of electricity for the data centers used for training and deployment of the AI system.",
          "FieldText": "",
          "FieldType": "MultiSelect:Primarily Renewable (e.g., Solar, Wind, Hydro)/Regional Grid Mix/Primarily Fossil Fuels/Unknown or Not Specified by Provider"
        },
        {
          "FieldName": "ENV-IA-2.1-ConsumptionMetrics",
          "FieldLabel": "Energy Consumption & Carbon Footprint Estimation",
          "FieldText": "Describe the methodology and provide the estimated energy consumption (e.g., in kWh) and carbon footprint (e.g., in tons of CO2e) for the system's lifecycle (training, validation, and deployment).",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "ENV-IA-3.1-HardwareEOL",
          "FieldLabel": "Select the primary strategies for managing the hardware (e.g., servers, GPUs) at the end of its useful life.",
          "FieldText": "",
          "FieldType": "MultiSelect:Certified E-waste Recycling Program/Component Refurbishment and Reuse/Return to Manufacturer for Disposal/Unmanaged Disposal"
        },
        {
          "FieldName": "ENV-IA-4.1-EfficiencyMeasures",
          "FieldLabel": "Select main strategies for minimizing negative environmental impacts.",
          "FieldText": "",
          "FieldType": "MultiSelect:Renewable Energy Use/Model Size Optimization/Cool Data Management/E-waste Recycling/Sustainable Supply Chain/Other (specify)"
        },
        {
          "FieldName": "ENV-IA-4.2-ImpactLevel",
          "FieldLabel": "Level of Environmental Impact",
          "FieldText": "Rate the degree of environmental burden/benefit (with rationale).",
          "FieldType": "Dropdown box with values:/Low/Medium/High"
        },
        {
          "FieldName": "VP-IMP-2.3-SeverityAssessment",
          "FieldLabel": "Describe the Environmental Impact",
          "FieldText": "Describe the degree of environmental burden/benefit.",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "ENV-IA-5.1-PositiveContribution",
          "FieldLabel": "Contribution to Environmental Sustainability Goals",
          "FieldText": "Does the AI system's intended application directly contribute to positive environmental outcomes (e.g., climate change modeling, energy grid optimization, biodiversity monitoring)? If yes, please describe.",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "ENV-IA-6.1-MonitoringReview",
          "FieldLabel": "Environmental Impact Monitoring and Review",
          "FieldText": "Describe the process and frequency for monitoring and reviewing the environmental performance metrics of the AI system over its lifecycle.",
          "FieldType": "TextBox"
        }
      ]
    }
  ],
  "Phase Data Preparation (Offline or Real-Time Knowledge Base Creation)": [
    {
      "StepName": "xInternal Data Sources",
      "WebFormTitle": "To uphold the principles of data integrity, relevance, currency, and compliance by creating a governed process to identify, review, and approve all proprietary data sources designated for the AI's knowledge base.",
      "Objectives": [
        {
          "Objective": "1. Data Integrity: This principle is about ensuring the information we use is the official source of truth. It means we select data from authoritative systems and explicitly exclude unofficial sources like draft documents, personal notes, or unverified information. This ensures the AI's answers are not just relevant and current, but are based on correct, approved company data."
        },
        {
          "Objective": "2. Data Relevance: This principle ensures that we only feed the AI information related to its designated function. Just as you wouldn't give a pilot a cookbook to fly a plane, we must prevent irrelevant data from polluting the AI's knowledge base. This step establishes a clear scope to keep the AI's answers focused, accurate, and helpful."
        },
        {
          "Objective": "3. Data Currency: An AI's answers are only as reliable as the information it has learned from. This step establishes a process to ensure that only up-to-date, current documents are used, and that obsolete information is explicitly excluded. This prevents the AI from providing answers based on outdated policies, which could lead to significant business risks."
        },
        {
          "Objective": "4. Data Compliance: This is a critical governance checkpoint to protect our company, employees, and customers. Before any data source is approved, it is reviewed to ensure it adheres to privacy laws (like GDPR) and our internal ethics policies. This prevents sensitive personal information or other inappropriate content from being included in the AI's knowledge base."
        }
      ],
      "Fields": [
        {
          "FieldName": "(A.4.2, A.4.3) - Dataset Repository",
          "FieldLabel": "Dataset Repository",
          "FieldText": "All data comprising the AI systems dataset will be aggregated and stored in a central data repository.  List the path of the data repository.",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "(A.4.2, A.4.3) - Dataset Description",
          "FieldLabel": "Dataset Description",
          "FieldText": "Brief overview of the dataset's content and general purpose.",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "(A.4.2, A.4.3, A.7.5) - Source",
          "FieldLabel": "Where does the data originate from?",
          "FieldText": "",
          "FieldType": "MultiSelect:Internal database/Third-party vendor/Public repository/Synthetic generation"
        },
        {
          "FieldName": "(A.4.2, A.4.3) - Intended Use",
          "FieldLabel": "What is the specific purpose(s) of the data?",
          "FieldText": "",
          "FieldType": "Dropdown box with values:/Training data/Validation data/Test data/Production data"
        },
        {
          "FieldName": "(A.4.2, A.4.3, A.7.3, A.7.2) - Acquisition of data",
          "FieldLabel": "How will the Data be acquired and selected?",
          "FieldText": "",
          "FieldType": "MultiSelect:Extracted from internal company databases (e.g., CRM, ERP)/Sourced from a publicly available dataset/Purchased or licensed from a third-party data provider/Collected directly from users with explicit consent/Scraped from public websites in compliance with terms of service/Streamed from IoT sensors or application logs/Artificially generated (synthetic data)/Selected based on defined quality and relevance criteria/Manually curated by subject matter experts/Sampled to ensure fair representation of subgroups (stratified sampling)/A combination of multiple sources"
        },
        {
          "FieldName": "(A.4.2, A.4.3) - Retention Schedules",
          "FieldLabel": "What is the data Retention schedules based on legal and operational requirements?",
          "FieldText": "",
          "FieldType": "Dropdown box with values:/1 Year/5 Years/10 Years"
        },
        {
          "FieldName": "(A.4.2, A.4.3) - Refresh Schedules",
          "FieldLabel": "How often does the data need to be refreshed?",
          "FieldText": "",
          "FieldType": "MultiSelect:Adhoc based on regulatory changes/Monthly/Quarterly/Yearly"
        },
        {
          "FieldName": "(A.4.2, A.4.3) - Secure Disposing",
          "FieldLabel": "Secure Disposing",
          "FieldText": "Descripbe the Secure methods for disposing of obsolete or redundant data.",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "(A.4.2, A.4.3) - Approximate Size",
          "FieldLabel": "Approximate Size",
          "FieldText": "What is the Estimated size of the data (e.g., number of records, GB, TB)?",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "(A.4.2, A.4.3) - Owner/Custodian",
          "FieldLabel": "Owner/Custodian",
          "FieldText": "Person or team responsible for the data maintenance, refresh and disposal?",
          "FieldType": "TextBox"
        },
        {
          "FieldType": "risk",
          "FieldName": "Malicious Data Ingestion (Data Poisoning & Indirect Prompt Injection)",
          "RiskDescription": "The risk that malicious actors could deliberately corrupt the internal data sources (e.g., SharePoint, Confluence) that feed the AI's knowledge base. This can be done by inserting false information to mislead users (Data Poisoning) or by embedding hidden commands to hijack the AI's behavior and potentially leak data (Indirect Prompt Injection).",
          "question": "Does the AI system ingest data from internal sources (like wikis, shared drives, or databases) where a number of users have permission to create or edit content?",
          "controls": [
            {
              "control": "[DATA-INT-01] - Implement and enforce Role-Based Access Control (RBAC) with the principle of least privilege on all source data repositories to restrict write and modify permissions to only authorized personnel.",
              "control_objective": "To prevent unauthorized users from introducing malicious or erroneous content at the source, directly mitigating risks of intentional Data Poisoning.",
              "control_evidence": "A documented list of user roles and their assigned permissions for the data repository; screenshots or configuration exports from the system's access control panel demonstrating the principle of least privilege; and records of periodic access reviews."
            },
            {
              "control": "[DATA-INT-02] - Enforce mandatory version control with detailed audit logs on all source data repositories. All changes, additions, and deletions must be attributable to a specific user and timestamp.",
              "control_objective": "To ensure a complete, auditable history of all changes to the knowledge base's source data, enabling rapid detection of unauthorized modifications and rollback to a last-known-good state.",
              "control_evidence": "Screenshots of the version control system's settings demonstrating that versioning is active, and a sample commit/change history log showing a clear attribution of changes to a specific user, timestamp, and a description of the change."
            },
            {
              "control": "[DATA-INT-03] - Establish a mandatory content approval workflow for the addition or significant modification of documents in designated high-sensitivity data sources before they are ingested by the AI system.",
              "control_objective": "To create a formal human-in-the-loop verification gate that ensures the authenticity and appropriateness of critical information, providing a strong defense against both deliberate Data Poisoning and unintentional quality issues.",
              "control_evidence": "Documentation of the content approval process including designated approvers, and sample evidence such as a completed Pull Request with mandatory reviewer approvals, or a change management ticket (e.g., in Jira) with a logged approval signature."
            },
            {
              "control": "[DATA-INT-04] - Ensure every data chunk processed and stored in the vector database retains immutable metadata linking it directly to its source document, version, and author.",
              "control_objective": "To maintain full data provenance, enabling users and administrators to verify the source of any information provided by the AI and to facilitate the targeted removal of compromised data if a poisoning incident is discovered.",
              "control_evidence": "A sample query output from the vector database displaying a data chunk alongside its associated metadata fields (e.g., source_document_name, version_id, ingestion_timestamp), and a code snippet from the data ingestion pipeline explicitly demonstrating how this metadata is extracted and attached."
            },
            {
              "control": "[DATA-INT-05] - During the 'Text Extraction & Cleaning' phase of the Data Processing Pipeline, implement a sanitization step that identifies and neutralizes or strips potential prompt injection payloads before the text is sent for embedding.",
              "control_objective": "To create an automated defense layer that actively blocks malicious instructions from ever entering the vector knowledge base, protecting the LLM from being hijacked by poisoned source data.",
              "control_evidence": "Code review of the data processing script showing the sanitization function or library in use (e.g., using regular expressions or a dedicated library like Rebuff). Unit test results demonstrating that known prompt injection strings are successfully identified and removed from a sample document."
            },
            {
              "control": "[DATA-INT-06] - The RAG Orchestrator must be configured to include citations or direct links back to the source documents (leveraging the metadata from control DATA-INT-04) within every response generated by the LLM.",
              "control_objective": "To empower users to verify the AI's claims and sources, thereby reducing the impact of successful data poisoning by making false information easily identifiable and auditable.",
              "control_evidence": "Screenshots or logs of the final user-facing response clearly showing embedded citations or links. A code snippet from the RAG Orchestrator demonstrating how the source metadata from the retrieved chunks is formatted and appended to the final LLM response."
            }
          ]
        }
      ]
    },
    {
      "StepName": "xData Processing Pipeline (Vectorise proprietary data)",
      "WebFormTitle": "To uphold the integrity and trustworthiness of the AI's knowledge base by ensuring all source data is accurately extracted, thoroughly cleaned of irrelevant artifacts, and logically chunked for optimal processing.",
      "Objectives": [
        {
          "Objective": "ToDo: 1. Accurate Extraction: This is the first and most crucial step to ensure the AI learns from the correct information. It involves carefully reading the source documents, including complex formats like scanned PDFs, to create a perfect digital text copy. An error here could be like the AI misreading a word, which could alter the meaning of a key fact."
        },
        {
          "Objective": "2. Thorough Cleaning: Documents often contain digital noise that isn't part of the core knowledge, such as page numbers, headers, footers, or website navigation links. This step acts as a filter, removing this irrelevant information so the AI can focus purely on the valuable content, leading to clearer and more relevant answers."
        },
        {
          "Objective": "3. Logical Chunking: An AI cannot process an entire 100-page document at once to answer a single question. This step intelligently breaks down long documents into smaller, bite-sized, and contextually complete paragraphs or ideas. This is like creating a perfectly indexed and bookmarked version of the knowledge, making it possible for the AI to quickly find the single most relevant piece of information later on."
        }
      ],
      "Fields": [
        {
          "FieldType": "plan",
          "FieldName": "(A.7.6) Sensitive Data Preparation Plan",
          "PlanObjective": "Ensure that raw sensitive data is anonymised before it is used by the AI system.",
          "PlanCriteria": [
            {
              "criteria": "ANON-PSEUD - 01: Direct personal identifiers must be pseudonymized using established techniques such as masking, tokenization, or hashing.",
              "control_objective": "To obscure or replace sensitive data elements in a reversible or non-reversible way, preventing direct identification of individuals while preserving data utility.",
              "criteria_evidence": "Data processing scripts or configuration files demonstrating the application of pseudonymization techniques. A comparative data sample showing data before and after the transformations, with sensitive fields appropriately masked, tokenized, or hashed."
            },
            {
              "criteria": "ANON-MIN - 01: The principle of data minimization must be applied by reducing data precision and removing unnecessary features.",
              "control_objective": "To reduce the risk of re-identification and adhere to data privacy principles by ensuring only essential data with the minimum required level of detail is processed.",
              "criteria_evidence": "The data schema for the final processed dataset, confirming the absence of pruned columns. A comparative data sample showing data before and after generalization has been applied to relevant fields."
            },
            {
              "criteria": "ANON-INT - 01: The dataset must be free of duplicate records to ensure its integrity and quality.",
              "control_objective": "To prevent data skew, improve processing efficiency, and ensure that each data point is unique, leading to a more reliable and clean dataset for the AI system.",
              "criteria_evidence": "Logs or reports from the data preparation script that quantify the number of duplicate records identified and removed. A data sample demonstrating the dataset before and after the de-duplication process."
            }
          ],
          "PlanSteps": [
            {
              "step": "ANON-PSEUD-TEST-01: Apply masking to a sample of raw data and verify that sensitive identifiers are correctly obscured with generic characters.",
              "step_objective": "To confirm that the masking technique is correctly implemented to limit the exposure of sensitive identifiers, fulfilling a component of criterion ANON-PSEUD-01."
            },
            {
              "step": "ANON-PSEUD-TEST-02: Apply tokenization to a sample of raw data and verify that sensitive elements are replaced with non-sensitive tokens.",
              "step_objective": "To validate that tokenization effectively substitutes sensitive values, reducing risk while meeting the requirements of criterion ANON-PSEUD-01."
            },
            {
              "step": "ANON-PSEUD-TEST-03: Apply a salted cryptographic hash function (e.g., SHA-256) to a sensitive field and verify that the output is a fixed-size, irreversible string.",
              "step_objective": "To ensure that data is protected using a one-way cryptographic method, satisfying the hashing requirement of criterion ANON-PSEUD-01."
            },
            {
              "step": "ANON-MIN-TEST-01: Apply generalization to a sample of data (e.g., age, date) and verify that the precision is correctly reduced to a less specific category (e.g., age range, year).",
              "step_objective": "To test that generalization is effectively reducing the risk of re-identification, providing evidence for criterion ANON-MIN-01."
            },
            {
              "step": "ANON-MIN-TEST-02: Execute the feature pruning script and verify that the specified non-essential columns are removed from the output dataset.",
              "step_objective": "To confirm adherence to the data minimization principle by eliminating superfluous data, as required by criterion ANON-MIN-01."
            },
            {
              "step": "ANON-INT-TEST-01: Process a sample dataset containing known duplicates and verify that the de-duplication script correctly identifies and removes the redundant records.",
              "step_objective": "To validate the effectiveness of the de-duplication process, ensuring the final dataset meets the integrity standard of criterion ANON-INT-01."
            }
          ]
        },
        {
          "FieldType": "plan",
          "FieldName": "(A.7.4, A.7.6) - Data quality requirements & Test Plan",
          "PlanObjective": "Ensure that data used to develop and operate the AI system meet defined Data Quality Requirements?",
          "PlanCriteria": [
            {
              "criteria": "DATA-SEN - 01: The accuracy level for sensitive data must be > 95% of data points correct when compared to a trusted source.",
              "control_objective": "To ensure that sensitive data is reliable, precise, and fit for high-stakes decision-making.",
              "criteria_evidence": "A data quality report or test results from a validation script showing an accuracy score of > 95%. Documentation of the validation methodology, including the definition of the 'trusted source'. The validation script itself should be available for review."
            },
            {
              "criteria": "DATA-SEN - 02: Sensitive data must have All critical information present and all necessary data fields populated.",
              "control_objective": "To guarantee that all necessary information required for analysis and operations is present in the sensitive dataset.",
              "criteria_evidence": "A data schema or data dictionary defining all critical and necessary data fields. A data profiling report or log from an automated script that verifies completeness, showing zero null or empty values in the designated critical fields."
            },
            {
              "criteria": "DATA-SEN - 03: Sensitive data must have No contradictory information and maintain integrity across related datasets.",
              "criteria_objective": "To maintain the integrity and trustworthiness of sensitive data by eliminating logical contradictions across related datasets.",
              "criteria_evidence": "Documentation of integrity rules and constraints applied to the data. Test results from validation scripts or database constraints (e.g., unit tests, SQL queries) that check for contradictions, with logs showing zero violations found."
            },
            {
              "criteria": "DATA-SEN - 04: Sensitive data must be maintained in Real-time.",
              "criteria_objective": "To ensure that sensitive data is timely and current for its intended use, especially in contexts requiring immediate action or decision.",
              "criteria_evidence": "System logs or monitoring dashboard metrics (e.g., from Kafka, Grafana, or a data pipeline tool) showing timestamps of data ingestion and processing. A Service Level Agreement (SLA) document defining the maximum acceptable latency, with monitoring reports confirming compliance."
            },
            {
              "criteria": "DATA-SEN - 05: The sensitive data's provenance must be fully traced and verified.",
              "criteria_objective": "To enable full auditing and verification by maintaining a complete, unalterable record of the sensitive data's origin and history (provenance).",
              "criteria_evidence": "A data lineage graph or document that maps the data flow from its origin to its final state. Immutable logs (e.g., from a blockchain or write-once log system) that record all transformations, including timestamps and the identity of the process or user performing the change."
            },
            {
              "criteria": "DATA-SEN - 06: A robust version control system like Git or DVC must be applied to manage and track sensitive data versions.",
              "criteria_objective": "To ensure that changes to sensitive data are tracked, auditable, and reversible, protecting against unauthorized or erroneous modifications.",
              "criteria_evidence": "A link to the version control repository (e.g., Git, DVC). A review of the repository's commit history demonstrating consistent and meaningful commits for data changes. A README file or documentation outlining the branching and tagging strategy for data versions."
            }
          ],
          "PlanSteps": [
            {
              "step": "BBT-EXT-ACC-01: Establish a 'golden dataset' by manually and accurately extracting all critical data points from a representative sample of 10-20 source documents of varying types (PDF, DOCX, etc.).",
              "step_objective": "To create a verified, ground-truth benchmark against which the automated extraction process's accuracy can be quantitatively measured, directly testing for control DATA-SEN-01."
            },
            {
              "step": "BBT-EXT-ACC-02: Process the sample documents through the automated pipeline and programmatically compare the extracted text against the corresponding 'golden dataset', calculating the data point accuracy percentage.",
              "step_objective": "To validate that the extraction process meets the specified >95% accuracy threshold (DATA-SEN-01) and to identify specific failure modes (e.g., issues with tables, complex layouts)."
            },
            {
              "step": "BBT-EXT-COM-01: For a sample of documents, compare the total word/character count of the source file against the total count of the extracted and cleaned text. Flag any document with a deviation greater than a set threshold (e.g., 15%).",
              "step_objective": "To perform a high-level check for major data loss, such as missed pages or entire sections, ensuring the process meets the completeness requirement of DATA-SEN-02."
            },
            {
              "step": "BBT-EXT-COM-02: Define and test for 'non-splittable entities' (e.g., a person's full name and title, a complete address, a single table row). Verify that the chunking mechanism does not break these entities across multiple, separate chunks.",
              "step_objective": "To ensure the chunking process preserves the semantic integrity of the data, preventing the creation of incomplete or misleading fragments and upholding the completeness rule (DATA-SEN-02)."
            },
            {
              "step": "BBT-EXT-CON-01: Identify documents in the source data that contain known, intentional contradictions. Process these documents and verify that the extracted chunks accurately reflect the original contradictions without introducing new ones.",
              "step_objective": "To confirm that the extraction and chunking process does not introduce new errors or artifacts that could be misinterpreted as data contradictions, thereby testing the integrity preservation required by DATA-SEN-03."
            },
            {
              "step": "BBT-EXT-TIM-01: Introduce a new document or an updated version of an existing document into the source data collection. Measure the end-to-end processing time until its corresponding chunks are generated and ready for embedding.",
              "step_objective": "To empirically test the data pipeline's latency and validate that it meets the timeliness requirements defined in DATA-SEN-04 for maintaining a real-time system."
            },
            {
              "step": "BBT-EXT-PRO-01: From the output of the vectorization pipeline, select a random sample of 50 text chunks. For each chunk, verify the presence and correctness of its metadata, including source filename, document version, and page number.",
              "step_objective": "To ensure that every piece of processed data maintains a verifiable link to its origin, satisfying the full provenance tracing requirement of DATA-SEN-05."
            },
            {
              "step": "BBT-EXT-PRO-02: Using only the metadata from a sampled chunk, perform a reverse lookup to retrieve the original source document and confirm the chunk's content matches the content at the specified location (e.g., page 5, paragraph 2).",
              "step_objective": "To validate the functional reliability of the provenance metadata, ensuring it is not just present but accurate and useful for auditing and verification as per DATA-SEN-05."
            },
            {
              "step": "BBT-EXT-VER-01: Perform a test data rollback. Check out a previous version of a source document from the version control system (e.g., Git/DVC), run it through the pipeline, and verify that the output chunks exactly match the state of that older version.",
              "step_objective": "To confirm that the version control system is properly integrated and that the data processing pipeline can reliably reproduce outputs from any given historical version of the data, fulfilling control DATA-SEN-06."
            }
          ]
        }
      ]
    },
    {
      "StepName": "Indexing and storing company's proprietary data",
      "WebFormTitle": "To uphold the principles of data confidentiality, integrity, and availability for all information stored in the AI's knowledge base by implementing comprehensive encryption, strict access controls, and robust disaster recovery protocols.",
      "Objectives": [
        {
          "Objective": "1. Data Confidentiality (Encryption): This ensures that the company's proprietary information, which has been converted into an AI-readable numerical format (vectorized), is unreadable to unauthorized parties. Think of this as storing the AI's knowledge in a digital safe (encryption at rest) and using a secure, armored courier when moving it between systems (encryption in transit)."
        },
        {
          "Objective": "2. Data Integrity and Access Control: This establishes strict controls over who can add, modify, or delete information in the AI's knowledge base. It prevents both accidental corruption and malicious tampering (data poisoning) by ensuring only authorized personnel or automated processes can manage the data, with all actions logged for auditing."
        },
        {
          "Objective": "3. Availability and Resilience: This guarantees that the AI's knowledge base, now a critical business asset, is protected against loss. It involves implementing robust backup and disaster recovery plans, ensuring the system can be restored quickly and reliably in the event of a technical failure, cyber-attack, or other disruption."
        }
      ],
      "Fields": []
    },
    {
      "StepName": "Connecting Internal Data <-> the Processing Pipeline",
      "WebFormTitle": "To ensure the secure and confidential transfer of all proprietary data from internal sources into the data processing pipeline by enforcing strong encryption for all data in transit.",
      "Objectives": [
        {
          "Objective": "1. Offline Path (Knowledge Base Building): Systematically transfer and process large volumes of internal documents to build the core, persistent knowledge base. This involves extracting, chunking, and embedding data to create a comprehensive, searchable vector index of the company's proprietary information."
        },
        {
          "Objective": "2. Real-Time Path (Query Augmentation): Provide an on-the-fly processing capability for user-uploaded documents. Data is processed immediately and stored in a temporary, session-specific index, allowing the system to answer questions on new information without altering the permanent knowledge base."
        },
        {
          "Objective": "3. Data in Transit Security: Protect all data during transfer between internal sources and the processing pipeline. This involves mandating strong encryption (e.g., TLS) for both offline bulk transfers and real-time user uploads to prevent eavesdropping and data interception, ensuring confidentiality is maintained throughout the ingestion process."
        }
      ],
      "Fields": []
    }
  ],
  "Phase Development (Real-Time Chat With Your Document)": [
    {
      "StepName": "User Interface",
      "WebFormTitle": "To uphold system integrity at the point of user interaction by validating user identity, sanitizing all submitted queries to neutralize potential threats, and ensuring non-repudiation through detailed audit logs.",
      "Objectives": [
        {
          "Objective": "1. Identity and Access: Establishes that the system must know who is making the request, which is the foundation for access control."
        },
        {
          "Objective": "2. Data Integrity: Covers the need to inspect the incoming data itself for malicious content, such as prompt injection, code, or other attacks that could compromise the system."
        },
        {
          "Objective": "3. Audibility and Non-repudiation: Creating an immutable record that proves a specific user performed a specific action, and they cannot later deny it. This is critical for security investigations and accountability."
        }
      ],
      "Fields": [
        {
          "FieldType": "risk",
          "FieldName": "LLM01 Prompt Injection",
          "question": "Will external users or other systems provide text prompts or instructions to the AI model?",
          "controls": [
            {
              "control": "[LLM01][1] - All user inputs within the UI must be validated to prevent the injection of malicious code.",
              "control_objective": "Prevent attackers from exploiting vulnerabilities in the UI to inject malicious code and compromise the AI system.",
              "control_evidence": "Unit test results demonstrating the rejection of malicious payloads (e.g., XSS, command injection strings)."
            },
            {
              "control": "[LLM01][2] - Implement input sanitization techniques to remove harmful characters from user inputs.",
              "control_objective": "Further mitigate the risk of malicious code injection attempts through the UI.",
              "control_evidence": "Code snippets showing the use of a sanitization library or function. Test cases with logs that display the 'before' and 'after' state of user inputs containing harmful characters."
            },
            {
              "control": "[LLM01][4] - Encrypt all sensitive data transmitted through APIs.",
              "control_objective": "Protect sensitive data from unauthorised interception or tampering during communication through APIs.",
              "control_evidence": "Web server or API gateway configuration files enforcing TLS 1.2 or higher. A report from an external security scanner (e.g., Qualys SSL Labs) confirming a strong HTTPS implementation."
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "LLM02 Insecure Output Handling",
          "question": "Will the AI output be used in other systems, documents, or communications without being verified first?",
          "controls": [
            {
              "control": "[LLM02][1] - Ensure that sensitive or IP data is not exposed in AI system outputs.",
              "control_objective": "To mitigate the risk of insecure output handling by treating LLM-generated outputs as potentially untrusted.",
              "control_evidence": "Implementation of an output filtering or sanitization layer that scans the LLM's response for sensitive data patterns (e.g., using regex or a DLP service) before it is sent to the user or downstream systems. Test results demonstrating that the filter successfully redacts or blocks known sensitive information or intellectual property."
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "LLM04 Model Denial of Service",
          "question": "Could this AI system be overwhelmed or misused in a way that impacts availability?",
          "controls": [
            {
              "control": "[LLM04][3] - Enforce API rate limits to restrict the number of requests an individual user or IP address can make within a specific timeframe.",
              "control_objective": "To control the rate of requests and prevent overwhelming the LLM with a high volume of concurrent requests.",
              "control_evidence": "Screenshots of the API gateway configuration, relevant code snippets defining the rate limits, or test results showing that requests are blocked after the limit is exceeded."
            },
            {
              "control": "[LLM04][4] - Limit the number of queued actions and the number of total actions in a system reacting to LLM responses.",
              "control_objective": "To prevent the accumulation of excessive workload and ensure that the system can effectively process LLM responses without becoming overwhelmed.",
              "control_evidence": "Configuration files from the task queue system (e.g., Celery, RabbitMQ), application code setting queue size or concurrency limits, or architectural diagrams illustrating these constraints."
            },
            {
              "control": "[LLM04][5] - Continuously monitor the resource utilisation of the LLM to identify abnormal spikes or patterns that may indicate a DoS attack.",
              "control_objective": "To detect and respond to anomalous resource usage patterns indicative of a denial of service attack on the LLM.",
              "control_evidence": "Screenshots from the monitoring dashboard (e.g., Grafana, Datadog) showing resource utilisation graphs, copies of the alert configurations for abnormal spikes, and the incident response procedure for such alerts."
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "LLM05: Supply Chain Vulnerabilities",
          "question": "Does the AI system depend on third-party libraries, tools, or models you haven’t vetted?",
          "controls": [
            {
              "control": "[LLM05][2] - Only use reputable plugins that have been tested for application requirements.",
              "control_objective": "Minimise plugin-related vulnerabilities.",
              "control_evidence": "A documented plugin vetting process, test results from plugin security assessments, and a list of approved plugins."
            },
            {
              "control": "[LLM05][4] - Maintain an up-to-date inventory using a Software Bill of Materials (SBOM).",
              "control_objective": "Track and manage components.",
              "control_evidence": "The current SBOM document for the application, evidence of a process for regularly updating the SBOM, and change logs."
            },
            {
              "control": "[LLM05][7] - Implement sufficient monitoring and a robust patching policy.",
              "control_objective": "Maintain system security and component currency.",
              "control_evidence": "The documented patching policy, vulnerability scan reports, and change management records demonstrating timely patch application."
            }
          ]
        }
      ]
    },
    {
      "StepName": "RAG Orchestrator",
      "WebFormTitle": "To ensure the secure and appropriate handling of user queries and retrieved data by enforcing strict access controls, mitigating complex inference-based attacks, and maintaining a detailed audit trail for accountability and non-repudiation.",
      "Objectives": [
        {
          "Objective": "1. Secure Workflow Management: The orchestrator acts as the central logic unit, managing the entire RAG process from receiving a user's query to retrieving data and generating a final, context-aware response."
        },
        {
          "Objective": "2. Access Control Enforcement: It must rigorously check a user's permissions *before* retrieving data from the knowledge base. This is the most critical function to prevent access control bypasses, where a user could otherwise access confidential information beyond their privilege level."
        },
        {
          "Objective": "3. Inference Attack Mitigation: The system must be designed to detect and counter inference attacks, where a user attempts to piece together sensitive information by asking numerous simple, seemingly harmless questions. This involves monitoring query patterns and applying appropriate safeguards."
        },
        {
          "Objective": "4. Audit and Accountability: To create an immutable record of all interactions, including user queries, data access events, and generated responses. This establishes a comprehensive audit trail critical for security monitoring, forensic analysis, and ensuring non-repudiation."
        }
      ],
      "Fields": [
        {
          "FieldType": "risk",
          "FieldName": "Insufficient Audit Logging and Traceability",
          "question": "Do you need to maintain a detailed, tamper-proof record of the AI system's operations to meet regulatory requirements and investigate incidents?",
          "controls": [
            {
              "control": "[AUL][1] - Implement capabilities in the RAG Orchestrator to enable for the automatic recording of events (logs) while the high-risk AI system is operating.",
              "control_objective": "To ensure a level of traceability of the AI system’s functioning throughout its lifecycle that is appropriate to its intended purpose.",
              "control_evidence": "Code review or configuration file for the **RAG Orchestrator** showing the logging service is automatically initialized on startup. A sample of raw event logs from the orchestrator capturing its primary operations (e.g., Step 1: 'Request', Step 4: 'Retrieve', Step 6: 'Generate', Step 7: 'Response'). SIEM dashboard showing these logs being successfully ingested in real-time from the **RAG Orchestrator**."
            },
            {
              "control": "[[AUL][2] - Implement capabilities in the RAG Orchestrator to ensure logging capabilities record the period of each use, the reference database, the input data, and the identity of the persons involved in verifying the results.",
              "control_objective": "To provide detailed operational transparency and accountability for AI systems used in critical public and justice-related applications.",
              "control_evidence": "A log schema definition document for transaction logs generated by the **RAG Orchestrator**. The schema must include fields for: 'period_of_use', 'reference_database' (e.g., 'Permanent Index' or 'Temporary In-Memory'), 'input_data' (the 'PDF + QUESTION' from Step 1), and 'identity_of_person_verifying' (a user ID from the **User Interface**). A log query tracing a single user session that populates all these fields, including a subsequent verification event from the **User Interface**."
            },
            {
              "control": "[AUL][3] - Implement capabilities in the RAG Orchestrator to ensure logging capabilities record the period of use, reference database, input data, and the identification of the person generating the match.",
              "control_objective": "To enhance auditability and accountability in the use of sensitive remote biometric identification technologies.",
              "control_evidence": "An audit log export of a complete \"Chat With Your Document\" session. The log entry, generated by the **RAG Orchestrator**, must include: 'period_of_use', 'reference_database', 'input_data', and 'identification_of_person' (the authenticated user ID from the **User Interface** who initiated the request). Access control policy demonstrating that only authorized users can send requests to the **RAG Orchestrator**."
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "[RAG-ORCH-01] - Failure to Enforce Permitted User Prompt Categories",
          "RiskDescription": "The risk that the RAG Orchestrator processes user prompts that do not align with the pre-defined, permitted categories (e.g., Q&A, Summarization). This can lead the orchestrator to misinterpret user intent, retrieve irrelevant documents, and generate unreliable or non-compliant responses, fundamentally undermining the system's intended use.",
          "question": "Does the RAG Orchestrator have a specific, built-in mechanism to classify incoming user prompts against the permitted categories and reject those that do not match?",
          "controls": [
            {
              "control": "[RAG-PC-01] - Implement a Prompt Classification Module: The RAG Orchestrator must include a pre-processing module that classifies the intent of every incoming user prompt into one of the pre-defined, permitted categories. Prompts that cannot be classified or are classified as out-of-scope must be rejected before the retrieval step.",
              "control_objective": "To create a robust, intelligent gatekeeper within the RAG Orchestrator that ensures only user prompts matching the system's approved functionalities are ever processed.",
              "control_evidence": "A code snippet or documentation of the classification logic (e.g., using a smaller classification model, keyword heuristics, or an LLM call); a copy of the configuration linking this module to the 'Permitted User Prompt Categories' defined in the initial procedure step; and system logs showing both successful classifications and rejections of out-of-category prompts."
            },
            {
              "control": "[RAG-PC-02] - Configure Category-Specific System Prompts: The RAG Orchestrator's prompt templating engine must be capable of dynamically adjusting the system prompt based on the classified category, reinforcing the specific task to the LLM.",
              "control_objective": "To improve the accuracy and relevance of the LLM's response by providing it with context-specific instructions tailored to the user's classified intent (e.g., 'You are a helpful assistant. Summarize the following document...').",
              "control_evidence": "A copy of the RAG Orchestrator's configuration file showing the different prompt templates for each category; and logs from the orchestrator demonstrating that the correct system prompt was selected based on a classified user query."
            },
            {
              "control": "[RAG-PC-04] - Implement Prohibited Terms Monitoring: The RAG Orchestrator must include a monitoring component that inspects user queries for keywords from a configurable blacklist of prohibited terms (e.g., 'performance review,' 'salary,' 'disciplinary action'). Matches must be logged with user and session details for later review.",
              "control_objective": "To create a critical audit trail for attempted misuse of the system, providing the necessary data for user retraining, incident response, and continuous improvement of the prompt classification module.",
              "control_evidence": "A copy of the configuration file defining the blacklist of prohibited terms; and a sample of the monitoring logs or a screenshot from the monitoring dashboard showing a user query that was successfully flagged for containing a prohibited term."
            },
            {
              "control": "[RAG-PC-03] - Provide Category-Aware UI Directives via API: The RAG Orchestrator's API must include fields in its response that inform the User Interface of the classified prompt category, enabling the UI to provide dynamic, context-aware guidance.",
              "control_objective": "To create a feedback loop where the orchestrator's classification actively guides the user, reinforcing the system's capabilities and improving the user experience.",
              "control_evidence": "A copy of the RAG Orchestrator's API documentation showing the fields for the classified category; and a sample JSON response from the API containing a classified category (e.g., 'category': 'Document Summarization')."
            },
            {
              "control": "[RAG-DM-01] - Detect and Flag Decision-Making Language in LLM Responses: The RAG Orchestrator must include a monitoring module that inspects the LLM's generated output for key phrases or language indicating business, health, safety, or rights-impacting decisions. Responses containing such language must be flagged for immediate human intervention and allow the overseer to stop or override the system.",
              "control_objective": "To fulfill human oversight requirements by identifying when the AI is making or suggesting decisions with significant impact, enabling prompt supervisory intervention as mandated by regulatory standards.",
              "control_evidence": "A documented list of monitored key phrases for decision-making language; logs or alerts showing flagged LLM responses containing decision language; and evidence of the mechanism that triggers the human intervention workflow, including override and stop controls."
            }
          ]
        }
      ]
    },
    {
      "StepName": "Generic LLM",
      "WebFormTitle": "To enforce strict operational security for the self-hosted LLM by isolating its network access and implementing governed MLOps deployment workflows.",
      "Objectives": [
        {
          "Objective": "1. Resource Isolation: To contain the LLM's operational environment by strictly controlling and restricting its access to all network resources, internal services, and APIs, preventing it from performing unauthorized actions."
        },
        {
          "Objective": "2. Governed Deployment: To ensure model integrity and security by implementing automated MLOps deployment pipelines that include robust governance, tracking, and formal approval workflows for all changes."
        }
      ],
      "Fields": []
    },
    {
      "StepName": "User Interface <->  RAG Orchestrator",
      "WebFormTitle": "To ensure that all queries and responses between the user interface and the RAG orchestrator are subject to strict, user-specific access control enforcement, preventing unauthorized data disclosure.",
      "Objectives": [
        {
          "Objective": "1. Enforce Access Control: The primary vulnerability is the failure to apply a user's specific permissions. This objective is to validate that the orchestrator, despite its high-level privileges, strictly filters all knowledge base queries and results according to the individual user's access rights."
        },
        {
          "Objective": "2. Prevent Data Leakage: Directly addresses the risk of a major data breach by ensuring that sensitive documents retrieved by the orchestrator are never passed back to an unauthorized user through the communication channel."
        },
        {
          "Objective": "3. Secure Communication Channel: To protect the integrity and confidentiality of the raw question and the final answer as they are transmitted between the user interface and the orchestrator, preventing interception or tampering."
        }
      ],
      "Fields": []
    },
    {
      "StepName": "RAG Orchestrator <->  Vector Database (Retrieval)",
      "WebFormTitle": "To retrieve relevant text chunks from the knowledge base in response to a user query while mitigating the risk of unintentionally over-fetching and exposing sensitive data.",
      "Objectives": [
        {
          "Objective": "1. Contextual Retrieval: To query the vector database with the user's vectorized input to find and return the most semantically relevant information needed to generate an accurate answer."
        },
        {
          "Objective": "2. Mitigate Data Over-fetching: To address the primary risk of retrieving document chunks that contain the answer but also include collateral sensitive data, such as Personally Identifiable Information (PII)."
        },
        {
          "Objective": "3. Prevent Sensitive Data Exposure: To ensure that unintentionally retrieved confidential details are not passed on to the LLM, thus preventing their potential exposure in the final generated answer."
        }
      ],
      "Fields": []
    },
    {
      "StepName": "RAG Orchestrator <-> Generic LLM (Augmented Prompt & LLM Call)",
      "WebFormTitle": "To generate a coherent, fact-based answer by sending a context-rich prompt to the LLM while mitigating risks of prompt injection and sensitive data leakage.",
      "Objectives": [
        {
          "Objective": "1. Prompt Augmentation: To combine the user's original question with the retrieved text chunks to create a single, comprehensive 'augmented prompt' for the LLM."
        },
        {
          "Objective": "2. Mitigate Prompt Injection: To defend against malicious instructions hidden within the user's query or retrieved data that could hijack the LLM and cause it to perform unintended actions."
        },
        {
          "Objective": "3. Prevent Data Leakage: To ensure the LLM does not inadvertently include sensitive PII or other confidential information from the context in its final answer, preventing the exposure of private data."
        }
      ],
      "Fields": []
    },
    {
      "StepName": "(A.4.7) AI Lifecycle Phase requirements - Generation",
      "WebFormTitle": "To generate a coherent, fact-based answer by sending an augmented prompt to the LLM, while safeguarding against prompt injection and preventing the leakage of sensitive data.",
      "Objectives": [
        {
          "Objective": "1. Augmented Prompt Construction: To combine the user's question with the retrieved text chunks into a single, comprehensive 'augmented prompt' and send it to the LLM for final answer generation."
        },
        {
          "Objective": "2. Mitigate Prompt Injection: To defend against vulnerabilities where a malicious instruction hidden in the user's query or retrieved data could hijack the LLM, causing it to perform unintended actions."
        },
        {
          "Objective": "3. Prevent Data Leakage: To ensure the LLM does not include sensitive PII or confidential notes from the provided context in its final answer, thus preventing inadvertent exposure of private information."
        }
      ],
      "Fields": []
    }
  ],
  "Phase Deployment": [
    {
      "StepName": "x(A.6.2.5) AI Lifecycle Phase requirements - Deployment",
      "WebFormTitle": "To orchestrate a secure and compliant AI system deployment by ensuring component integrity through secure packaging, formalizing a deployment plan, and verifying all prerequisite conditions are met prior to system activation.",
      "Objectives": [
        {
          "Objective": "1. Deployment Planning: To formalize the deployment strategy by documenting the complete process, including timelines, resource allocation, technical steps, and rollback procedures, ensuring a predictable and controlled rollout."
        },
        {
          "Objective": "2. Prerequisite Verification: To confirm that the AI system meets all established technical, security, ethical, and performance benchmarks through rigorous pre-deployment testing and validation, preventing the release of a non-compliant or unstable system."
        },
        {
          "Objective": "3. Secure Packaging: To guarantee the integrity and security of the deployable artifacts by ensuring that all components, particularly container images, are built from trusted sources, scanned for vulnerabilities, and hardened before deployment."
        }
      ],
      "Fields": [
        {
          "FieldType": "risk",
          "FieldName": "Insecure AI component Packaging",
          "question": "Are any components of the AI system be packaged in containers?",
          "controls": [
            {
              "control": "PROTE.02 Configure development tools, orchestrators, and container runtimes to exclusively use encrypted channels when connecting to registries.",
              "control_objective": "To safeguard the integrity and confidentiality of container images and code during transit to and from registries.",
              "control_evidence": "Configuration files for development tools, orchestrators (e.g., Kubernetes), and container runtimes demonstrating the use of TLS-encrypted connections (e.g., registry URLs starting with 'https://')."
            },
            {
              "control": "PROTE.03 Implement time-triggered pruning of registries to remove unsafe or vulnerable container images.",
              "control_objective": "To maintain the security and integrity of container images in registries by eliminating outdated and vulnerable images.",
              "control_evidence": "Configuration of the automated pruning job (e.g., a CronJob manifest) and execution logs showing that vulnerable or old images have been successfully removed."
            },
            {
              "control": "PROTE.04 Enforce read/write access control for registries containing proprietary or sensitive container images.",
              "control_objective": "To restrict unauthorised access and modifications to container images stored in registries.",
              "control_evidence": "Screenshots or configuration exports of the registry's Role-Based Access Control (RBAC) settings, showing defined user roles and their permissions for specific repositories."
            },
            {
              "control": "PROTE.05 Control access to cluster-wide administrative accounts using strong authentication methods like multifactor authentication and single sign-on to existing directory systems where applicable.",
              "control_objective": "To ensure secure and controlled access to administrative accounts within the cluster.",
              "control_evidence": "Identity Provider (IdP) configuration showing MFA is enforced for the cluster administrator group, and the orchestrator's authentication configuration file pointing to the SSO provider (e.g., OIDC or SAML settings)."
            },
            {
              "control": "PROTE.06 Implement network isolation protocols that configure orchestrators to segregate network traffic based on sensitivity levels.",
              "control_objective": "To maintain distinct network environments for different levels of data sensitivity, enhancing overall network security.",
              "control_evidence": "Copies of network policy manifests (e.g., Kubernetes 'NetworkPolicy' YAML files) or firewall rules that define and enforce network segmentation."
            },
            {
              "control": "PROTE.07 Deploy policies that configure orchestrators to isolate deployments to specific sets of hosts based on security requirements or sensitivity levels.",
              "control_objective": "To ensure that deployments are conducted on secure, appropriate hosts in alignment with their security needs.",
              "control_evidence": "Orchestrator deployment configurations (e.g., YAML files) showing the use of node selectors, taints, and tolerations to restrict pods to specific nodes."
            },
            {
              "control": "PROTE.12 Implement mechanisms to reduce Host Operating System (OS) attack surfaces, including\na) using container-specific OSs with unnecessary services disabled (e.g., print spooler)\nb) employing read-only file systems\nc) regularly updating and patching OSs and lower-level components like the kernel\nd) validating versioning of components for base OS management and functionality.",
              "control_objective": "To minimise vulnerabilities and enhance the security of the host operating systems used in containerised environments.",
              "control_evidence": "Patch management reports, host configuration files showing a minimal OS install (e.g., CIS hardened image), disabled services, and read-only file system settings. A Software Bill of Materials (SBOM) for the host OS."
            },
            {
              "control": "PROTE.13 Establish mechanisms to prevent the mixing of containerised and non-containerised workloads on the same host instance.",
              "control_objective": "To segregate containerised workloads from non-containerised ones, reducing the risk of cross-contamination and attacks.",
              "control_evidence": "Host inventory documentation or orchestrator node labels and taints that dedicate specific hosts exclusively to containerised workloads."
            },
            {
              "control": "PROTE.14 Implement mechanisms to enforce minimal file system permissions for all containers, ensuring that they cannot mount sensitive directories on the host's file system.",
              "control_objective": "To restrict container access to the host's file system, preventing unauthorised access or manipulation of sensitive data.",
              "control_evidence": "Pod security policies or admission controller configurations that enforce restrictions on hostPath volumes. Deployment manifests showing the container 'securityContext' is configured with minimal permissions."
            },
            {
              "control": "PROTE.16 Ensure that only images from trusted image stores and registries are permitted to run in the environment.",
              "control_objective": "To safeguard the environment from untrusted or potentially harmful container images.",
              "control_evidence": "Configuration of an admission controller (e.g., OPA Gatekeeper, Kyverno) that implements a policy to only allow images from an approved list of registries."
            },
            {
              "control": "PROTE.17 Utilise network policies and firewall rules to restrict container network access and isolate sensitive workloads.",
              "control_objective": "To enhance network security by controlling container access and isolating sensitive workloads.",
              "control_evidence": "Network policy manifests (e.g., Kubernetes 'NetworkPolicy') or service mesh configurations (e.g., Istio 'AuthorizationPolicy') that define granular ingress and egress rules for pods."
            },
            {
              "control": "PROTE.18 Adopt the use of immutable containers, which cannot be altered post-deployment, wherever feasible.",
              "control_objective": "To prevent runtime attacks by ensuring container configurations remain unchanged after deployment.",
              "control_evidence": "Deployment manifests showing the container's root file system is set to read-only ('readOnlyRootFilesystem: true'). CI/CD pipeline configuration demonstrating that changes are deployed by building and shipping a new image."
            },
            {
              "control": "PROTE.19 Implement security measures for APIs, including robust API authentication mechanisms (e.g., OAuth 2.0, API keys), fine-grained access controls, and rate limiting to protect against abuse.",
              "control_objective": "To ensure the secure operation of APIs",
              "control_evidence": "API gateway configuration files or screenshots demonstrating the enforcement of authentication, authorisation (e.g., access control lists), and rate-limiting policies."
            },
            {
              "control": "PROTE.20 Images should be configured to run as non-privileged users.",
              "control_objective": "To enhance security by minimising the potential impact of a security breach from a containerised environment.",
              "control_evidence": "The 'Dockerfile' showing the 'USER' instruction is used. The deployment manifest showing the 'securityContext' specifies 'runAsNonRoot: true' and a non-zero 'runAsUser' ID."
            },
            {
              "control": "PROTE.21 Secrets should be stored outside of images and provided dynamically at runtime as needed.",
              "control_objective": "To protect sensitive information like credentials and keys by managing them securely and separately from container images.",
              "control_evidence": "Review of the 'Dockerfile' to confirm no secrets are present. Orchestrator manifests showing that secrets are mounted from a secure source (e.g., Kubernetes Secrets, HashiCorp Vault) at runtime."
            },
            {
              "control": "PROTE.22 Implement security policies and access controls at both the container and host levels to restrict unauthorised access and privilege escalation.",
              "control_objective": "To enhance container and host security by limiting access and preventing unauthorised privilege escalation.",
              "control_evidence": "Host-level AppArmor or SELinux profiles. Container-level pod security standards or custom admission controller policies that restrict privileged operations."
            },
            {
              "control": "PROTE.23 Utilise built-in security features of your containerisation platform.",
              "control_objective": "To leverage platform-specific security features to enhance the security posture of containerised applications.",
              "control_evidence": "A document or report detailing the enabled platform-specific security features, such as Kubernetes Pod Security Standards, Security Contexts, and RBAC configurations."
            },
            {
              "control": "PROTE.24 Mechanisms exist to implement resource limitations to prevent containers from consuming excessive resources and potentially causing a Denial of Service (DoS) attack.",
              "control_objective": "To prevent containers from over-utilising system resources, thereby safeguarding against resource exhaustion and DoS attacks.",
              "control_evidence": "Deployment manifests (e.g., Kubernetes pod spec) showing that CPU and memory requests and limits are defined for all containers."
            }
          ]
        }
      ]
    },
    {
      "StepName": "(A.8.4) Communication of incidents",
      "Objectives": [
        {
          "Objective": "Define and document the information, reporting, and incident communication plan for all internal and external interested parties."
        }
      ],
      "Fields": [
        {
          "FieldName": "(A.8.4) - Incident Communication Plan - Data Breach",
          "FieldLabel": "Data Breach",
          "FieldText": "Describe how incidents related to \"Unintended exposure of training data\" will be comunicated.",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "(A.8.4) - Incident Communication Plan - Model Misuse",
          "FieldLabel": "Model Misuse",
          "FieldText": "Describe how incidents related to \"AI model used outside intended scope\" will be comunicated.",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "(A.8.4) - Incident Communication Plan - Model Failure",
          "FieldLabel": "Model Failure",
          "FieldText": "Describe how incidents related to \"False predictions causing harm\" will be comunicated.",
          "FieldType": "TextBox"
        }
      ]
    },
    {
      "StepName": "(A.8.2) AI System Documentation and User Information",
      "WebFormTitle": "To provide all users and stakeholders with clear, comprehensive, and accessible information required for the safe, effective, and responsible use of the AI system.",
      "Objectives": [
        {
          "Objective": "1. Clarity and Accessibility: To ensure all documentation is written in plain language, is well-structured, and is tailored to the technical level of the intended audience, from end-users to technical administrators."
        },
        {
          "Objective": "2. Comprehensive Operational Guidance: To provide clear, step-by-step instructions for all system functionalities, including standard operating procedures and guidance for handling common errors or unexpected outputs."
        },
        {
          "Objective": "3. Transparency of Capabilities and Limitations: To explicitly document the AI system's intended use, its known limitations, and potential risks. This includes describing what the system is not designed to do to prevent misuse."
        },
        {
          "Objective": "4. Communication of User Responsibilities: To clearly communicate the 'Objectives for responsible use' to all users, ensuring they understand their role and accountability in operating the system ethically and in accordance with its intended purpose."
        }
      ],
      "Fields": []
    }
  ],
  "Phase Post deployment verification": [
    {
      "StepName": "(A.6.2.4) AI Systems verifications",
      "Objectives": [
        {
          "Objective": "1. Overall System Validation: To execute a formal suite of verification and validation tests that ensure the AI system meets all pre-defined technical, ethical, and operational requirements before deployment."
        },
        {
          "Objective": "2. Performance and Stability Verification: To validate that the AI system is robust, stable, and scalable under realistic load conditions by executing the 'Performance & Load Test Plan'."
        },
        {
          "Objective": "3. Responsible AI Compliance: To verify that the system operates ethically by confirming adherence to fairness, bias, transparency, and explainability standards through the 'Fairness and Bias' and 'Transparency and Explainability' test plans."
        },
        {
          "Objective": "4. Operational Integrity and Safety: To ensure the system can be used safely and correctly by validating its technical guardrails against misuse through the 'Purpose Limitation and Prompt Category Enforcement Test Plan'."
        }
      ],
      "Fields": [
        {
          "FieldType": "plan",
          "FieldName": "(A.12.1.2, A.14.2.5) - Performance & Load Test Plan",
          "PlanObjective": "To validate that the AI system meets its defined non-functional requirements for performance, stability, and scalability under realistic load conditions.",
          "PlanCriteria": [
            {
              "criteria": "PERF-TEST-01: The system's average API response time must remain below [e.g., 200ms] and the 95th percentile response time below [e.g., 500ms] during peak load simulation.",
              "control_objective": "To ensure a responsive user experience during high-traffic periods.",
              "criteria_evidence": "The final load test report containing time-series graphs of P50 and P95 latency metrics, demonstrating compliance with the defined Non-Functional Requirements (NFRs)."
            },
            {
              "criteria": "PERF-TEST-02: The system's server-side error rate must not exceed [e.g., 0.1%] of total requests during the peak load test.",
              "control_objective": "To confirm system stability and prevent service degradation under stress.",
              "criteria_evidence": "A summary table in the load test report showing the total number of requests, the number of failed requests, and a calculated error rate below the defined threshold."
            },
            {
              "criteria": "PERF-TEST-03: CPU and memory utilization for all system components must remain below a sustainable threshold (e.g., 80% average) without memory leaks throughout the duration of the load test.",
              "control_objective": "To ensure the system has adequate resource capacity and operates efficiently without risk of crashing due to resource exhaustion.",
              "criteria_evidence": "Monitoring dashboard screenshots (e.g., from Grafana, Datadog) or exported metrics showing CPU and memory usage over the test duration, confirming they stay within the acceptable range."
            }
          ],
          "PlanSteps": [
            {
              "step": "PLT-EXEC-01: Define and document specific Non-Functional Requirements (NFRs) for performance, including target response times (P50, P95), maximum acceptable error rate, and resource utilization thresholds.",
              "step_objective": "To establish clear, quantitative success criteria for the load tests, directly informing the benchmarks needed to pass criteria PERF-TEST-01, 02, and 03."
            },
            {
              "step": "PLT-EXEC-02: Develop automated test scripts using a load testing tool (e.g., JMeter, k6, Gatling) to simulate realistic user workflows and data processing volumes, creating profiles for baseline, peak, and stress load.",
              "step_objective": "To create a repeatable and accurate simulation of real-world user traffic, ensuring the test conditions are valid for assessing system performance."
            },
            {
              "step": "PLT-EXEC-03: Provision a dedicated, production-like test environment and configure monitoring tools to capture key performance indicators (KPIs) including latency, error rates, CPU, and memory usage for all services.",
              "step_objective": "To ensure that test results are accurate and representative of production performance, and that all necessary data is collected for analysis."
            },
            {
              "step": "PLT-EXEC-04: Execute the load test profiles against the test environment, starting with a baseline and gradually ramping up to peak and stress levels, while actively monitoring system health.",
              "step_objective": "To systematically apply stress to the system in a controlled manner to gather performance data across different load scenarios."
            },
            {
              "step": "PLT-EXEC-05: Aggregate and analyze the collected metrics, comparing the results against the predefined NFRs. Identify any performance bottlenecks or components that fail to meet the criteria.",
              "step_objective": "To quantitatively evaluate the system's performance against the success criteria (PERF-TEST-01, 02, 03) and pinpoint areas for optimization."
            },
            {
              "step": "PLT-EXEC-06: Compile a comprehensive load testing report summarizing the methodology, NFRs, test results (including graphs and tables), and a final conclusion on whether the system passed the performance benchmarks.",
              "step_objective": "To produce the final piece of evidence required by all criteria, providing a clear record of the test outcomes for stakeholder review and approval."
            }
          ]
        },
        {
          "FieldType": "plan",
          "FieldName": "(A.7.4) Fairness and Bias in AI Systems Test Plan",
          "PlanObjective": "Ensure AI systems are designed, developed, and tested with defined fairness objectives to prevent discriminatory or inequitable outcomes.",
          "PlanCriteria": [
            {
              "criteria": "FAIR-CTX - 01: The context for fairness analysis, including the system's purpose, favorable outcomes, and protected attributes, must be clearly defined and documented.",
              "control_objective": "To establish a clear and unambiguous framework for all subsequent fairness assessments, ensuring that tests are relevant to the system's specific use case.",
              "criteria_evidence": "A signed-off document specifying: 1. The AI system's intended purpose. 2. The definition of the 'favorable outcome' variable. 3. A comprehensive list of protected attributes relevant to the legal jurisdiction (e.g., GDPR in Luxembourg)."
            },
            {
              "criteria": "FAIR-REP - 01: The dataset's demographic composition must be representative of the target real-world population.",
              "control_objective": "To identify and mitigate representation bias, ensuring that the dataset does not systematically under- or over-represent certain demographic groups.",
              "criteria_evidence": "A data profiling report that includes: 1. Visualizations (e.g., bar charts) of the distribution of all subgroups for each protected attribute. 2. A comparative analysis of these distributions against external population benchmarks (e.g., census data, applicant pool statistics)."
            },
            {
              "criteria": "FAIR-HIST - 01: The dataset must not exhibit significant statistical disparities in historical outcomes between protected groups.",
              "control_objective": "To prevent the AI model from learning and amplifying past discriminatory patterns present in the training data.",
              "criteria_evidence": "A historical bias analysis report demonstrating: 1. Calculation of favorable outcome rates for all subgroups. 2. Application of a statistical disparity test (e.g., the Four-Fifths Rule), with results showing that no subgroup's outcome rate is less than 80% of the most favorable group's rate."
            },
            {
              "criteria": "FAIR-PRX - 01: Features that could act as proxies for protected attributes must be identified, analyzed, and documented.",
              "control_objective": "To prevent indirect discrimination by ensuring the model does not rely on non-protected features that are highly correlated with sensitive attributes.",
              "criteria_evidence": "A correlation analysis report (e.g., correlation matrix, feature importance analysis) that identifies features highly correlated with protected attributes. A documented decision for the inclusion or exclusion of each identified proxy."
            },
            {
              "criteria": "FAIR-GOV - 01: A formal governance process must be in place to report on and mitigate identified fairness risks.",
              "control_objective": "To ensure accountability and a structured, auditable response to any fairness issues discovered in the dataset before model development.",
              "criteria_evidence": "1. A formal 'Dataset Bias Report' summarizing all findings from the context, representation, historical, and proxy analyses. 2. A documented and approved mitigation plan outlining the actions to be taken (e.g., re-sampling, re-weighting, risk acceptance)."
            }
          ],
          "PlanSteps": [
            {
              "step": "FAIR-CTX-TEST-01: Document the intended purpose and operational context of the dataset (e.g., screening job applicants, predicting loan defaults).",
              "step_objective": "To establish the specific fairness context, which dictates the relevance of different biases and informs the selection of appropriate fairness metrics, as required by FAIR-CTX-01."
            },
            {
              "step": "FAIR-CTX-TEST-02: Identify and document the 'favorable outcome' variable and its values within the dataset (e.g., loan_approved = 1, hired = TRUE).",
              "step_objective": "To provide a clear, measurable target for performing historical bias analysis and calculating outcome rates across groups, fulfilling a key part of criterion FAIR-CTX-01."
            },
            {
              "step": "FAIR-CTX-TEST-03: List the protected attributes relevant to the system's context and jurisdiction (e.g., Gender, Age, Ethnicity, Disability per GDPR in Luxembourg).",
              "step_objective": "To define the specific demographic and sensitive groups against which all subsequent fairness and bias tests will be conducted, completing the requirements for FAIR-CTX-01."
            },
            {
              "step": "FAIR-REP-TEST-01: For each protected attribute, calculate and visualize the distribution of individuals across all subgroups (e.g., bar charts showing counts for each gender).",
              "step_objective": "To quantify the demographic composition of the dataset and identify any underrepresentation, directly testing for criterion FAIR-REP-01."
            },
            {
              "step": "FAIR-REP-TEST-02: Compare the dataset's demographic proportions against relevant real-world population benchmarks (e.g., national census data, applicant pool statistics).",
              "step_objective": "To formally assess for representation bias by determining if the dataset is a skewed or accurate sample of the target population, providing the core evidence for FAIR-REP-01."
            },
            {
              "step": "FAIR-HIST-TEST-01: Calculate and compare the rate of the 'favorable outcome' for each subgroup within a protected attribute (e.g., calculate the loan approval rate for males vs. females).",
              "step_objective": "To measure for historical bias by identifying disparities in past outcomes, which is the first step in validating criterion FAIR-HIST-01."
            },
            {
              "step": "FAIR-HIST-TEST-02: Apply a quantitative disparity threshold, such as the 'Four-Fifths (80%) Rule', to determine if the observed differences in outcome rates are statistically significant.",
              "step_objective": "To provide a concrete, defensible method for confirming that the dataset meets the disparity requirements of criterion FAIR-HIST-01."
            },
            {
              "step": "FAIR-PRX-TEST-01: Conduct a correlation analysis to identify non-protected features that are strong predictors of protected attributes (e.g., check correlation between zip code and ethnicity).",
              "step_objective": "To uncover hidden sources of bias by identifying proxy variables, thereby generating the required analysis report for criterion FAIR-PRX-01."
            },
            {
              "step": "FAIR-GOV-TEST-01: Compile all findings into a formal 'Dataset Bias Report' detailing representation biases, historical biases, and identified proxies.",
              "step_objective": "To create a comprehensive, auditable record of the fairness assessment, producing the primary piece of evidence required by FAIR-GOV-01."
            },
            {
              "step": "FAIR-GOV-TEST-02: Based on the report, select and document a clear mitigation strategy (e.g., collect more data, apply re-sampling/re-weighting, remove proxies, or formally accept the risk).",
              "step_objective": "To ensure that identified biases are not ignored and that a deliberate, documented action plan is put in place, fulfilling the mitigation plan requirement of FAIR-GOV-01."
            }
          ]
        },
        {
          "FieldType": "plan",
          "FieldName": "(A.7.4) Transparency and Explainability in AI Systems Test Plan",
          "PlanObjective": "Ensure AI systems are designed, developed and tested to provide understandable and sufficient information about its decisions to affected individuals.",
          "PlanCriteria": [
            {
              "criteria": "TRN-DEF - 01: Explainability requirements, including audiences, their specific needs, and legal obligations, must be formally defined and documented.",
              "control_objective": "To establish a clear, stakeholder-aligned foundation for the design and implementation of the explanation system, ensuring it is fit-for-purpose and compliant.",
              "criteria_evidence": "A signed-off 'Explainability Requirements Document' that specifies: 1. A list of all identified audiences (e.g., end-users, developers). 2. The core questions each audience needs answered. 3. A review of applicable legal obligations (e.g., GDPR)."
            },
            {
              "criteria": "TRN-TECH - 01: Appropriate explanation techniques must be selected and implemented to cover both global model behavior and local, individual predictions.",
              "control_objective": "To ensure the system has the technical capability to generate explanations that address the full spectrum of stakeholder needs, from high-level validation to individual case analysis.",
              "criteria_evidence": "System architecture diagrams and documentation that specify: 1. The rationale for model selection (interpretable vs. complex). 2. The chosen global explanation technique (e.g., Feature Importance). 3. The chosen local explanation technique (e.g., SHAP, LIME)."
            },
            {
              "criteria": "TRN-VAL - 01: The implemented explanation system must be technically robust, plausible, and stable.",
              "control_objective": "To ensure that the generated explanations are reliable, accurate, and trustworthy, preventing misleading or nonsensical outputs.",
              "criteria_evidence": "Validation test reports including: 1. Evidence of successful integration into the prediction pipeline. 2. A log of domain expert reviews confirming the plausibility of sample explanations. 3. Results from stability tests showing consistent explanations for minor input variations."
            },
            {
              "criteria": "TRN-COM - 01: Technical explanation outputs must be translated into formats that are understandable, accessible, and actionable for each target audience.",
              "control_objective": "To bridge the gap between complex model outputs and user comprehension, thereby achieving true transparency and empowering users.",
              "criteria_evidence": "A 'Communication & UI/UX Design' document containing: 1. Natural language templates for end-user explanations. 2. Designs for actionable or counterfactual explanations. 3. Mockups or screenshots of visualizations for internal dashboards."
            }
          ],
          "PlanSteps": [
            {
              "step": "TRN-DEF-TEST-01: Identify and document all audiences for explanations, categorizing them into groups such as Developers/Auditors, Business Owners, and Affected End-Users.",
              "step_objective": "To gather the necessary inputs for the 'Explainability Requirements Document' and satisfy a core component of criterion TRN-DEF-01."
            },
            {
              "step": "TRN-DEF-TEST-02: For each identified audience, define and document the primary question the explanation needs to answer (e.g., debugging, business logic validation, or personal recourse).",
              "step_objective": "To ensure explanations are relevant and useful, fulfilling the audience needs analysis required by criterion TRN-DEF-01."
            },
            {
              "step": "TRN-DEF-TEST-03: Conduct and document a review of legal and regulatory obligations for explainability, such as those related to GDPR in Luxembourg.",
              "step_objective": "To verify that the system's transparency features are designed for legal compliance, as mandated by criterion TRN-DEF-01."
            },
            {
              "step": "TRN-TECH-TEST-01: For a given use case, justify the choice of model type (interpretable vs. complex) based on performance and transparency trade-offs.",
              "step_objective": "To test that the model selection process aligns with the principle of using the simplest effective model, as outlined in criterion TRN-TECH-01."
            },
            {
              "step": "TRN-TECH-TEST-02: For a complex model, generate and review a global explanation report (e.g., a feature importance plot) and verify it is produced correctly.",
              "step_objective": "To confirm the implementation of a global explanation technique, providing evidence for criterion TRN-TECH-01."
            },
            {
              "step": "TRN-TECH-TEST-03: For a complex model, generate a local explanation (e.g., a SHAP force plot) for a specific prediction and confirm its output.",
              "step_objective": "To confirm the implementation of a local explanation technique, providing evidence for criterion TRN-TECH-01."
            },
            {
              "step": "TRN-VAL-TEST-01: Run an end-to-end test of the prediction pipeline and verify that an explanation artifact is generated and stored alongside every prediction.",
              "step_objective": "To validate the technical integration of the XAI library, providing evidence for the robustness requirement of TRN-VAL-01."
            },
            {
              "step": "TRN-VAL-TEST-02: Submit a sample of 10-20 explanations to a domain expert for review and collect their signed approval that the explanations are logical and plausible.",
              "step_objective": "To test the contextual meaningfulness of the explanations, fulfilling the plausibility requirement of criterion TRN-VAL-01."
            },
            {
              "step": "TRN-VAL-TEST-03: Create two near-identical input samples with only minor, irrelevant variations. Generate explanations for both and verify that the resulting explanations are highly similar.",
              "step_objective": "To empirically test the output's robustness, fulfilling the stability requirement of criterion TRN-VAL-01."
            },
            {
              "step": "TRN-COM-TEST-01: Take a raw SHAP value output and process it through the natural language template. Verify the output is grammatically correct and easy to understand for a non-technical user.",
              "step_objective": "To test the effectiveness of the translation layer, providing evidence for the user-facing communication criterion TRN-COM-01."
            },
            {
              "step": "TRN-COM-TEST-02: For a simulated negative outcome (e.g., loan rejection), verify that the generated explanation includes actionable guidance or a counterfactual.",
              "step_objective": "To confirm that explanations are designed to empower users, directly testing a key component of criterion TRN-COM-01."
            },
            {
              "step": "TRN-COM-TEST-03: Generate an explanation for a business user and verify it is displayed as a simple, clear visualization on the internal dashboard.",
              "step_objective": "To ensure explanations for internal stakeholders are effective and easy to interpret, as required by criterion TRN-COM-01."
            },
            {
              "step": "TRN-GOV-TEST-01: Review the project's Model Card and confirm the presence and completeness of the 'Explainability' section.",
              "step_objective": "To audit the formal documentation and ensure it meets the governance standards set by criterion TRN-GOV-01."
            },
            {
              "step": "TRN-GOV-TEST-02: Follow the published user process to request an explanation. Verify that the request is logged and handled according to the documented procedure and timelines.",
              "step_objective": "To perform an end-to-end test of the operational process, confirming compliance with criterion TRN-GOV-01."
            }
          ]
        },
        {
          "FieldType": "plan",
          "FieldName": "(A.9.4) - Purpose Limitation and Prompt Category Enforcement Test Plan",
          "PlanObjective": "To test and validate that the RAG Orchestrator's technical controls are effective in enforcing the AI system's documented intended use by correctly classifying user prompts, rejecting out-of-scope requests, and monitoring for prohibited terms.",
          "PlanCriteria": [
            {
              "criteria": "[PL-TEST-01]: The RAG Orchestrator's Prompt Classification Module must accurately classify user prompts and reject any that are out-of-scope before the retrieval step.",
              "control_objective": "To create a robust, intelligent gatekeeper within the RAG Orchestrator that ensures only user prompts matching the system's approved functionalities are ever processed.",
              "criteria_evidence": "System logs showing both successful classifications for in-scope prompts and explicit rejections for a variety of out-of-category prompts."
            },
            {
              "criteria": "[PL-TEST-02]: The RAG Orchestrator's prompt templating engine must dynamically select the correct category-specific system prompt based on the classified user intent.",
              "control_objective": "To improve the accuracy and relevance of the LLM's response by providing it with context-specific instructions tailored to the user's classified intent.",
              "criteria_evidence": "Logs from the RAG Orchestrator demonstrating that the correct system prompt template (e.g., 'Summarization' template) was selected and applied based on the classified user query."
            },
            {
              "criteria": "[PL-TEST-03]: The RAG Orchestrator must log any user query containing a term from the prohibited terms blacklist.",
              "control_objective": "To create a critical audit trail for attempted misuse of the system, providing the necessary data for user retraining and incident response.",
              "criteria_evidence": "A screenshot from the monitoring dashboard or a sample from the monitoring logs showing that a user query containing a prohibited term was successfully flagged."
            },
            {
              "criteria": "[PL-TEST-04]: The RAG Orchestrator's API response must include the correct category-aware UI directives for consumption by the front-end.",
              "control_objective": "To create a feedback loop where the orchestrator's classification actively guides the user, reinforcing the system's capabilities and improving the user experience.",
              "criteria_evidence": "A sample JSON response from the API containing the correct classified category (e.g., 'category': 'Document Summarization') for a given user prompt."
            }
          ],
          "PlanSteps": [
            {
              "step": "PL-EXEC-01: Develop a test suite of at least 20 user prompts, including a mix of prompts that clearly fall within each permitted category and several prompts that are deliberately out-of-scope or ambiguous.",
              "step_objective": "To create a comprehensive set of test cases to validate the accuracy and robustness of the Prompt Classification Module as required by criterion [PL-TEST-01]."
            },
            {
              "step": "PL-EXEC-02: Execute the test suite against the RAG Orchestrator endpoint and analyze the system logs to verify that all in-scope prompts were correctly classified and all out-of-scope prompts were rejected with the appropriate error message.",
              "step_objective": "To empirically test and gather the evidence needed to confirm that the Prompt Classification Module meets the success criteria defined in [PL-TEST-01]."
            },
            {
              "step": "PL-EXEC-03: For each successfully classified prompt from the test suite, review the RAG Orchestrator's detailed logs to confirm that the corresponding category-specific system prompt template was selected and applied.",
              "step_objective": "To gather the log-based evidence required to validate that the dynamic prompt templating engine is functioning as specified in criterion [PL-TEST-02]."
            },
            {
              "step": "PL-EXEC-04: Submit a series of prompts that each contain a different term from the prohibited terms blacklist (e.g., 'salary', 'performance review').",
              "step_objective": "To create test events that specifically trigger the monitoring control and allow for the validation of criterion [PL-TEST-03]."
            },
            {
              "step": "PL-EXEC-05: Review the monitoring dashboard and system logs to confirm that each prompt submitted in the previous step was successfully flagged and logged with the correct user and session details.",
              "step_objective": "To collect the necessary screenshot and log evidence to prove that the Prohibited Terms Monitoring control is operational as required by [PL-TEST-03]."
            },
            {
              "step": "PL-EXEC-06: For each permitted category, submit a valid prompt and capture the full API JSON response from the RAG Orchestrator. Inspect the response to verify that the 'category' field accurately reflects the prompt's intent.",
              "step_objective": "To confirm that the orchestrator correctly communicates context to the front-end, thereby gathering the evidence needed to satisfy criterion [PL-TEST-04]."
            }
          ]
        }
      ]
    }
  ],
  "Phase Operations": [
    {
      "StepName": "(A.6.2.6) AI Lifecycle Phase requirements - Operation and Monitoring",
      "Objectives": [
        {
          "Objective": "Define the security requirements for lifecycle phase - Deployment, Maintenance and Updates."
        }
      ],
      "Fields": [
        {
          "FieldType": "risk",
          "FieldName": "Insufficient Scalability Management",
          "question": "Will this AI be used across multiple teams, products, or regions — and is it ready to scale reliably?",
          "controls": [
            {
              "control": "[SC][1] - Implement separate horizontal auto-scaling mechanisms for the RAG Orchestrator (CPU-bound) and the privately-hosted LLM inference service (GPU-bound). Scaling policies for each should be based on relevant metrics (e.g., request latency for the orchestrator, GPU utilization for the LLM).",
              "control_objective": "To independently scale the logic and generation components of the system, ensuring that a bottleneck in one does not unnecessarily consume costly resources in the other.",
              "control_evidence": "Two distinct Infrastructure-as-Code (IaC) configurations (e.g., separate Kubernetes HPAs or Terraform auto-scaling groups). Load test results demonstrating that orchestrator and LLM instances scale independently under different load patterns. Monitoring dashboards showing separate scaling events."
            },
            {
              "control": "[SC][2] - Architect the RAG Orchestrator and the LLM inference endpoints as stateless services. Any state required for multi-turn conversations must be managed by the User Interface or externalized to a scalable cache, not held in-memory on the backend instances.",
              "control_objective": "To enable seamless load balancing and instant failover across all backend components, ensuring that any instance can process any incoming request without being dependent on local session data.",
              "control_evidence": "System architecture diagram showing stateless services and external state stores. Code review of the orchestrator and LLM service handlers confirming the absence of in-memory session storage. High-availability test report showing user sessions persist after forced restarts of multiple service instances."
            },
            {
              "control": "[SC][3] - Ensure the Permanent Index (vector database) is deployed on an architecture that supports horizontal scaling to handle high query throughput. This should be achieved by scaling out the number of nodes/pods or by implementing index sharding.",
              "control_objective": "To prevent the vector search operation from becoming a performance bottleneck as the number of concurrent users and the size of the knowledge base grow.",
              "control_evidence": "Vector database configuration files (e.g., Helm charts, cloud service settings) showing sharding or replica settings. Performance benchmarks demonstrating query latency remains stable as the query load increases. A documented procedure for scaling the vector database cluster."
            },
            {
              "control": "[SC][4] - Design the asynchronous Data Processing Pipeline using a queue-based architecture (e.g., RabbitMQ, SQS). This decouples the initial vectorization of proprietary data from the core application, allowing it to handle large-volume data ingestion without impacting real-time query performance.",
              "control_objective": "To ensure that the creation and updating of the knowledge base is a robust, fault-tolerant, and scalable process that does not degrade the responsiveness of the live chat service.",
              "control_evidence": "Architecture diagram illustrating the message queue between the data sources and the processing pipeline. Configuration of the queue and the worker pool. Logs showing successful processing of a large batch of documents, with metrics on processing time and throughput."
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "[C.3.6] - Poor Management of AI System Evolution and Updates",
          "question": "When the AI system is updated (e.g., retrained model, new features), are there formal processes for testing, documenting, and deploying these changes, including a plan to roll back if the update causes problems?",
          "controls": [
            {
              "control": "[C.3.6][BP-10] - Implement a CI/CD pipeline that automates the testing and deployment of AI model updates. The pipeline must enforce a sequence of validation gates (e.g., unit tests, data validation, integration tests, and model performance evaluation on a holdout dataset) before allowing a deployment.",
              "control_objective": "To automate quality assurance, reduce human error, and ensure that only thoroughly vetted and validated model updates are promoted to production.",
              "control_evidence": "The CI/CD pipeline configuration file (e.g., `gitlab-ci.yml`, Jenkinsfile). Test reports and logs generated by the pipeline showing successful completion of all gates. A deployment manifest that references the specific model version and code commit hash deployed."
            },
            {
              "control": "[C.3.6][BP-11] - Utilize a version control system that atomically bundles code, data schemas/references, and model artifacts for each release. Every production deployment must be linked to a single, immutable commit hash or tag.",
              "control_objective": "To ensure complete reproducibility of any deployed AI system version and enable reliable, one-step rollbacks to a previous stable state.",
              "control_evidence": "Git repository history showing tagged releases. A `dvc.yaml` or similar data versioning file that pins data versions to specific code commits. Deployment logs explicitly stating the commit hash or tag being deployed for each release."
            },
            {
              "control": "[C.3.6][BP-13] - All data ingestion pipelines must include an automated data validation gate. This gate must verify data schemas, check for statistical drift in key features, and validate data quality against predefined rules before new data is accepted into the training dataset.",
              "control_objective": "To prevent model performance degradation caused by upstream data source changes, ensuring data integrity, consistency, and stability across model versions.",
              "control_evidence": "Configuration files for a data validation tool (e.g., Great Expectations, Pandera). CI/CD logs showing the successful execution of the data validation step. Generated data quality reports and drift analysis dashboards."
            },
            {
              "control": "[C.3.6][BP-14] - Any modification to the production dataset within the central data repository (as defined in control A.4.2, A.4.3), including additions, deletions, or schema changes, must be executed through a formal change management ticket that requires peer review and explicit approval from a designated data owner.",
              "control_objective": "To maintain the integrity, traceability, and quality of the training dataset by preventing unauthorized or undocumented changes that could adversely affect model performance and reliability.",
              "control_evidence": "Documented data change management procedure. Completed change request tickets (e.g., in Jira, ServiceNow) with approval history. Audit logs from the data repository or data pipeline tools confirming that changes were applied post-approval."
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "Inadequate Disaster Recovery and Business Continuity Planning",
          "question": "What would happen to your business process if this AI system suddenly stopped working?",
          "controls": [
            {
              "control": "[BU][1] - Define the stateless services—specifically the RAG Orchestrator and User Interface—using Infrastructure-as-Code (IaC). The IaC templates must be version-controlled to allow for the rapid, automated deployment of a functional replica in a designated disaster recovery region.",
              "control_objective": "To ensure the core logic and user-facing components of the RAG system can be redeployed consistently and quickly, minimizing the Recovery Time Objective (RTO) for the application's brain and entry point.",
              "control_evidence": "Version-controlled IaC files (e.g., Terraform, CloudFormation) for the RAG Orchestrator and UI services. A disaster recovery runbook detailing the execution steps. Logs from a successful DR drill showing these services becoming operational in the failover environment."
            },
            {
              "control": "[BU][2] - Implement automated, cross-region replication or point-in-time backups for the Permanent Index (vector database). The recovery process for this data store must be tested to ensure the Recovery Point Objective (RPO) is met.",
              "control_objective": "To prevent catastrophic loss of the vectorized knowledge base, which is costly and time-consuming to recreate, by ensuring a recent copy is always available in a geographically separate location.",
              "control_evidence": "Cloud console configurations or database settings showing cross-region replication or automated backups are active for the vector database. A log from a recent data restoration test. Monitoring dashboards displaying replication lag or backup completion status."
            },
            {
              "control": "[BU][3] - Configure a DNS-based failover mechanism for the User Interface's public endpoint. The system must use health checks to monitor the primary RAG Orchestrator's availability and be able to redirect traffic to the replica in the disaster recovery region.",
              "control_objective": "To provide a seamless transition for end-users during a regional outage by automatically or manually redirecting their requests to the healthy standby environment.",
              "control_evidence": "DNS record configuration (e.g., AWS Route 53, Azure Traffic Manager) showing the failover routing policy. Health check configuration and status logs. A report from a planned failover test that confirms traffic was successfully redirected with minimal user-facing downtime."
            },
            {
              "control": "[BU][4] - Ensure the privately-hosted LLM inference endpoint is deployed with a multi-region disaster recovery strategy. This must include either an active-standby deployment defined in IaC or a documented procedure to rapidly redeploy the model from a central, replicated model registry into the failover region.",
              "control_objective": "To guarantee that the core generative capability of the system is recoverable, preventing a complete service outage if the primary LLM hosting environment becomes unavailable.",
              "control_evidence": "IaC templates for the LLM endpoint deployment (e.g., SageMaker endpoint config, Kubernetes manifests). A DR runbook with steps for LLM failover/redeployment. Screenshots of the model artifact stored in a resilient registry (e.g., S3 with cross-region replication)."
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "Insufficient Performance Monitoring and Analysis",
          "question": "Will the AI’s outputs need to be continuously monitored to ensure they stay accurate and relevant?",
          "controls": [
            {
              "control": "[PER][1] - Instrument the RAG Orchestrator with distributed tracing to measure the end-to-end latency of a user query. The trace must capture the duration of each critical operation: retrieval from the vector index, augmentation of the prompt, and response generation by the LLM.",
              "control_objective": "To pinpoint performance bottlenecks within the live RAG chain, enabling targeted optimization of either the retrieval or generation components to improve user-perceived response time.",
              "control_evidence": "A distributed tracing dashboard (e.g., Jaeger, Datadog) showing a waterfall diagram of a user request. A metrics dashboard displaying P95/P99 latency for each individual step (retrieval, generation)."
            },
            {
              "control": "[PER][2] - Implement dedicated monitoring for the privately-hosted LLM inference endpoint. Dashboards must track GPU utilization, memory usage, time-to-first-token, and output tokens per second, with alerts configured for significant degradations.",
              "control_objective": "To ensure the most resource-intensive component of the stack is operating efficiently and to proactively detect hardware or model issues before they impact the entire system.",
              "control_evidence": "A monitoring dashboard (e.g., Grafana) with panels for GPU metrics and inference latency. The configuration file for the alerting rules (e.g., in Prometheus/Alertmanager). Load test reports showing performance under stress."
            },
            {
              "control": "[PER][3] - Establish a feedback loop by logging a sample of user queries, the IDs of the retrieved document chunks, and the final generated response. Implement an offline job to periodically calculate retrieval quality metrics (e.g., hit rate, Mean Reciprocal Rank) against a golden dataset.",
              "control_objective": "To monitor the relevance and accuracy of the RAG system's outputs over time, detecting concept drift or degradation in retrieval quality as the knowledge base evolves.",
              "control_evidence": "A sample of the logged data from the production system. A script from a CI/CD pipeline or scheduled job that runs the evaluation. A report or dashboard showing the trend of retrieval quality metrics over time."
            },
            {
              "control": "[PER][4] - Instrument the asynchronous Data Processing Pipeline to monitor its throughput (documents processed per hour) and error rate. Track the average time spent on each stage: text extraction, chunking, and embedding generation.",
              "control_objective": "To ensure the knowledge base creation process is scalable and reliable, allowing for timely updates and additions to the system's proprietary data without system degradation.",
              "control_evidence": "A metrics dashboard for the data pipeline showing throughput and error rates. Logs from pipeline workers detailing the processing time for each stage. An alert that triggers if the pipeline's failure rate exceeds a set threshold."
            }
          ]
        },
        {
          "FieldType": "risk",
          "FieldName": "Inadequate Security Monitoring and Threat Detection",
          "question": "Do you need visibility into whether this AI system is under attack or being misused?",
          "controls": [
            {
              "control": "[STM][1] - Implement logging for all prompts sent to the RAG Orchestrator and the final augmented prompts sent to the LLM. All LLM responses must also be logged before being sent to the user. These logs must be streamed to a centralized SIEM for analysis.",
              "control_objective": "To create an auditable trail for detecting and investigating prompt injection attacks, data exfiltration attempts, and misuse of the generative model's capabilities.",
              "control_evidence": "SIEM dashboard showing ingested prompt and response logs. A documented alert rule that triggers on signatures of known prompt injection techniques (e.g., 'ignore previous instructions'). Code review demonstrating the logging calls within the RAG Orchestrator."
            },
            {
              "control": "[STM][2] - The Data Processing Pipeline must integrate a file scanning mechanism (e.g., ClamAV) to inspect all internal proprietary documents *before* text extraction. Any documents flagged as malicious must be quarantined, and an alert must be generated.",
              "control_objective": "To prevent the ingestion of weaponized documents that could exploit vulnerabilities in the processing pipeline or poison the permanent vector index with malicious content.",
              "control_evidence": "Logs from the file scanner showing files being successfully scanned or quarantined. An example security alert generated by a malicious test file. The pipeline's configuration file or code showing the integration of the scanning step."
            },
            {
              "control": "[STM][3] - Enable and centralize audit logs from the Permanent Index (vector database). Configure alerts for anomalous query patterns, such as an unusually high volume of retrieval requests from a single user or attempts to enumerate large portions of the index.",
              "control_objective": "To detect attempts to exfiltrate large amounts of proprietary data from the knowledge base or unauthorized attempts to access restricted data segments.",
              "control_evidence": "Vector database configuration file with auditing enabled. Screenshots of the SIEM dashboard displaying query logs. A documented alert rule that triggers on a high-frequency query threshold from a single source IP or user account."
            },
            {
              "control": "[STM][4] - Deploy network flow logging for traffic between all internal components (UI, RAG Orchestrator, LLM, Vector DB). Establish a baseline of normal traffic patterns and configure alerts for deviations, such as unexpected connections or unusually large data payloads.",
              "control_objective": "To detect potential lateral movement by an attacker or compromised components within the architecture, particularly to and from the isolated LLM.",
              "control_evidence": "VPC flow logs or network monitoring tool dashboards. A documented network traffic baseline report. An active alert that triggers when a service attempts to connect to another service on a non-standard port."
            }
          ]
        }
      ]
    }
  ],
  "Phase 6": [
    {
      "StepName": "AI Systems approvals",
      "Objectives": [
        {
          "Objective": "Stakeholder Approval and Governance: To obtain formal sign-off from all relevant stakeholders, confirming that the deployment plan is sound and all prerequisites have been satisfied, thereby providing a clear governance gate and accountability for the deployment decision."
        }
      ],
      "Fields": [
        {
          "FieldName": "AI System Security Approver",
          "FieldLabel": "Security Approver",
          "FieldText": "Name/Role of the Security Aprover",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "AI System Security Approval",
          "FieldLabel": "Security Approved",
          "FieldText": "",
          "FieldType": "Option box with values:Yes/No"
        },
        {
          "FieldName": "AI System DPO Approver",
          "FieldLabel": "DPO Approver",
          "FieldText": "Name/Role of the DPO Aprover",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "AI System DPO Approval",
          "FieldLabel": "DPO Approved",
          "FieldText": "",
          "FieldType": "Option box with values:Yes/No"
        },
        {
          "FieldName": "AI System Risk Approver",
          "FieldLabel": "Risk Approver",
          "FieldText": "Name/Role of the Risk Aprover",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "AI System Risk Approval",
          "FieldLabel": "Risk Approved",
          "FieldText": "",
          "FieldType": "Option box with values:Yes/No"
        },
        {
          "FieldName": "AI System Business Approver",
          "FieldLabel": "Business Approver",
          "FieldText": "Name/Role of the Business Approver",
          "FieldType": "TextBox"
        },
        {
          "FieldName": "AI System Business Approval",
          "FieldLabel": "Business Approved",
          "FieldText": "",
          "FieldType": "Option box with values:Yes/No"
        }
      ]
    }
  ]
}